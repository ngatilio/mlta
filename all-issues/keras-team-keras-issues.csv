url,title,comments,state,created_at,closed_at
https://api.github.com/repos/keras-team/keras/issues/15986,b'Extra GPU-CPU memory transfer when broadcasting operations between integer tensors',[],open,2022-01-31 09:43:15,
https://api.github.com/repos/keras-team/keras/issues/15985,b'`image_dataset_from_directory()` should return both training and validation datasets',[],open,2022-01-31 09:14:48,
https://api.github.com/repos/keras-team/keras/issues/15984,b'Cannot add class weights for multilabel classification?',[],open,2022-01-31 07:29:00,
https://api.github.com/repos/keras-team/keras/issues/15983,b'Fix minor typo in Model.compile docstring',[],open,2022-01-31 01:16:02,
https://api.github.com/repos/keras-team/keras/issues/15982,"b'Rename ""premade"" (the meaning of which was ambiguous) to ""premade_models"" (clearer).'",[],open,2022-01-30 22:21:14,
https://api.github.com/repos/keras-team/keras/issues/15980,b'TimeDistributed(Flatten()) does not support masking',[],open,2022-01-30 20:53:58,
https://api.github.com/repos/keras-team/keras/issues/15979,b'Update metrics.py',[],open,2022-01-30 17:34:58,
https://api.github.com/repos/keras-team/keras/issues/15978,b'Silent swallowing of Exceptions thrown in `get_config` and potentially `from_config` / Documentation of SavedModel for custom Model',[],open,2022-01-30 08:19:25,
https://api.github.com/repos/keras-team/keras/issues/15977,b'Update callbacks.py',[],open,2022-01-29 07:35:20,
https://api.github.com/repos/keras-team/keras/issues/15976,b'How to create a dynamic embedding layer in keras ?',[],open,2022-01-29 06:55:28,
https://api.github.com/repos/keras-team/keras/issues/15975,b'Copy image utils from keras_preprocessing directly into core keras',[],open,2022-01-29 00:35:26,
https://api.github.com/repos/keras-team/keras/issues/15972,b'Feature/documentation request: re-add activation histograms or remove from docs',"[b'I can try to solve this', b'Hey @bersbersbers as far as I know you can visualize histograms in \r\ntensorboard by using tf.summary.histogram<br>Can you please attach your code so that I can see if we are on the same page?', b'@Cheril311 please see https://github.com/tensorflow/tensorflow/issues/42027#issuecomment-670719469. Keras writes weight histograms, but no activation histograms.', b'You seem to be right @bersbersbers , for now I have changed the documentation as I do know how to compute activation histograms but I am unaware on how to write them as tensorboard images', b'> I do know how to compute activation histograms but I am unaware on how to write them as tensorboard images\n\nI would think activation histograms can be written the same way as the weight histograms, at least in a first iteration. After all, both should have the exact same dimensions.', b'> > I do know how to compute activation histograms but I am unaware on how to write them as tensorboard images\r\n> \r\n> I would think activation histograms can be written the same way as the weight histograms, at least in a first iteration. After all, both should have the exact same dimensions.\r\n\r\nYup I agree with that but there is some other process done in the function  _log_weight_as_image in callbacks.py and I am not sure how to do the same for activations, if you can help me maybe we can submit a pr', b'> > I would think activation histograms can be written the same way as the weight histograms, at least in a first iteration. After all, both should have the exact same dimensions.\r\n> \r\n> Yup I agree with that but there is some other process done in the function _log_weight_as_image in callbacks.py and I am not sure how to do the same for activations, if you can help me maybe we can submit a pr\r\n\r\nTo my understanding, activations (for one sample) and weights have the same shape, so you can do exactly the same thing as here:\r\n\r\nhttps://github.com/keras-team/keras/blob/8015f697d0109ccd6021fe1b8f2bf5b133b259b9/keras/callbacks.py#L2570-L2580\r\n\r\nSince you mentioned in https://github.com/keras-team/keras/issues/15972#issuecomment-1024857466 that you know how to compute activation histograms (although I have yet to find a good solution myself, compare https://stackoverflow.com/q/60816678, https://gist.github.com/SiLiKhon/3965c967c3283feccc79822e6252b34c, https://stackoverflow.com/q/66779524), this should be straightforward then. Good luck!']",open,2022-01-28 14:20:40,
https://api.github.com/repos/keras-team/keras/issues/15967,b'RNN to model  lowpass filter',[],open,2022-01-27 18:47:05,
https://api.github.com/repos/keras-team/keras/issues/15964,b'LSTM model save warning',"[b""@StevenHuang2020 I was able to run the code successfully on colab using TF v2.7.0 and tf-nightly(2.9.0.dev20220126)  and didn't face the warning reported here.Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/997a7d67340735a857995fe3231a33f2/15964.ipynb#scrollTo=SH7P1z-zZVVr) and confirm the same?\r\nPlease provide the complete code and let us know if we are missing something to replicate the issue? Thanks!"", b'Here is my code.\r\nhttps://github.com/StevenHuang2020/COVID-19-Statistics/blob/e73c17cdc14ce38616164334dde3ed7c09543328/coronavirus/predictStatistics.py\r\n\r\nThe warning message is shown when model.save called after training\r\n\r\n![image](https://user-images.githubusercontent.com/61686583/151493297-229f2618-f805-4d4d-97f8-ccf864e7ac68.png)\r\n', b'I simplified it to 2 files, the dataset and py file.\r\n\r\ndataset link:\r\nhttps://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\r\ntraining file:\r\n[predictStatistics.py.txt](https://github.com/keras-team/keras/files/7956105/predictStatistics.py.txt)\r\nYou can verify the above problem with this.\r\n', b'@jvishnuvardhan Was able to replicate the issue on colab using TF v2.7.0 , please find the gist [here ](https://colab.research.google.com/gist/sushreebarsa/001c1cfa939c1ae5e385c3f2c00ba072/keras15964.ipynb) for reference.Thanks!']",open,2022-01-27 11:21:38,
https://api.github.com/repos/keras-team/keras/issues/15958,b'Stop testing keras in the core tf docstest',[],open,2022-01-26 20:42:34,
https://api.github.com/repos/keras-team/keras/issues/15952,b'Support checkpointing ShardedVariables in optimizer slot variables.',[],open,2022-01-25 20:54:58,
https://api.github.com/repos/keras-team/keras/issues/15947,b'Fix tf.name_scope support for Keras nested layers.',[],open,2022-01-25 09:08:20,
https://api.github.com/repos/keras-team/keras/issues/15941,b'mode.predict freezes',[b'@shubham9630 Can you please share a simple standalone code to reproduce the issue? Thanks!'],open,2022-01-23 20:37:31,
https://api.github.com/repos/keras-team/keras/issues/15940,b'The model saving is disturbed by the error raised intentionaly.',"[b""@realmadridmarcelo I am not able to reproduce the issue. When I ran your code, I was able to save the model in `h5` format but couldn't load it. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/f53d38e7cc05c9fef9a505aa5aaecd7e/untitled1145.ipynb). Thanks!\r\n\r\nPlease check whether I made any mistake. Thanks!"", b'Thank you for your examination!\r\nYour code and mine are almost the same.\r\nOnly one difference is the python version. (My env: 3.8.12, Your env: 3.7)\r\nBut I think that it is not essential for this issue.']",open,2022-01-23 11:26:02,
https://api.github.com/repos/keras-team/keras/issues/15939,b'Shape issues in tf.keras.metrics.SparseTopKCategoricalAccuracy with multiple dimensions',"[b'This is indeed a surprising behavior -- it seems the format expectation for the `sample_weight` argument is different in the case of the `SparseTopKCategoricalAccuracy` in this case. Would you like to open a PR with a fix?', b""Sure I'll have a look. I'll also double-check if this happens for the others in 'metrics' to see if they fail in a similar manner."", b'Thank you!']",open,2022-01-23 04:48:16,
https://api.github.com/repos/keras-team/keras/issues/15936,"b'TensorFlow ""TypeError: Target data is missing"" though dataset with 2 dimension tuple was supplied'","[b'Can you provide a code example where I can run your code to replicate the issue', b'Here is the full code of the file:\r\n\r\n```\r\nfrom datetime import datetime\r\n\r\nfrom keras import layers\r\nfrom keras.layers import Reshape\r\nfrom numpy import loadtxt\r\nimport tensorflow_addons as tfa\r\nimport numpy as np\r\nimport sys\r\n\r\nsys.path.append(""./transformer/models/official/vision/beta/projects/vit/modeling/"")\r\nsys.path.append(""./transformer/models"")\r\n\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras import regularizers\r\nimport tensorflow as tf\r\nfrom vit import VisionTransformer\r\nfrom tensorflow.keras import mixed_precision\r\npolicy = mixed_precision.Policy(\'mixed_float16\')\r\nmixed_precision.set_global_policy(policy)\r\n\r\nREGULARIZATION = 1e-10\r\nBIAS_REGULARIZATION = 1e-8\r\nDATASET_SPLIT_PERCENTAGE = 1\r\nprint(""Num GPUs Available: "", len(tf.config.list_physical_devices(\'GPU\')))\r\n#load csv file data\r\nimport csv\r\nwith open(\'/sda/Eran/imagenet-in-np/all-imagenet-images.csv\', \'r\') as csvfile:\r\n    reader = csv.reader(csvfile, delimiter=\',\')\r\n    feature = list(reader)\r\ndef convert_token_list_to_id_list(token_list):\r\n    \'\'\'\r\n    given list of one cell list of tokens, assign each token a unique id, and then replaces each instance of the token with the matched id\r\n    returns the original list of token by the original order, with each token replaced by id\r\n    \'\'\'\r\n    token_list = [item for sublist in token_list for item in sublist]\r\n    unique_tokens = list(set(token_list))\r\n    token_to_id_dict = dict(zip(unique_tokens, range(len(unique_tokens))))\r\n    id_list = [token_to_id_dict[token] for token in token_list]\r\n    return id_list\r\ndef numpy_one_hot_vector(id_list, num_classes):\r\n    \'\'\'\r\n    given a list of ids, return a numpy array of one-hot vectors\r\n    \'\'\'\r\n    num_classes+=1\r\n    one_hot_vector_list = np.zeros((len(id_list), num_classes), dtype=np.float32)\r\n    one_hot_vector_list[np.arange(len(id_list)), id_list] = 1\r\n    return one_hot_vector_list\r\n\r\nfeature=convert_token_list_to_id_list(feature)\r\nfeature=numpy_one_hot_vector(feature,1000)\r\nsamples=np.load(""/sda/Eran/imagenet-in-np/extracted-wavelet-of-all-imagenet.npy"").T\r\nNUM_OF_SAMPLES = samples.shape[0]\r\nBLOCK_DIMENSION = [NUM_OF_SAMPLES, 16, 17, 10]\r\n\r\nmean = (samples.mean(axis=0))\r\nstd=(samples.std(axis=0))\r\nmean=mean.T\r\nstd=std.T\r\nepsilon=np.finfo(float).tiny\r\nsamples=((samples - mean)/(std+epsilon))\r\n\r\n\r\n###\r\n#@todo change the shape to something meaningfull\r\nsamples=np.reshape(samples, BLOCK_DIMENSION)\r\n# define the keras model\r\nmodel=Sequential()\r\nmodel.add(VisionTransformer(input_specs=layers.InputSpec(shape=BLOCK_DIMENSION)))\r\nmodel.add(Dense(1000,activation=""softmax""))\r\nmodel.add(Reshape([ 1000]))\r\n# compile the keras model\r\nmodel.compile(loss=""BinaryCrossentropy"", optimizer=tfa.optimizers.LAMB(),\r\n              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()]\r\n              )\r\n\r\nlog_dir = ""/home/eran/logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\nprint(model.summary())\r\n\r\ndef gen():\r\n    for element in zip(samples,feature):\r\n        yield element\r\nds = tf.data.Dataset.from_generator(gen,output_types=tf.dtypes.float32)\r\n\r\n\r\n\r\nmodel.fit(ds,\r\n          epochs=150,\r\n          #callbacks=[tensorboard_callback]\r\n          )\r\nmodel.save(""/sda/Eran/imagenet-in-np/transformer"")\r\n\r\npass\r\n```', b'Can you also provide the data files you are using or maybe preferably generate random data in the code so that I can run your code. Also provide the version of packages you are using ', b'@exx8 Can you please share a simple standalone code to reproduce the issue? Looks like the issue is more related to data generator, so use some simple model with a data generator (of random data). Thanks!']",open,2022-01-21 17:09:03,
https://api.github.com/repos/keras-team/keras/issues/15933,b'Higher validation and test loss and lower accuracy using tf.data.dataset with tfrecords than using numpy',[],open,2022-01-21 11:46:59,
https://api.github.com/repos/keras-team/keras/issues/15931,"b""Using learning-rate decay schedule with Keras (transfer learning with RESNET) generates unsupported operand error 'ExponentialDecay'""",[b'@castorgit this problem appears to be fixed in the latest TF-nightly version. See here https://colab.research.google.com/gist/amahendrakar/697836288a6d02f048d11e08b7044079/41639-tf-nightly.ipynb'],open,2022-01-21 07:21:09,
https://api.github.com/repos/keras-team/keras/issues/15925,"b""Use fully qualified name in keras.utils.custom_object_scope's docstring.""",[],open,2022-01-20 16:54:58,
https://api.github.com/repos/keras-team/keras/issues/15917,b'Loading optimizer state without using model.compile',"[b'@ttdd11 It looks like this is more related to `TPU distribution strategy`.  Can you open this issue on Tensorflow where I can triage it to TPU team? Thanks!', b""@jvishnuvardhan I did a few days ago, sorry that I didn't mention it here: https://github.com/tensorflow/tensorflow/issues/53844 , still waiting to hear back. Any added visibility would be greatly appreciated, as all of our compute experiments are currently on hold. ""]",open,2022-01-19 14:22:44,
https://api.github.com/repos/keras-team/keras/issues/15916,b'DeeplabV3+ Training on custom dataset',"[b'@parth29-vc \r\nIn order to reproduce the issue reported here, could you please provide the standalone code and the dataset ? Thanks!']",open,2022-01-19 06:01:06,
https://api.github.com/repos/keras-team/keras/issues/15915,b'BUG when converting from pytorch GRU model to Keras GRU model',[],open,2022-01-19 03:11:37,
https://api.github.com/repos/keras-team/keras/issues/15912,"b'keras.layers.SimpleRNN/LSTM output NaN when setting ""activation"" to ""exponential""'","[b""Hey, I think Pytorch does not have any 'exponential' activation function, so when your code is being converted using onnx do you know which pytorch activation function is being used"", b""> Hey, I think Pytorch does not have any 'exponential' activation function, so when your code is being converted using onnx do you know which pytorch activation function is being used\r\n\r\nYes, following the implementation of onnx2pytorch, the exponential activation will be converted to `torch.exp`.\r\nhttps://github.com/ToriML/onnx2pytorch/blob/89a3fe7b208952760138c550c499fe9c57c35d45/onnx2pytorch/convert/operations.py#L115\r\n\r\nApart from this, may I know that SimpleRNN/LSTM output NaN when setting activation as `Exponential` is the expected behavior?\r\n"", b'I think this is because of the **exploding gradients** problem as the gradients are increasing exponentially in your case. As technically you get NaNs when you perform computations that dont make sense, for example _inf-inf_. However I am not sure if getting those NaNs in your case is expected in place of Infs. I think Stack Overflow will be a better place to ask this.']",open,2022-01-17 08:42:11,
https://api.github.com/repos/keras-team/keras/issues/15911,b'drop_connect_rate in efficientnet is not documented',[],open,2022-01-15 23:23:54,
https://api.github.com/repos/keras-team/keras/issues/15909,b'Regularization Loss is getting added twice if model is encapsulated in another model',"[b'@sachinprasadhs Was able to reproduce the issue in TF v2.7 please find the gist [here](https://colab.research.google.com/gist/kumariko/a7d76966c629ffeb6fcc06fed0af8ddc/keras-regularizer-issue.ipynb#scrollTo=ABp9hJnVTxjr). Thanks!', b'Triage notes: we had similar bug before, and should look into the root cause for this.']",open,2022-01-15 15:26:04,
https://api.github.com/repos/keras-team/keras/issues/15900,b'The keras.layers.CategoricalEncoding layer removes batch dimension for single-element batches.',"[b'@jvishnuvardhan ,\r\nI was able to reproduce the issue in v2.7 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6451be832f2bcc872d06898773b276e2/categoryencoding.ipynb).']",open,2022-01-13 23:40:05,
https://api.github.com/repos/keras-team/keras/issues/15897,b'How to make Keras Layer class to be immutable or is this a bug?',"[b'@quaesitor-scientiam In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!']",open,2022-01-13 11:46:43,
https://api.github.com/repos/keras-team/keras/issues/15896,b'The GRU/LSTM function bugs when using TF2.7',"[b'@hayatejp ,\r\nPlease take a look at this SO [link](https://stackoverflow.com/questions/48851558/tensorflow-estimator-valueerror-logits-and-labels-must-have-the-same-shape) and [issue](https://github.com/tensorflow/tensorflow/issues/42824) with the similar error.It helps.Thanks!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n']",open,2022-01-13 05:43:28,
https://api.github.com/repos/keras-team/keras/issues/15895,b'Regarding documentation for RegNets',"[b'Triage notes: We should provide a user accessible way to produce performance benchmarks. Or we need to at least have it available internally to produce the number for user.', b'I think it might be useful to put up a guide for benchmarks for contributors. ', b'Thanks @sayakpaul. Please feel free to write up a guide for how to setup this env. \r\n\r\nOn a side note, we are going to explore a setup for keras cv and nlp to benchmark the performance for applications.', b'I think the Keras team should write to reduce the chances of errors, inconsistencies, etc. \r\n\r\nBut if you could share some references on how the team usually goes about it, I can take a dig. \r\n\r\nFor benchmarking performance (not the throughput, just accuracy), I usually do something like this:\r\n\r\nhttps://github.com/sayakpaul/ViT-jax2tf/tree/main/i1k_eval. ']",open,2022-01-13 04:58:34,
https://api.github.com/repos/keras-team/keras/issues/15892,b'use tf.io.gfile instead of os.path to support cloud paths.',[],open,2022-01-12 07:18:22,
https://api.github.com/repos/keras-team/keras/issues/15887,b'High memory consumption with model.fit in TF 2.x',"[b'Than you very much for doing this. Let me post also the slightly modified test code with built-in memory measurements, which may be more convenient:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport psutil\r\nimport os\r\n\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Input, Lambda, Conv2D\r\nfrom tensorflow.keras.callbacks import Callback\r\n\r\nprint(""Tensorflow version: {}"".format(tf.__version__),flush=True)\r\n\r\nK = 5000 # Number of images\r\nN = 512  # Image size\r\n\r\nMAX_SIGNAL = 5000 # The values of the training data range from 0 to this\r\n\r\nclass MemoryUsageCallback(Callback):\r\n  \'\'\'Monitor memory usage on epoch begin and end.\'\'\'\r\n\r\n  def on_epoch_begin(self,epoch,logs=None):\r\n    print(\'**Epoch {}**\'.format(epoch))\r\n    print(\'Memory usage on epoch begin: {}\'.format(psutil.Process(os.getpid()).memory_info().rss))\r\n\r\n  def on_epoch_end(self,epoch,logs=None):\r\n    print(\'Memory usage on epoch end:   {}\'.format(psutil.Process(os.getpid()).memory_info().rss))\r\n    \r\ndef build_model():\r\n  \'\'\'Create a simple test model.\'\'\'\r\n  \r\n  inputs = Input((N,N,1))\r\n  s = Lambda(lambda x: x / MAX_SIGNAL) (inputs)\r\n  s = Conv2D(1, (3, 3), activation=\'sigmoid\', padding=\'same\')(s)\r\n  outputs = s\r\n\r\n  return Model(inputs=[inputs], outputs=[outputs])\r\n\r\n# Generate some random data\r\nx_train = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_train = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\nx_val   = np.random.randint(MAX_SIGNAL+1,size=(K,N,N,1),dtype=np.uint16) # Should be 2 560 000 kB\r\ny_val   = np.random.randint(1+1         ,size=(K,N,N,1),dtype=np.bool)   # Should be 1 280 000 kB\r\n# In total, the above arrays should be 7 680 000 kB\r\n\r\nmodel = build_model()\r\n\r\ncallbacks = [MemoryUsageCallback()]\r\noptimizer = tf.keras.optimizers.Adam()\r\nloss = tf.keras.losses.BinaryCrossentropy()\r\n\r\nmodel.compile(optimizer=optimizer, loss=loss)\r\nmodel.fit(x=x_train, y=y_train, validation_data=(x_val,y_val), batch_size=8, epochs=10, callbacks=callbacks, verbose=0)\r\n```\r\n\r\nThe above is meant to reproduce the issue with data passed to `model.fit` as numpy arrays. To test the behavior with TF datasets, replace the last line with the following:\r\n\r\n```python\r\nds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(8)\r\nds_val = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(8)\r\nmodel.fit(ds_train, validation_data=ds_val, batch_size=8, epochs=10, callbacks=callbacks, verbose=0)\r\n```', b'Also, anybody investigating the root of the problem, be sure to note the issue tensorflow/tensorflow#35030, where @mihaimaruseac has tracked the point at which the bug has been introduced.', b'Triage notes: This is a long stand performance issue, and we should have someone look into this.', b""Hello @gdudziuk, it'd be helpful if you could help us with the following,\r\n\r\n- Is this a regression from any previous TF versions?\r\n- If you construct a custom training loop that achieves the same thing as this `Model.fit`, are you seeing smaller memory usage?\r\n\r\nThanks!""]",open,2022-01-11 04:35:32,
https://api.github.com/repos/keras-team/keras/issues/15881,b'test_activity_regularizer_batch_independent_v2_eager  and test_activity_regularizer_batch_independent_v2_function fail on aarch64',"[b'@ggardet ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'> \r\n\r\nWe just run:\r\n```\r\nfor f in keras/tests/*_test.py; do\r\n  # ValueError: Please provide a TPU Name to connect to.\r\n  [ ""$f"" = ""keras/tests/automatic_outside_compilation_test.py"" ] && continue || :\r\n  # import error: test file not installed into sitelib\r\n  [ ""$f"" = ""keras/tests/get_config_test.py"" ] && continue || :\r\n  # no tensorflow.compiler.tests\r\n  [ ""$f"" = ""keras/tests/tracking_util_xla_test.py"" ] && continue || :\r\n  # Note: These might be genuine errors, but in order to get it checked in, we rather ignore them now.\r\n  #       It is still better than not running any tests at all.\r\n  $python $f\r\ndone\r\n```\r\nWith `$python` being the current python interpreter.']",open,2022-01-10 13:20:05,
https://api.github.com/repos/keras-team/keras/issues/15863,b'Support relative (in addition to absolute) `min_delta` parameters in `keras.callbacks.ReduceLROnPlateau`',"[b'@rchao Rick, could you take a closer look? Thanks!']",open,2022-01-06 04:30:35,
https://api.github.com/repos/keras-team/keras/issues/15858,b'Error in build. ',"[b""Sadly we usually just verify our build process on Linux, and current we don't have a windows OSS build. Will discuss this on the triage meeting."", b""@old-school-kid \r\n\r\nNot sure, but I think you can try one of the following:\r\n- Docker: use a docker container if possible, it'll make things much simpler.\r\n- In your particular case, restarting the computer might help. In my case proto related errors disappeared after restarting. Also, don't forget to add bazel to `PATH`."", b'Thanks @AdityaKane2001  for the reply. @old-school-kid could u check if this error is caused by any local env issue?', b""> In your particular case, restarting the computer might help. In my case proto related errors disappeared after restarting. Also, don't forget to add bazel to PATH.\r\n\r\nTests start and actually first 106 out of 165 of them pass too. So I dont think there is an issue with local env. And I restarted my computer but didn't help. \r\n\r\n> Docker: use a docker container if possible, it'll make things much simpler.\r\n\r\nWill try to do this but isn't it supposed to build locally in a Windows machine."", b""> Will try to do this but isn't it supposed to build locally in a Windows machine.\r\n\r\nThings are not really supported well on Windows. Hence the recommendation. ""]",open,2022-01-05 17:51:08,
https://api.github.com/repos/keras-team/keras/issues/15857,b'Performance regression for FP16 Relu kernel from TF2.6 to TF2.7',"[b'Triage notes: Assign to Reed for FP16 related quesitons.', b""I cannot reproduce on a Titan V with CUDA 11.2.2 and cuDNN 8.1.1. With both TF 2.6 and 2.7, the training throughput is roughly 180 ms/s, and looking in the Tensorboard profile, relu takes approximately the same amount of time, although I do see the new MLIR kernel in TF 2.7. Note in your example I had to add the line `x_train = np.random.normal(size=(60000, 6400))` to define `x_train`.\r\n\r\nCan you clarify exactly what GPUs and CUDA versions you are using? Also, can you try running the following dedicated relu benchmark to see if TF 2.7 is slower?\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\nx = tf.random.normal((8192, 8192), dtype=tf.float16)\r\np = tf.constant(0.)\r\ntf.nn.relu(x)  # Warmup\r\nstart = time.time()\r\nfor _ in range(1000):\r\n  tf.nn.relu(x)\r\n(p + 1.).numpy()  # Synchronize GPU\r\nend = time.time()\r\n\r\nprint(f'Time taken: {end - start}')\r\n```"", b'I am running on V100x8 with cuda-11.5 and cudnn 8.3.1. For the relu benchmark above, I still see difference. In TF2.7, time taken is 0.43 compared to 0.34 in TF2.6. Nsys profile is shown below.\r\n\r\n Time(%)  Total Time (ns)  Instances  Average (ns)  Minimum (ns)  Maximum (ns)  StdDev (ns)                                                  Name                    \r\n -------  ---------------  ---------  ------------  ------------  ------------  -----------  ----------------------------------------------------------------------------------------------------\r\n    99.7        447310843       1001      446864.0        428189        472509      13013.3  Relu_GPU_DT_HALF_DT_HALF_kernel         \r\n\r\n\r\n    99.7        345094128       1001      344749.4        340638        352797       1722.4  void Eigen::internal::EigenMetaKernel<Eigen::TensorEvaluator<const Eigen::TensorAssignOp<Eigen::Ten\xe2\x80\xa6\r\n', b'The benchmark prints the time, so no need to get the time from a profiler. In any case, I do not have a new enough driver for cuda 11.5, but i tried on cuda 11.4. On the relu benchmark on a Titan V, I got 0.465 seconds and 0.487 seconds on TF 2.6 and TF 2.7 respectively, meaning there is a 5% regression on TF 2.7. On a Titan RTX, I got 0.521 seconds and 0.599 seconds respectively, which is a 15% regression on TF 2.7.\r\n\r\nI could only reproduce a 15% regression, but this is still significant. @sherhut, can you take a look? I cannot assign the GitHub issue to you, probably because you not part of the Keras GitHub team.', b'@wenscarl, can you please update the repro script to fix the missing `x_train` definition? (as pointed out by @reedwm as well)\r\n', b""@tlemo It's updated. "", b'Thanks @reedwm for flagging. I indeed cannot edit the issue. @chsigg is currently looking into this. We have some changes inflight to produce better code.', b""This should be fixed now with the these two changes:\r\n\r\nhttps://reviews.llvm.org/D117219 [LLVM] vectorize vector loads\r\n[tensorflow/b826391](https://github.com/tensorflow/tensorflow/commit/b8263919112cb63e4389dbd0c7b39e27a30cc17d): unroll relu kernel\r\n\r\nThe following changes improve the generated code further, but don't change relu kernel performance because it's already memory bound:\r\n\r\n[tensorflow/271845a](https://github.com/tensorflow/tensorflow/commit/271845ad0af96b8fb4de6ade92acb0961fd31785): generate sm_80 code\r\nhttps://reviews.llvm.org/D117010: [MLIR] Mark arith.minf, arith.maxf as commutative\r\nhttps://reviews.llvm.org/D117011: [MLIR] Remove NaN from arith.maxf expansion\r\nhttps://reviews.llvm.org/D117101: [MLIR] Fold cmpf with NaN\r\nhttps://reviews.llvm.org/D117204: [NVPTX] Lower to fmax.NaN for sm_80+.\r\nhttps://reviews.llvm.org/D118126: [LLVM] instcombine into fmaximum (pending)\r\n\r\n@wenscarl, would you mind confirming?""]",open,2022-01-04 22:10:25,
https://api.github.com/repos/keras-team/keras/issues/15852,b'Unable to restore custom metric.',"[b'@sachinprasadhs Was able to reproduce this issue on colab using TF v2.7.0, please find the gist [here](https://colab.sandbox.google.com/gist/sushreebarsa/a2e34b07d25b1b92d00e4cad669aaa04/keras15852.ipynb).Thank you! ', b'@saddamhijazi This was resolved in recent `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/a3204815dd9750a790f29a952e4ede9e/textvectorization.ipynb) is a gist for reference.\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!']",open,2022-01-04 08:28:20,
https://api.github.com/repos/keras-team/keras/issues/15846,"b""model doesn't learn when using shuffle='batch'""","[b'Attaching the colab [Gist](https://colab.research.google.com/gist/sachinprasadhs/11a8d7b648b31600afec15360cc48fbc/untitled471.ipynb) reproducing the reported behavior.', b'Triage notes: clearly a bug on our side. Need to assign to a proper owner.']",open,2021-12-30 22:01:07,
https://api.github.com/repos/keras-team/keras/issues/15845,b'text.tokenizer not removing filters char from text',[b'Triage notes: Assign to Matt for preprocessing related issues.'],open,2021-12-30 21:40:35,
https://api.github.com/repos/keras-team/keras/issues/15843,b'model.fit excess memory usage with GPU',"[b'@sachinprasadhs Was able to reproduce the issue in TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/kumariko/52c1dc589e72116c5c6242591fbd7661/github_53577.ipynb#scrollTo=2GGz3DqX6ATy).Thanks!', b'Which memory are you comparing, the RAM usage or the GPU memory usage between those tests?\r\nCould you also provide what is the difference in memory you see for each tests in both CPU and GPU for 10 epochs. Thanks!', b'I am comparing CPU memory. The difference is almost 10x', b""I don't see the excess memory usage from GPU for the examples provided, instead the GPU is consuming the less RAM compared to it's CPU equivalent, which is expected. Please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/30e36ed1b6f8a217f38ba5cdcf29b72c/github_53577.ipynb#scrollTo=X08-wUOj2yAf) for detailed usage. I'm printing the total ram consumed after each example/cell, you can check the difference in mb with the previous example.""]",open,2021-12-30 14:48:21,
https://api.github.com/repos/keras-team/keras/issues/15842,b'TripletSemiHardLoss produces only NaNs when training with a TPU',"[b'@sachinprasadhs Was able to reproduce the issue in TF v2.7.0 ,please find the [gist](https://colab.research.google.com/gist/kumariko/69901db667512210a0f3f9d2e721f628/tripletsemihardloss-fails-on-tpu-but-not-cpu-gpu.ipynb#scrollTo=KDK8BIYTjelZ) here.Thanks!', b""Shouldn't this bug be filed to tf.addon repository?""]",open,2021-12-29 18:30:27,
https://api.github.com/repos/keras-team/keras/issues/15839,b'GET and SET the accelerator device (cpu-gpu-tpu) more concise. ',"[b""Isn't it possible to have such a feature? ""]",open,2021-12-28 15:18:26,
https://api.github.com/repos/keras-team/keras/issues/15837,"b'""ValueError: Layer ""model_1"" expects 1 input(s), but it received 2 input tensors"" as more than one face appears in frame. Real Time Mask Detection System'","[b'Hi @Alex-Zab \r\nI assume your keras model `maskNet` only takes 1 image as input. But it appears `frame` has two faces and thus `faces` becomes a list of two images. Instead of calling `preds = maskNet.predict(faces)` you can  call `preds.append(maskNet.predict(face))` inside the loop. So you pass one image and also the overhead of checking len(faces) is removed.\r\nHope this helps\r\n', b'@old-school-kid Thanks for your reply! But, if I put `preds.append(maskNet.predict(face))` instead of `preds = maskNet.predict(faces)`, my program crashes with another error - \r\n```\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\MD\\test_mask_video.py"", line 106, in <module>\r\n    (noproperMask, mask, withoutMask) = pred\r\nValueError: not enough values to unpack (expected 3, got 1)\r\n```\r\nand \r\n```\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\MD\\test_mask_video.py"", line 100, in <module>\r\n    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\r\n  File ""C:\\Users\\MD\\test_mask_video.py"", line 67, in detect_and_predict_mask\r\n    preds.append(maskNet.predict(face))\r\nUnboundLocalError: local variable \'face\' referenced before assignment\r\n```', b'You have to first initialize the variable `face`. Like after `faces.append(face)` line you can call `preds.append(maskNet.predict(face))`. \r\nThe first error rose because of the second error.', b'@old-school-kid Ok, so I put `preds.append(maskNet.predict(face))` after `faces.append(face)`, commented line `#if len(faces) > 0:` and still receiving error - \r\n```\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\MD\\test_mask_video.py"", line 108, in <module>\r\n    (noproperMask, mask, withoutMask) = pred\r\nValueError: not enough values to unpack (expected 3, got 1)\r\n```', b'@Alex-Zab \r\nCan you please provide the code for the model. It appears that the pred has got one value while you are trying to access three values.', b""@old-school-kid I have `model.h5` file which is 11Mb, I can't input that much text. Also, after searching `pred` in file nothing was found."", b'@Alex-Zab \r\n`model.h5` contains the weight of the model. I just need a look of the model architecture. So can you provide the code for initializing the architecture? It appears that your model has 1 output only while you are trying to unpack 3.', b'@old-school-kid Can You, please, clarify, where should I find it?', b'@old-school-kid You can take a look at all of the files in here - https://github.com/Nitin1901/Face-mask-detection']",open,2021-12-27 16:06:18,
https://api.github.com/repos/keras-team/keras/issues/15832,b'[TF] Loading a Keras model which contains __ne__ or __eq__ operators fails',"[b""Okay, I found the issue. It was `Layer._call_fn_args`, as suspected. For some reason the patch I applied for `Layer._call_fn_args` didn't work, but it does now.\r\n\r\nAs you can see [here](https://github.com/keras-team/keras/blob/v2.7.0/keras/engine/node.py#L178), when serializing a node (happens while saving a Keras model), the function explicitly calls `_call_fn_args`, which filters out the `self` argument."", b'Mentioning @omalleyt12, as you\'re the last one who\'ve touched this function (at least it says so in the TF source control).\r\nI\'m curious about [this comment](https://github.com/keras-team/keras/blob/v2.7.0/keras/engine/base_layer.py#L3007). It implies that `self` appears in `self.call`\'s argument list in case a decorator has been applied. However, in most cases `self.call` is a bound method, which means it should contain a `self` argument. So this is the ""likely"" code path.\r\n\r\nThis makes me thing the most plausible solution for this issue is just changing the function signatures in the TF repository.\r\nSomething like `def tensor_equals(self, other)` -> `def tensor_equals(lhs, rhs)`.', b'@sachinprasadhs Was able to reproduce the issue in TF v2.7.0 ,please find the gist [here](https://colab.sandbox.google.com/gist/sushreebarsa/60e61e020c086b758ca0a20b40fec90c/keras_load_model_bug.ipynb#scrollTo=oKMnt8y-fUbq).Thanks!', b'Thanks for the issue! Would you like to submit a PR to fix the issue? Thanks!', b""Yeah, I can open a PR to fix this.\r\nI'm not sure about the correct solution though.\r\nWhat do you think about changing the function signature in TF?\r\nIs there any other way you think we can solve this in Keras?"", b""Thanks! You mentioned \r\n\r\n```These functions' first argument name is self, which gets lost somewhere. I was able to change the argument name for these functions, and make Keras load the model successfully.```\r\n\r\nCould you paste the changed function signature here? If I understand correctly, your fix is to force keeping the `self` argument?""]",open,2021-12-22 15:39:12,
https://api.github.com/repos/keras-team/keras/issues/15822,b'Obtained baseline accuracy for ResNet50v2 is different than officially reported',"[b'Is there any update for this bug?', b'@gcunhase have you checked on some small-scale data set like mnist/cifar-10/100? Have you faced the same abnormality? ']",open,2021-12-21 01:57:08,
https://api.github.com/repos/keras-team/keras/issues/15818,b'embedding_constraint does not work',"[b'The application in mind is an embedding matrix with a skew distribution for entries, that is also being regularized. The regularization is applied to the whole matrix and can drive rare entries to zero, the constraint (assuming it is also applied to the whole matrix) provides control over this. One can try activity regularization, it may work for some use cases, but the constrain may work better for others.', b'@sachinprasadhs Was able to reproduce this issue in TF v2.7.0 and tf-nightly , please find the gist [here ](https://colab.sandbox.google.com/gist/sushreebarsa/39dcb248b3a8cbf30f8ca907b005b077/kerasgist15818.ipynb#scrollTo=U1RX0hMFeqxh)for reference.Thanks!', b'this looks like a bug, I will take a closer look into it, thanks for reporting!', b'It seems that this is an intended behavior: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#variable_constraints_2\r\n\r\nAlthough it seems incorrect to not being able to have constraint on the embedding layer. Will need to try removing this check  and run some tests.']",open,2021-12-20 19:34:24,
https://api.github.com/repos/keras-team/keras/issues/15816,b'Add ResNeSt to keras.applications',"[b'@innat Please refer this implementation (https://github.com/RichardXiao13/TensorFlow-ResNets). ', b""@dathudeptrai no, it's a 3rd party implementation and comparatively much better tf-implementation is already mentioned above. "", b'@innat The implementation I mentioned above has almost the same structure and coding style as the models in tf.keras.applications', b""@dathudeptrai you're missing the point. I'm not looking for model implementation. Please read the first post. "", b""Triage note: We'll consider this once KerasCV is ready which we believe is a better place for the application to live."", b'@rchao is there any possibility that all the models under `keras.applicaitons` will move to the `keras-cv` project? ', b'@QiaoranC @RichardXiao13', b'@LukeWood do you have an answer to the `keras-cv` question?', b'> @LukeWood do you have an answer to the `keras-cv` question?\r\n\r\nif this is referring to:\r\n""@rchao is there any possibility that all the models under keras.applicaitons will move to the keras-cv project?""\r\n\r\nI believe that the answer to this is that in the long term: yes.  In the short/mid term: probably no as it would break many users.\r\n\r\nThe code of applications will likely exist in both locations for a period of time, with some small differences (i.e. defaulting the weights argument to None for applications instead of `imagenet`)', b""I'd personally be in favor of including this in KerasCV instead of keras.applications. This would allow us to avoid duplicating some code. @fchollet may have some thoughts here as to where the best place to contribute this in the near term would be.""]",open,2021-12-20 12:25:20,
https://api.github.com/repos/keras-team/keras/issues/15813,b'Saved model evaluate and validation always giving 0.50 accuracy despite reaching 97% on exact same data during training',"[b'@aliencaocao This is not a bug. The discrepancy is due to the `base_model` used in the model development. When i replaced `EfficientNetB0` with `MobileNetV2`, it gave very high accuracy for training and validation. Also, gave very high accuracy after reloading the model. \r\n\r\nThe discrepancy could be due to layers (such as normalization layer) used in the model. Another thing is try to freeze the base model as the data is very small. Alternatively, you could also update architecture.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/757d8abaaf3379031042770b23ddd7d9/untitled1151.ipynb) is a gist for reference. Thanks\r\n\r\nPlease verify once close the issue. [TF Forum](https://discuss.tensorflow.org/tags) or stackoverflow is better place to discuss more about this model architecture.']",open,2021-12-20 08:59:37,
https://api.github.com/repos/keras-team/keras/issues/15811,b'F2 Score and mAP metric for object detection model. ',"[b'@Suzan009,\r\n\r\nCan you check below metrics and  let us know if it helps you? \r\n\r\nF2 Score : [tfa.metrics.FBetaScore ](https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/FBetaScore) with`beta=2.0` \r\n\r\nmAP:  [tfr.keras.metrics.MeanAveragePrecisionMetric\r\n](https://www.tensorflow.org/ranking/api_docs/python/tfr/keras/metrics/MeanAveragePrecisionMetric)\r\n\r\nThanks!\r\n\r\n\r\n\r\n\r\n', b""@chunduriv \r\n\r\nactually no. check this,  [tfa.metrics.FBetaScore](https://github.com/tensorflow/addons/blob/v0.15.0/tensorflow_addons/metrics/f_scores.py#L26-L32), it clearly states and it's not suitable for object detection. And same as this one too [tfr.keras.metrics.MeanAveragePrecisionMetric](https://www.tensorflow.org/ranking/api_docs/python/tfr/keras/metrics/MeanAveragePrecisionMetric#compute_output_shape)\r\n\r\nPlease correct me with a working example if I miss something. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"", b'You can find the details in the supported object detection evaluation protocols [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/evaluation_protocols.md).', b'Please note that I need keras implementation so that I can integrate it conveniently into my pipeline. That repo is a complete mess for me. I also loss work motivation when I see that repo, no offense. ', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n""]",open,2021-12-19 12:22:36,
https://api.github.com/repos/keras-team/keras/issues/15810,b'Using batch-wise maximum sequence lengths for padding',"[b'This can be done quite elegantly if you work with a `tf.RaggedTensor`.\r\n\r\n```python\r\nbatch = [\r\n    [""a"", ""b"", ""c""],\r\n    [""a"", ""b"", ""c"", ""d"", ""e""],\r\n    [""a"", ""b""]\r\n]\r\nragged_batch = tf.ragged.constant(batch)\r\n\r\npad_value = tf.constant(""[PAD]"")\r\npadded_batch = ragged_batch.to_tensor(default_value=pad_value)\r\n```\r\n\r\n`padded_batch` then becomes\r\n```\r\n<tf.Tensor: shape=(3, 5), dtype=string, numpy=\r\narray([[b\'a\', b\'b\', b\'c\', b\'[PAD]\', b\'[PAD]\'],\r\n       [b\'a\', b\'b\', b\'c\', b\'d\', b\'e\'],\r\n       [b\'a\', b\'b\', b\'[PAD]\', b\'[PAD]\', b\'[PAD]\']], dtype=object)>\r\n```\r\n\r\nYou can do the same thing with `keras.layers.TextVectorization` if you are fine with the inputs being sentences and having the layer handle splitting and lookup. Padding value will always be `0`.\r\n```python\r\nbatch = tf.constant([\r\n    ""a b c"",\r\n    ""a b c d e"",\r\n    ""a b""\r\n])\r\nvec = TextVectorization(\r\n    vocabulary=[""a"", ""b"", ""c"", ""d"", ""e""],\r\n    output_mode=""int"",\r\n    output_sequence_length=None,\r\n)\r\nbatch = vec(batch)\r\n```\r\n`batch` then becomes\r\n```\r\n<tf.Tensor: shape=(3, 5), dtype=int64, numpy=\r\narray([[2, 3, 4, 0, 0],\r\n       [2, 3, 4, 5, 6],\r\n       [2, 3, 0, 0, 0]])>\r\n```', b'@chjort this is not padding to the maximum sequence length present in a batch. All the batches (in your example) still have uniform sequence lengths. Consider the following figure to understand what I am trying to mean:\r\n\r\n<img width=""596"" alt=""variable_length_batch"" src=""https://user-images.githubusercontent.com/22957388/147175705-3613c83e-dea9-46ac-8af1-49e65a1a11a1.png"">\r\n \r\nOn the left-hand side, we have a bunch of sequences. After batching, we can notice that a batch of sequences has been padded (when necessary) to the maximum sequence length particular to that batch (right-hand side). ', b'Well this can be achieved using my first suggestion with the `tf.RaggedTensor` and a `tf.data.Dataset`:\r\n\r\n```python\r\ndata = [\r\n    [1, 2, 3, 4, 5],\r\n    [1, 2, 3],\r\n    [1, 2, 3, 4, 5, 6],\r\n    [1, 2, 3, 4, 5],\r\n    [1, 2],\r\n    [1, 2, 3, 4],\r\n    [1, 2, 3, 4, 5, 6, 7],\r\n    [1, 2, 3],\r\n    [1],\r\n    [1, 2, 3],\r\n    [1, 2, 3, 4, 5, 6],\r\n    [1, 2, 3, 4, 5],\r\n    [1, 2],\r\n    [1, 2, 3, 4, 5, 6, 7],\r\n    [1, 2, 3, 4, 5],\r\n]\r\ndata_ragged = tf.ragged.constant(data)\r\ndataset_ragged = tf.data.Dataset.from_tensor_slices(data_ragged)\r\ndataset_ragged = dataset_ragged.batch(5)\r\n\r\n\r\ndef batch_pad(batch: tf.RaggedTensor):\r\n    pad_value = 0\r\n    return batch.to_tensor(default_value=pad_value)\r\n\r\n\r\ndataset_padded = dataset_ragged.map(batch_pad)\r\n\r\nfor batch in dataset_padded:\r\n    print(batch.shape)\r\n```\r\nyielding the following batch shapes:\r\n```\r\n(5, 6)\r\n(5, 7)\r\n(5, 7)\r\n```\r\nexactly like in your above figure.']",open,2021-12-19 10:41:03,
https://api.github.com/repos/keras-team/keras/issues/15808,"b'Add averaging methods for classification metrics(Precion, Recall and F-score), in tensorflow.keras or tensorflow-addons'",[],open,2021-12-18 13:35:38,
https://api.github.com/repos/keras-team/keras/issues/15807,b'Memory leak in saving and loading a keras model containing CategoricalEncoding and Lookup layers',"[b'@AdrianFuchsData Sorry for the late response!\r\nI tried to replicate the issue on colab using TF v2.7.0,  tf-nightly (2.9.0.dev20220126) and faced different outcome.Could you please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/13d66848ea802cc3f36c67f08ee6acb0/15807.ipynb) for reference and confirm the same ? \r\nThanks!']",open,2021-12-17 09:43:42,
https://api.github.com/repos/keras-team/keras/issues/15801,b'Fix to resolve repeated namescope issue',[],open,2021-12-16 18:09:24,
https://api.github.com/repos/keras-team/keras/issues/15800,b'Surface encoding to TextVectorization layer',"[b'@seanmor5 ,\r\nCan you please share a reproducible code that supports your statement so that the issue can be easily understood? Thanks!', b""Sure, this example raises an error:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# Example of a character that differs between utf-8 and latin-1,\r\n# If you try with something like basic ascii you wouldn't catch\r\n# this because they're (I think) mostly encoded exactly the same\r\ncorpus = [chr(0xa9).encode('latin-1')]\r\n\r\nvectorizer = tf.keras.layers.TextVectorization()\r\nvectorizer.adapt(corpus)\r\n\r\nvectorizer.get_vocabulary()\r\n```\r\n\r\n```\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\n<ipython-input-33-aac8958f9c88> in <module>()\r\n      9 vectorizer.adapt(corpus)\r\n     10 \r\n---> 11 vectorizer.get_vocabulary()\r\n\r\n4 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/compat.py in as_text(bytes_or_text, encoding)\r\n    107     return bytes_or_text\r\n    108   elif isinstance(bytes_or_text, bytes):\r\n--> 109     return bytes_or_text.decode(encoding)\r\n    110   else:\r\n    111     raise TypeError('Expected binary or unicode string, got %r' % bytes_or_text)\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xa9 in position 0: invalid start byte\r\n```"", b'Regarding @seanmor5 stated issue,\r\nThe issue resides in the code: https://github.com/keras-team/keras/blob/f1e9c76675981ee6683f54a3ce569212d551d12d/keras/layers/preprocessing/text_vectorization.py#L448 \r\nOn this code we need to add extra argument to explicitly describe text encoding type. currently the code sets up only to utf-8 refer\xef\xbc\x9a refer https://github.com/keras-team/keras/blob/f1e9c76675981ee6683f54a3ce569212d551d12d/keras/layers/preprocessing/string_lookup.py#L321\r\nhttps://github.com/keras-team/keras/blob/f1e9c76675981ee6683f54a3ce569212d551d12d/keras/layers/preprocessing/string_lookup.py#L322\r\n\r\nIf you are fine with adding extra encoding param i can go on modify the code and raise the PR for this issue.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Thanks for filing! I believe the solution proposed by @hockeyjudson is correct. We should add an encoding parameter to the TextVecotorization layer, include it in the config, and mirror it down to the underlying StringLookup layer.\r\n\r\nMarking this as open for contributions.']",open,2021-12-16 15:45:30,
https://api.github.com/repos/keras-team/keras/issues/15799,b'Improve `saved_model.load.get_config`',"[b""@MiWeiss Can you please check @fchollet's comments and keep us posted ? Thanks!"", b""As indicated by @fchollet, this is missing an adaption of the test which checks for the raised exception to also check for the new message, which should be reasonable quick.\r\n\r\nHowever, it appears that not only there is no such test, but moreover the file `keras.saving.saved_model` has no test class yet. At the moment, I do not have sufficient spare time to familiarize myself enough with saved_model internals to set up such a class, Thus, @fchollet @gbaned feel free to close or keep open as you prefer. \r\n\r\np.s. I'm not too familiar with keras's internals. If there is a corresponding test, I'm happy for a quick pointer.""]",open,2021-12-16 14:18:17,
https://api.github.com/repos/keras-team/keras/issues/15797,b'RNN predict() raised error if shape is not the same as training set for the first time',"[b'@henrysky,\r\n\r\nI tried running the stand alone code with `TF 2.7.0`, but I am getting the same `ValueError` while running `model.predict(x_test[0:1, :10, :])` second time as well. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/28e4f0dbe625cb45e75d5dc6967edcf3/tf_rnn_predict_issue.ipynb). Thanks!', b'@sanatmpa1 I just ran your gist notebook and second time no error as expected. Please check, thanks!\r\n![image](https://user-images.githubusercontent.com/28623434/146414763-b8b7b1fd-3726-41f2-bfaf-007a76fb7218.png)\r\n', b""@henrysky,\r\n\r\nTried running again and as you said the error didn't occur while running second time. Here's the updated [gist](https://colab.sandbox.google.com/gist/sanatmpa1/728c94ea32ba10df5d08b49f9396b9ca/tf_rnn_predict_issue.ipynb)""]",open,2021-12-15 21:04:47,
https://api.github.com/repos/keras-team/keras/issues/15796,b'Saving a keras.layers.RNN with a custom cell substitutes custom initial states with zeros',"[b'@sachinprasadhs Was able to reproduce this issue in TF v2.7.0 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/f7297f1e467a0b2e80aebf33af17d4d5/untitled16.ipynb) for reference.Thanks!', b""It seems that despite the fact that during saving the model several calls are made to `get_initial_state()`, the loaded model still uses the initial state provided during model construction, and thus works correctly. Still, I think that this call is confusing and there might be other bugs lurking around, so it's worth investigating why these calls are made in the first place.\r\n\r\nPlease see a Colab notebook [here](https://colab.research.google.com/gist/dniku/6ab3eec117c63b0eeb79c734dbd5f9ab/keras_rnn_layer_initial_state_redundant_call.ipynb).""]",open,2021-12-15 20:51:27,
https://api.github.com/repos/keras-team/keras/issues/15795,b'Plotting a model (with model_to_dot) fails if the inputlabels/outputlabels contain brackets',"[b'@cBournhonesque Can you please share a simple standalone code to reproduce the issue? Thanks!', b'You were right to ask!\r\nIt looks like when the input is a dictionary, the resulting label is parsed correctly by graphviz, so the graph works:\r\n```\r\nfrom typing import Dict, Tuple\r\nimport tensorflow as tf\r\n\r\nclass DictLayer(tf.keras.layers.Layer):\r\n    def call(self, inputs: Dict[str, tf.Tensor]) -> tf.Tensor:\r\n        return tf.concat(list(inputs.values()), axis=1)\r\n\r\ninputs={""a"": tf.keras.Input(name=""a"", shape=(1), dtype=tf.float32),\r\n        ""b"": tf.keras.Input(name=""b"", shape=(1), dtype=tf.float32)}\r\noutputs=DictLayer()(inputs)\r\nmodel = tf.keras.Model(\r\n    inputs=inputs,\r\n    outputs=outputs,\r\n)\r\ntf.keras.utils.plot_model(model,\r\n                          to_file=\'model.png\',\r\n                          show_shapes=True,\r\n                          show_dtype=True,\r\n                          show_layer_names=True)\r\n```\r\n![image](https://user-images.githubusercontent.com/8112632/146255072-a999c833-5940-4350-ba2e-81ec5cfece8b.png)\r\n(the brackets are removed in the label name, so graphviz must have interpreted them in some way. I think it would be better if the bracket characters were escaped even in that case)\r\n\r\nThe error comes when the input is a tuple of Dicts, that\'s where you have brackets in the label names:\r\n\r\nExample code:\r\n```\r\nfrom typing import Dict, Tuple\r\nimport tensorflow as tf\r\n\r\nclass DictLayer(tf.keras.layers.Layer):\r\n    def call(self, inputs: Tuple[tf.Tensor, Dict[str, tf.Tensor]]) -> tf.Tensor:\r\n        tensor_input, dict_input = inputs\r\n        return tf.concat(list(dict_input.values()), axis=1)\r\n\r\ninputs={""a"": tf.keras.Input(name=""a"", shape=(1), dtype=tf.float32),\r\n        ""b"": tf.keras.Input(name=""b"", shape=(1), dtype=tf.float32)}\r\noutputs=DictLayer()((inputs[""a""], inputs))\r\nmodel = tf.keras.Model(\r\n    inputs=inputs,\r\n    outputs=outputs,\r\n)\r\ntf.keras.utils.plot_model(model,\r\n                          to_file=\'model.png\',\r\n                          show_shapes=True,\r\n                          show_dtype=True,\r\n                          show_layer_names=True)\r\n```\r\n\r\nError: `Error: bad label format dict_layer|DictLayer|float32\\\\n|{input:|output:}|{{((None, 1), {\'a\': (None, 1), \'b\': (None, 1)})}|{(None, 2)}}\\n""`\r\n                          \r\nThe error goes away after escaping the brackets, or removing them\r\n\r\n\r\n\r\n', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Please do not close the issue.']",open,2021-12-15 18:56:52,
https://api.github.com/repos/keras-team/keras/issues/15794,"b'Saving & loading a keras.layers.RNN with a custom cell with multiple inputs, states, and outputs throws an exception'","[b'@dniku I can reproduce the issue when saving the model with `h5` format. However, there is no issue if you save the model in `tf` format. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/9cc88d13715457c417cf8e64df39918e/keras_rnn_layer_saving_bug.ipynb). Thanks!\r\n\r\nwe will check why this is an issue with `h5` format. Thanks!']",open,2021-12-15 14:49:53,
https://api.github.com/repos/keras-team/keras/issues/15792,b'tf.compat.v1.keras.backend.get_session gives erroneous deprecation warning',[b'I am able to reproduce the issue and I am getting the erroneous warning even after using `tf.compat.v1.keras.backend.get_session`. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/fa73dba013dcb85a0cc56af1225f11bd/33182.ipynb). Thanks!'],open,2021-12-15 13:42:58,
https://api.github.com/repos/keras-team/keras/issues/15790,b'Batch Normalization with virtual_batch_size not equal to None not implemented correctly for inference time',"[b'I am able to reproduce the issue reported in colab with `TF 2.7.0`. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/36a5739b57dd0aed170a2a158f9f1428/15790.ipynb). Thanks!', b""Triage: keeping this open for contribution as the team doesn't currently have the bandwidth."", b'@rchao @ccurro \r\nCorrect me if I am wrong. Arent ghost batches like mini batches. That is the ghost batched will be subset of the batch?\r\nSo the batch size must be divisble by ghost batch number which is exactly what is recommended by keras-team in this [line of comment](https://github.com/keras-team/keras/blob/2c48a3b38b6b6139be2da501982fd2f61d7d48fe/keras/layers/normalization/batch_normalization.py#L759)\r\nSo here the error is raised since 1 cannot be divided by 32.', b'@old-school-kid That is true when `training=True` but when `training=False` you should be able to have any batch size. `virtual_batch_size` should have no effect during inference. ']",open,2021-12-15 00:12:52,
https://api.github.com/repos/keras-team/keras/issues/15789,b'Tuner tutorial rebuilds model rather than saving it',[],open,2021-12-14 11:58:05,
https://api.github.com/repos/keras-team/keras/issues/15783,b'Not able to restore the model from config with tf.einsum operation',"[b'I am able to reproduce the issue in colab with `TF 2.7.0`. Please find the [gist attached](https://colab.sandbox.google.com/gist/sanatmpa1/0fd98ad625058e67ab2e3967208bbc7f/15783.ipynb)\r\n\r\n', b'For all passing by who needs to use `einsum` as we do: possible workaround is to use a custom layer.\r\n```\r\nclass EinsumLayer(tf.keras.layers.Layer):\r\n    """"""\r\n    Layer wrapping a single tf.einsum operation.\r\n\r\n    Usage:\r\n    x = EinsumLayer(""bmhwf,bmoh->bmowf"")((x1, x2))\r\n    """"""\r\n\r\n    def __init__(self, equation: str):\r\n        super().__init__()\r\n        self.equation = equation\r\n\r\n    def call(self, inputs, *args, **kwargs):\r\n        return tf.einsum(self.equation, *inputs)\r\n\r\n    def get_config(self):\r\n        return {""equation"": self.equation}\r\n```\r\n\r\nFYI, I also found out that models saved in 2.3.4 works. Problem occurred probably when serialization wrapper changed from `TfOpLambda` to `TensorFlowOpLayer`.)']",open,2021-12-13 05:34:02,
https://api.github.com/repos/keras-team/keras/issues/15782,"b'Scoring for metrics and losses, '","[b'Triage notes: we might want to add a keras.io example for how to choose a proper loss/metric.', b'This answer gives references for examples of using strictly proper scoring rules:    \r\nhttps://stats.stackexchange.com/questions/279053/how-to-see-how-good-my-probability-estimations-are/279056#279056\r\n\r\nHere are some of the references it gives:\r\n\r\n- https://www.tandfonline.com/doi/abs/10.1198/016214506000001437\r\n- https://www.tandfonline.com/doi/abs/10.1198/jbes.2010.08110\r\n- https://www.sciencedirect.com/science/article/abs/pii/S0304407611000807 ', b'> Some rules to consider:\r\n\r\nBrier/quadratic scoring rule\r\nHyvarinen scoring rule\r\nSpherical Scoring Rule\r\nLogarithmic scoring rule (log-probability)\r\n\r\n@EngrStudent if I am not wrong this is a feature-request to add these losses in keras, right?']",open,2021-12-13 03:28:42,
https://api.github.com/repos/keras-team/keras/issues/15780,b'Updating the ResNet-* weights',"[b""Notes from triage:\r\n\r\nIf we don't have a contributor we probably will not have time for this.  Also, before a contributor prepares a PR we'd like to see if @fchollet thinks this is a good idea."", b'This issue proposes the introduce of a family of `tf.keras.applications.ResNet50RS` models with new implementations and new checkpoints. This sounds good to me, but we won\'t have cycles to work on it, so it would have to be contributed by the community. Marking as ""contributions welcome"".', b'Relevant thread: https://discuss.tensorflow.org/t/resnet-rs-rewritten-in-tensorlfow-keras/5733/5. ', b'I can contribute on this, @sayakpaul Please assign it to me.', b'@sebastian-sz will be working on it. Also, I cannot assign tasks to anyone. Only Keras team members can do that. ', b""@sayakpaul I'm kind of confused whether we should work on it or wait for the implementation from `keras-cv`. There seems to be [some works](https://github.com/keras-team/keras-cv/issues/36) going on with it. Reproducible models trained from scratch in Keras would be better than the converted ones.\r\n\r\nAlso I did not fully understand @bhack 's [commment on TF forum](https://discuss.tensorflow.org/t/resnet-rs-rewritten-in-tensorlfow-keras/5733/19?u=sebastian-sz) - Is this a green light or a red light for such PR?"", b'@sebastian-sz I think that having reusable components it Is better then having an embedded trainabile model that It Is better then just the converted weights.\n\nSo I think that reusable components it is a little bit low level then all the other things.\n\nBut as we have not defined a public process to produce validated weights from the community using these new reusable components I think that currently this roadmap need to be exposed by the internal teams. E.g. when the reusable components will be ready who Is going to produce validated weights?\n\nI think that, more in general, by a community point of view this is partially similar to the Tensorflow Model garden process.\n\nSee:\nhttps://discuss.tensorflow.org/t/a-tensorflow-model-garden-contributor-report/6993\n\n', b'This issue is tagged with ""Contributions welcome"". So, I am not sure if that arises any confusion. \r\n\r\nJust to clear some confusion, @sebastian-sz has already written the ResNet-RS model classes and then they were populated with the original weights. I think this does expose reusable components which are quite in the spirit of `keras.applications`. \r\n\r\nSebastian also provides code to validate the converted weights i.e., ImageNet-1k validation set scores. \r\n\r\nFurthermore, EfficientNetV2 models were contributed similarly if I am not wrong. There is also a standing PR on RegNets and from the EfficientNetV2 PR, I believe the Keras team will validate the weights on their end as well. \r\n\r\n@fchollet am I missing anything here?', b'IMHO when the reusable components are landing in Keras-cv it is not so clear to me what is the path of having reproducible training vs `keras.application` only availablity vs models garden.\n\nOf course we could duplicate our contribution paths as we want but I don\'t think It will  a clear and transparent approach for the community.\n \nProbably it is still a little bit clear for me with TFHUB only models as generally they are not so end-users ""manipulable"". \nBut if I remember correctly we had also a discussion on this specific topic at:\n\nhttps://discuss.tensorflow.org/t/modification-of-layers-in-hub-keraslayer/2508/8', b""> IMHO when the reusable components are landing in Keras-cv it is not so clear to me what is the path of having reproducible training vs keras.application only availablity vs models garden.\r\n\r\nI am also on the same boat there. \r\n\r\nBut that said, as for this PR, I do not think there's anything blocking @sebastian-sz to work on a ResNet-RS PR since it's already cleared by the Keras team as mentioned in https://github.com/keras-team/keras/issues/15780#issuecomment-1006828492. "", b""> > IMHO when the reusable components are landing in Keras-cv it is not so clear to me what is the path of having reproducible training vs keras.application only availablity vs models garden.\n> \n> I am also on the same boat there. \n> \n> But that said, as for this PR, I do not think there's anything blocking Sebastian to work on a ResNet-RS PR since it's already cleared by the Keras team as mentioned in https://github.com/keras-team/keras/issues/15780#issuecomment-1006828492. \n\nOk but we need to consider also that we have already official regular weights in:\n\nhttps://github.com/tensorflow/models/blob/master/official/vision/beta/MODEL_GARDEN.md#resnet-rs-models-trained-with-various-settings"", b'Sure thing. But I think Keras users would love to avail the flexibilities they get while working with `tf.keras.applications`. I find the official code harder to work with than pure Keras. ', b'> Ok but we need to consider also that we have already official regular weights in:\r\n> \r\n> https://github.com/tensorflow/models/blob/master/official/vision/beta/MODEL_GARDEN.md#resnet-rs-models-trained-with-various-settings\r\n\r\n@bhack there are two Issue with the mentioned repository:\r\n1. These weights don\'t work. I tried to use it when rewriting the models to Keras, also opened an Issue but it didn\'t attract much attention https://github.com/tensorflow/models/issues/10234\r\n2. The models building blocks are composed as `tf.keras.layers.Layer` subclasses rather than functional blocks like `keras.applications`. Please, correct me if I\'m wrong but this  limits functionality. In `keras.applications` I can access all model layers as such\r\n```python\r\nsome_layer, another_layer = [\r\n    backbone.get_layer(layer_name).output\r\n    for layer_name in [""block_4_middle_conv"", ""block_3_first_conv""]\r\n]\r\n```\r\nThe above will work for functional models with flat structure but I\'m afraid it might fail where blocks are implemented as `tf.keras.layers.Layer` subclasses. ', b""I suppose this why there is a Keras-cv effort to have a `more reusable` impl of things that we have embedded in models garden.\nBut in the end if we don't have a plan to contribute/expose Keras-cv trained models It couldn't create any conflict or resoruce wasting."", b'@sayakpaul Update from me: sorry I will not be able to submit this PR - I am very short on time recently.\r\n\r\nIf anyone wants to work on this (@spatil6 ) feel free to use my repository (I would appreciate mentioning me thought, if you do).']",open,2021-12-12 13:41:54,
https://api.github.com/repos/keras-team/keras/issues/15771,"b'Correcting documentation of padding=""same"" for conv layers'",[b'@sanatmpa1  Can you please resolve conflicts? Thanks!\r\n'],open,2021-12-09 17:16:00,
https://api.github.com/repos/keras-team/keras/issues/15760,"b'Different loss values in ""model.fit"" and ""print(the-loss)""'","[b'I was able to reproduce the issue on colab using TF 2.7 and TF-nightly(2.8.0-dev20211210).Please find the [gist here](https://colab.research.google.com/gist/chunduriv/eca717d57a6326fcd75f9913ffff778a/15760.ipynb) for reference.Thanks!', b'@chunduriv \r\nThank you for the reproduction survey.\r\nIs there anything I can do?', b'@DameNianch Quick question. Are you comparing `loss` from `model.fit` (which is around loss: 0.1063) and `print(""loss"", tf.keras.losses.MeanSquaredError()(z_est, z).numpy())` (which is around loss 3.5583981169095245e-16)? Thanks!\r\n\r\n', b'@jvishnuvardhan \r\n> Are you comparing loss from model.fit (which is around loss: 0.1063) and print(""loss"", tf.keras.losses.MeanSquaredError()(z_est, z).numpy()) (which is around loss 3.5583981169095245e-16)? \r\n\r\nYes!!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b'Hi, this seems to be an implementation error. One potential issue is the loss during training is aggregated from the whole epoch, so you should not compare it against the last batch. could you try write your code in custom training loop for debugging? It could provide more insights, thanks!', b'@chenmoneygithub \r\nThank you for your reply. I have two quesions.\r\n\r\n1. Is the ""last batch"" the final epoch? Or is it the second `check_model`?\r\n2. Is a ""custom training loop"" a loop that uses Gradient Tape?\r\n\r\nSince the initial value is the optimum solution, even if the loss during training is aggregated from the entire epoch, it feels strange that the loss is not almost zero. You can see from the first `check_model` that the initial value is the optimal solution.', b""1. Not the final epoch, but the last batch in the final epoch. Each epoch consists of a bunch of batches.\r\n2. Exactly, please check this tutorial: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\r\n\r\nYes, it looks strange, but maybe it's just because the last batch is easy to process, so writing a custom training loop should get us a more clear understanding."", b'@chenmoneygithub @jvishnuvardhan @chunduriv \r\nThank you for your helping.\r\nThis was a bug due to my use of `np.squeeze` in `check_model`. When I remove `np.squeeze` from `check_model`, both loss is not almost zero.  I misunderstood that tf.keras.losses.MeanSquaredError would do a proper cast internally. If you run into the same issue as this issue, I recommend adding `z = layers.Flatten () (z)`.']",open,2021-12-07 12:12:48,
https://api.github.com/repos/keras-team/keras/issues/15757,b'Support multiple validation sets in Model.fit',"[b'There are a couple of solutions for this:\r\n\r\n1) Use a callback to run over a set of eval dataset. You can use multiple for different eval datasets.\r\n2) Use as many `SidecarEvaluator` as you like, either in separate threads, or separate processes optionally on different machines.\r\n3) Call `Model.fit` without a validation data provided, followed by `Model.evaluate` calls each of which has different dataset used.\r\n\r\nHope this helps!']",open,2021-12-06 22:59:45,
https://api.github.com/repos/keras-team/keras/issues/15752,b'Allowing Input Tensors to be dynamic for Predictions when using the Subclass method for defining a model',"[b'@AshwinJay101, I tried to run your code on colab using TF2.7 and faced error\r\n\r\n   ```\r\nValueError: Exception encountered when calling layer ""custom_model"" (type CustomModel).\r\n    Could not find matching concrete function to call loaded from the SavedModel.\r\n```\r\n\r\nPlease find the [gist here](https://colab.research.google.com/gist/chunduriv/b48979958feb8fa4bcd2d71ff4fc3049/15752.ipynb) for reference. Thanks!', b""@chunduriv I mentioned that you get a similar error in TF2.7. If you notice, it mentions that it got inputs of shape (None, 512, 512, 6) while it expected inputs of the shape to be (None, 256, 256, 6)\r\n\r\nThis is the error message \r\n\r\n    Could not find matching concrete function to call loaded from the SavedModel. Got:\r\n          Positional arguments (1 total):\r\n            * (<tf.Tensor 'inputs:0' shape=(None, 512, 512, 6) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(None, 256, 256, 6) dtype=float32>)\r\n          Keyword arguments: {}\r\n    \r\n     Expected these arguments to match one of the following 2 option(s):\r\n    \r\n    Option 1:\r\n      Positional arguments (1 total):\r\n        * (TensorSpec(shape=(None, 256, 256, 6), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 256, 256, 6), dtype=tf.float32, name='inputs/1'))\r\n      Keyword arguments: {}\r\n    \r\n    Option 2:\r\n      Positional arguments (1 total):\r\n        * (TensorSpec(shape=(None, 256, 256, 6), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 256, 256, 6), dtype=tf.float32, name='input_2'))\r\n      Keyword arguments: {}\r\n    \r\n    Call arguments received:\r\n      \xe2\x80\xa2 args=(('tf.Tensor(shape=(None, 512, 512, 6), dtype=float32)', 'tf.Tensor(shape=(None, 256, 256, 6), dtype=float32)'),)\r\n      \xe2\x80\xa2 kwargs=<class 'inspect._empty'>``""]",open,2021-12-06 14:56:04,
https://api.github.com/repos/keras-team/keras/issues/15751,b'How to pass a switch python argument into input_signature when using tf.function? ',"[b'Not a developer here; if I understand it correctly, the Python (non-tf.Tensor) arguments can be used only when the `input_signature` is not explicitly specified. Quoting from https://www.tensorflow.org/api_docs/python/tf/function :\r\n> `input_signature` A possibly nested sequence of tf.TensorSpec objects specifying the shapes and dtypes of the Tensors that will be supplied to this function. If None, a separate function is instantiated for each inferred input signature.  If input_signature is specified, every input to func must be a Tensor, and func cannot accept **kwargs.\r\n\r\nIn other words, if `input_signature` is given, there is always just a single concrete function.\r\n\r\nSo the question is whether you must specify the `input_signature` -- in the past, it was needed if you wanted to support dynamic shapes with a single concrete function, but that can be alleviated today using the `experimental_relax_shapes`. Also, the `experimental_follow_type_hints` can play a part of the role of `input_signatures` -- it specifies that some arguments are always Tensors, but unfortunately not their size nor dtype.\r\n\r\n', b'Emmmm, what about the training argument?']",open,2021-12-06 11:25:12,
https://api.github.com/repos/keras-team/keras/issues/15730,b'multi-GPUs are under-utilized when using prediction on tensorflow_io dataset',"[b'@yuefengz could someone on the distribute team take a look at this?', b'Hi @kretes , could you try to grab a profile? https://www.tensorflow.org/guide/profiler', b'Hi @yuefengz , I did the profiling by running .start() and .stop() of the profiler around the .predict() method and I increased no of batches to 12 - [Colab](https://colab.research.google.com/drive/1uZucXZ_sQJjudYHLosWw6nHyguAP3KMJ?usp=sharing) for reference.\r\n\r\n[Profiler output](https://drive.google.com/file/d/1ZOop9ITv3uB7IaXhWkXulV-lF69XMM2J/view?usp=sharing) \r\n\r\nLet me know if you need anything else', b'@yuefengz have you been able to look at the profiler output and this issue?']",open,2021-12-01 08:49:54,
https://api.github.com/repos/keras-team/keras/issues/15721,"b'When calling self.add_weight, use self.trainable instead of defaulting to True.'",[],open,2021-11-29 20:06:09,
https://api.github.com/repos/keras-team/keras/issues/15715,b'Many metrics cannot handle predictions out of [0..1] range',"[b'@jvishnuvardhan, The issue was able to reproduce the issue on Colab using TF v2.7  and TF-nightly. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/5bb79d652fe2d3a2afb20c2a61d66378/15715.ipynb) for reference.Thanks!  ', b""@raftAtGit I think you forgot to add `sigmoid` activation for  a classification model with `binary_crossentropy` loss. When I updated your code with `sigmoid`, everything worked as expected. Please check the gist [here](https://colab.research.google.com/gist/jvishnuvardhan/9939bb986234ec6a1c7d3d949053f05d/15715.ipynb)\r\n\r\n`tf.keras.layers.Dense(1, activation='sigmoid', name='output',)`"", b'@jvishnuvardhan `sigmoid` activation for a classification model with `BinaryCrossentropy(from_logits=False)` loss is **fine**, no problem with that.\r\n\r\nproblem is, `BinaryCrossentropy(from_logits=True)` loss is not suitable with `sigmoid` activation as mentioned and suggested in the docs and in the below logs:\r\n```\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: UserWarning: ""`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?""\r\n```\r\n\r\n', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b':robot: ', b""Waiting for triage.\r\nSummary: Many metrics don't work with logits, but only probabilities. However, the user may want to use loss with from_logits=True, then they cannot use the metrics."", b'Triage notes:\r\nTo make metrics compatible with the loss with `from_logits=True`, we should add this `from_logits` to the metrics, too.', b'@fchollet Any thoughts on this feature?', b""Many of these metrics already implement the `thresholds` parameter, so they could be used as-is with `from_logits=True` if the `output in [0, 1]` constraint were removed.\r\n(`thresholds=.0, from_logits=True` is equivalent to `thresholds=0.5, from_logits=False`, as `sigmoid(.0) = 0.5`.)\r\n\r\nAnother interesting point is that logits are preferred and are used whenever available, even if `from_logits=False` (this happens in [losses.binary_crossentropy](https://github.com/keras-team/keras/blob/2c48a3b38b6b6139be2da501982fd2f61d7d48fe/keras/backend.py#L5129) and [losses.categorical_crossentropy](https://github.com/keras-team/keras/blob/2c48a3b38b6b6139be2da501982fd2f61d7d48fe/keras/backend.py#L4950)). So, from the user perspective, isn't always better to use `from_logits=False`? In that case, deleting the `from_logits` parameter from each and every loss/metric and defining an expected output (with/without activation) would solve this confusion."", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'\xf0\x9f\xa4\x96 ', b'Need a parameter `from_logits` in the official metrics classes.\r\n\r\nI found that bce with sigmoid is not stable in tf (2.4.4). So I prefer bce with logits.\r\n\r\nTherefore, it will be more friendly if there is a `from_logits` parameter in the official metrics classes.', b'@haifeng-jin \r\nCan a `contributions-welcome` ticket be added to this?', b""I would like to mention that in the official tensorflow documentation, that mistakes happen frequently, e.g. mixing `BinaryCrossentropy(from_logits=True)` with `metrics=['accuracy']`.\r\nHere are some of them:\r\n\r\n- https://github.com/tensorflow/tensorflow/issues/41413\r\n- https://github.com/tensorflow/tensorflow/issues/41933\r\n\r\nEven worse, not all people are aware of this, then they simply replace the fixed documentation and introduce the mistake again.\r\n\r\nAt this point I think it's pretty clear that we need to make the API more intuitive, and I believe adding `from_logits` to the metrics would make this clearer to the user.\r\n\r\nEDIT: I also see the same mistake in official keras documentation, e.g. mixing `BinaryCrossentropy(from_logits=True)` with `BinaryAccuracy()` with default threshold https://keras.io/guides/transfer_learning/""]",open,2021-11-28 14:16:26,
https://api.github.com/repos/keras-team/keras/issues/15714,b'Tensor dimension becomes None after using gather or boolean_mask in Tensorflow 2.0',"[b'@jvishnuvardhan, I was able able to reproduce the issue on Colab using `TF-nightly(2.8.0-dev20211201)`. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/61f7033fd1b3f1a6cf1a700b1f07ce30/15714.ipynb#scrollTo=hvMnb3gBkAe-) for reference.Thanks!']",open,2021-11-28 13:37:13,
https://api.github.com/repos/keras-team/keras/issues/15710,b'Keras Nadam optimizer behaves different to `ApplyAdamOp` with `use_nesterov` option',[],open,2021-11-27 03:14:20,
https://api.github.com/repos/keras-team/keras/issues/15708,b'Model stops training with variable-size dataset',"[b'@sachinprasadhs, I was able to reproduce the issue on Colab using `TF2.7` and `tf-nightly(2.8.0-dev20211201)`. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/f0cd2210dfa4d1462a0c518c5a370848/15708.ipynb#scrollTo=UKhWhvRLVvmp) for reference.Thanks!', b'You can use the solution mentioned [here](https://stackoverflow.com/questions/59864408/tensorflowyour-input-ran-out-of-data) to avoid warning and continue training.', b'I know a workaround, but that is very ugly: \r\n- measure dataset length by hands (i use bucketing, so len(dataset) is None and measuring length in my case takes around 30 min)\r\n- dataset = dataset.repeat(2).take(measured_len)\r\n\r\nThis is a bad solution. In my opinion model should continue training until reaches num_epochs even if some epoch has less batches then first one.\r\nDisplaying the number of steps remaining within an epoch is not as important as the completion of all epochs.', b'Waiting for triage.\r\nSummary:\r\nWhen the dataset has a different number of samples from epoch to epoch (the batch size are the same, the number of steps are different), the training will stop at a epoch, whose number of steps is different from the first epoch.', b""Thanks for reporting the issue - one solution is to use a `steps_per_epoch` that's large enough for the number of data in all epochs, and have the termination of an epoch rely on exhaustion of data (`OutOfRangeError`). Can you check if this works?""]",open,2021-11-26 06:00:40,
https://api.github.com/repos/keras-team/keras/issues/15705,b'Reorder channel layer for smooth native RGB - BGR',"[b'@piEsposito, The issue will move to closed status once the [PR](https://github.com/keras-team/keras/pull/15707) is merged. Thanks!', b""Hi @piEsposito \r\nIsn't this a specific use case of [permute layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute)?"", b""Hi,\nActually no. While permite layers rearranges the order of the axes and\nchanges the output shape while transposing the matrix, ReorderChannels keep\nthe same output shape put reorders the channels of a specific dimension.\n\nEm qui., 2 de dez. de 2021 \xc3\xa0s 19:15, Surya Prakash Mishra <\n***@***.***> escreveu:\n\n> Hi @piEsposito <https://github.com/piEsposito>\n> Isn't this a specific use case of permute layer\n> <https://www.tensorflow.org/api_docs/python/tf/keras/layers/Permute>?\n>\n> \xe2\x80\x94\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/keras-team/keras/issues/15705#issuecomment-985045480>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALLYRXWV5WWCFDXM2VEW323UO7VZZANCNFSM5IYXQA6A>\n> .\n>\n-- \n--\n*Pi Esposito | piesposito.github.io <http://piesposito.github.io>*\n"", b'Let me know if you undestood. But the `Permute` layer transposes the dimensions of the Tensor, while the proposed `ReorderChannels` performs a `tf.gather` operation to reorder the channels over a specific axis. ', b'Ah, yes I got it. Thank you @piEsposito ', b'@old-school-kid you are welcome. Thank you for giving me the opportunity to make that clear.  ', b""Let's move this conversation to keras-cv:\r\nhttps://github.com/keras-team/keras-cv/issues/20 @piEsposito ""]",open,2021-11-25 14:56:09,
https://api.github.com/repos/keras-team/keras/issues/15703,"b'Padding=""same"" documentation is incorrect in all Conv layers'","[b'@kkraoj,\r\n\r\nAs you pointed out, I can see that the `padding=same` preserves the input dimensions only when `stride=1`, and I can submit a PR to modify the documentation as \r\n\r\n> ""same"" results in padding with zeros evenly to the left/right or up/down of the input such that the kernel can fully cover the input""\r\n\r\nPlease confirm if that looks good, Thanks!', b'I confirm. That change looks good to me.', b'@kkraoj,\r\n\r\nI have submitted a [PR](https://github.com/keras-team/keras/pull/15771) addressing the doc string change. Thanks!']",open,2021-11-25 06:28:52,
https://api.github.com/repos/keras-team/keras/issues/15701,"b'__len__, __getitem__ and __data_generation gets called multiple times '","[b'Hi @sanatmpa1 - Thanks for taking up this issue. Is there anything wrong with the way I have implemented the class?', b'Hi, could you provide the logging for this issue? Like how many times those methods are called, and where they are getting called. \r\n\r\nIf possible, could you provide a reproducable example, so we can look closer into it, thanks!']",open,2021-11-25 01:19:23,
https://api.github.com/repos/keras-team/keras/issues/15699,b'Error when Saving model with data augmentation layer on Tensorflow 2.7',"[b'@moumed,\r\n\r\nCan you share a simple stand alone code to reproduce the issue from our end? Thanks!', b'I have the same problem. In fact the with TF2.7, saving and loading a model seems to have some issues.  ', b'> \r\n\r\n@sanatmpa1  the problem is not specific to any kind of data or code, just creating and compiling the model with data_augmentation_rgb and then saving the model raises the error, and when removing the data data_augmentation_rgb the error disappears.\r\n\r\nI tried to test the provided colab in tensorflow core documentation with the provided tf_flowersdataset: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb\r\nthe error appears when saving model with the option 1 of Making the preprocessing layers part of the model.', b'Hi @moumed \r\n\r\n> One reason could be that a stateful \r\n   object or a variable that the function depends on is not assigned to an attribute of \r\n   the serialized trackable object\r\n\r\nmeans you are using objects that arent serializable.\r\n\r\n> layers.RandomFlip(""horizontal""),\r\n    layers.RandomFlip(""vertical""),\r\n    layers.RandomRotation(0.5),\r\n    layers.RandomZoom(0.5),\r\n    layers.RandomContrast(0.5),\r\n\r\nAs a matter of fact you have passed arguments to the layers and those cannot be saved. The proper way to do this is to create a class for the model that inherits keras.Model or a class for the whole thing as a layer that inherits keras.layers.Layer and then define `get_config` and `from_config` methods. The documentation can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects). \r\n\r\nA workaround is [saving the model as a `.h5` model](https://www.tensorflow.org/tutorials/keras/save_and_load#hdf5_format) and then [loading only the weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) from it.\r\nHope this helps.', b'@sachinprasadhs \r\nI think we can close this issue.', b'@moumed, Refer to the quoted reply from @old-school-kid and let us know if you need any further clarification, refer [save_and_load](https://www.tensorflow.org/tutorials/keras/save_and_load) documentation for further clarification. Thanks!\r\n\r\n> Hi @moumed\r\n> \r\n> > One reason could be that a stateful\r\n> > object or a variable that the function depends on is not assigned to an attribute of\r\n> > the serialized trackable object\r\n> \r\n> means you are using objects that arent serializable.\r\n> \r\n> > layers.RandomFlip(""horizontal""),\r\n> > layers.RandomFlip(""vertical""),\r\n> > layers.RandomRotation(0.5),\r\n> > layers.RandomZoom(0.5),\r\n> > layers.RandomContrast(0.5),\r\n> \r\n> As a matter of fact you have passed arguments to the layers and those cannot be saved. The proper way to do this is to create a class for the model that inherits keras.Model or a class for the whole thing as a layer that inherits keras.layers.Layer and then define `get_config` and `from_config` methods. The documentation can be found [here](https://www.tensorflow.org/tutorials/keras/save_and_load#saving_custom_objects).\r\n> \r\n> A workaround is [saving the model as a `.h5` model](https://www.tensorflow.org/tutorials/keras/save_and_load#hdf5_format) and then [loading only the weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights) from it. Hope this helps.\r\n\r\n', b'hey guys,\r\nsorry for being late to answer. thank you for all your answers. \r\nthe problem is resolved, you can see the correction steps in this link: https://stackoverflow.com/questions/69955838/saving-model-on-tensorflow-2-7-0-with-data-augmentation-layer\r\n\r\nI will close this issue now.', b'the issue is not resolved ,\r\nas although it is possible to save the model using .h5 format but reloading it and evaluating it gives way less accuracy\r\n\r\nresults before saving:\r\n![image](https://user-images.githubusercontent.com/75084722/150778459-a2ac9172-981a-4b8a-aaed-f623501f1e4c.png)\r\n\r\nresults after reloading:\r\n![image](https://user-images.githubusercontent.com/75084722/150778647-5ffde093-13c1-4cb4-894a-28aa934cf246.png)\r\n\r\n', b""@ashwinshetgaonkar interesting remark, i didn't verify the results. \r\nWe have to admit that the solution proposed is only an alternative. \r\n\r\nThe main issue is not really resolved.\r\n\r\n"", b'Hi @ashwinshetgaonkar,\r\nThe error arises because you havent loaded the custom objects like the factors for `RandomRotation` or `RandomContrast`. They cannot be saved without a config since these values are not serialized. Instead of loading the model you can try out `load_weights` if you dont want to declare methods for config. \r\nThis is not a workaround as all the objects you want to save have to be serialized. @moumed you can refer to the issue you raised in stackoverflow which says the same, ie, you have to use methods `get_config` and `from_config` methods. \r\n', b""@old-school-kid \r\nI had tried using `get_config` ,`from_config`,`load_weights` but the issue persists.\r\n\r\nresults before saving:\r\n![image](https://user-images.githubusercontent.com/75084722/150786751-6794d823-be0c-4da3-a386-e74519111a73.png)\r\n\r\nresults after reloading:\r\n![image](https://user-images.githubusercontent.com/75084722/150786874-b5048eb4-6567-4c03-9f10-17b54193e3b3.png)\r\n\r\nthis is the data augmentation layer I have used:\r\n![image](https://user-images.githubusercontent.com/75084722/150787049-fad318a9-6394-4c75-842b-e140382d5f90.png)\r\n\r\nAlso taking into account that these methods does not save the parameters of Data Augmentation layers this shouldn't have affected the model in inference mode."", b""> @ashwinshetgaonkar interesting remark, i didn't verify the results. We have to admit that the solution proposed is only an alternative.\r\n> \r\n> The main issue is not really resolved.\r\n\r\nSo better alternative is to go back to ImageDataGenerator and compromise on speed.""]",open,2021-11-24 17:32:52,
https://api.github.com/repos/keras-team/keras/issues/15698,b'tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (-1018167296 vs. 0)',"[b""Update: I downgraded the Amazon Linux instance to Cuda 10.2 and Tensorflow 2.3 and still have the same problem. I had seen in the release notes that Cuda 10.2 had fixed an issue with very large batches and mixed precision, but it doesn't fix this.\r\n"", b'@sanatmpa1,\r\n\r\nCan you please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) which shows the corresponding `CUDA` version for each version of `Tensorflow`. Also please share the fully reproducible stand alone code to expedite the trouble shooting process. Thanks!', b'Tested with with following configurations on Windows 10 and AWS Linux (RHEL7) with hardware as described above:\r\nTensorflow 2.60\r\nPython 3.8.5\r\nCuDNN: 8.1\r\nCuda 11.2\r\nPip: 20.2.4\r\n\r\nFull source not available, relevant source:\r\n    def create_generator(self):\r\n        self.logger.debug(\'called\')\r\n\r\n        inputs = tf.keras.Input(shape=(self._noise_dim,))\r\n        dense = Dense(128 * self.dim * self.dim,\r\n                      kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))\r\n        x = dense(inputs)\r\n        x = BatchNormalization(momentum=0.8)(x)\r\n        x = LeakyReLU(0.2)(x)\r\n        x = Reshape((self.dim, self.dim, 128))(x)\r\n        x = UpSampling2D(size=(self.upSamp, self.upSamp))(x)\r\n        x = Conv2D(64, kernel_size=(5, 5), padding=\'same\')(x)\r\n        x = BatchNormalization(momentum=0.8)(x)\r\n        x = LeakyReLU(0.2)(x)\r\n        x = UpSampling2D(size=(self.upSamp, self.upSamp))(x)\r\n        x = Conv2D(1, kernel_size=(5, 5), padding=\'same\')(x)\r\n        x = BatchNormalization(momentum=0.8)(x)\r\n        x = Flatten()(x)\r\n        outputs = Dense(self.feature_size, activation=\'tanh\', dtype=np.float32)(x)  # All output between -1 and 1\r\n\r\n        generator = tf.keras.Model(inputs=inputs, outputs=outputs, name=\'Generator\')\r\n        generator.compile(loss=self.generator_loss_function, optimizer=self.generator_optimizer)\r\n\r\n        return generator\r\n\r\n    def create_discriminator(self):\r\n        self.logger.debug(\'called\')\r\n\r\n        inputs = tf.keras.Input(shape=(self.feature_size,))\r\n        dense = Dense(self.dim * self.dim * 32, input_shape=(self.feature_size,),\r\n                                kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))\r\n        x = dense(inputs)\r\n        x = Reshape((self.dim, self.dim, 32))(x)\r\n        # x = Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding=\'same\')(x)\r\n        # x = BatchNormalization(momentum=0.8)(x)\r\n        # x = LeakyReLU(0.2)(x)\r\n        # x = Dropout(rate=0.3)(x)\r\n\r\n        x = Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=\'same\')(x)\r\n        # discriminator.add(BatchNormalization(momentum=0.8)(x)\r\n        x = LeakyReLU(0.2)(x)\r\n        x = Dropout(rate=0.25)(x)\r\n\r\n        x = Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding=\'same\')(x)\r\n        # x = BatchNormalization(momentum=0.8)(x)\r\n        x = LeakyReLU(0.2)(x)\r\n        x = Dropout(rate=0.25)(x)\r\n\r\n        x = Flatten()(x)\r\n        outputs = Dense(1, activation=\'sigmoid\', dtype=np.float32)(x)  # Original + dtype for mixed-precision\r\n        # x= Dense(1)(x) # https://medium.com/swlh/gan-generative-adversarial-network-3706ebfef77e\r\n\r\n        discriminator = tf.keras.Model(inputs = inputs, outputs=outputs, name=\'Discriminator\')\r\n        discriminator.compile(loss=self.discriminator_loss_function,\r\n                              optimizer=self.discriminator_optimizer, metrics=[\'accuracy\'])\r\n\r\n        return discriminator\r\n\r\n    def discriminator_loss(self, real_output, synthetic_output):\r\n        # https://stackoverflow.com/questions/55936611/why-doesnt-the-discriminators-and-generators-loss-change\r\n        cross_entropy = tf.keras.losses.BinaryCrossentropy()\r\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\r\n        fake_loss = cross_entropy(tf.zeros_like(synthetic_output), synthetic_output)\r\n        total_loss = (real_loss + fake_loss) / 2\r\n\r\n        return total_loss\r\n\r\n    def generator_loss(self, fake_output) -> object:\r\n        # https://stackoverflow.com/questions/55936611/why-doesnt-the-discriminators-and-generators-loss-change\r\n        cross_entropy = tf.keras.losses.BinaryCrossentropy()\r\n        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\r\n\r\n        return gen_loss\r\n\r\n    # Notice the use of `tf.function`. This annotation causes the function to be ""compiled"".\r\n    @tf.function()\r\n    def _train_gen(self, real_data):\r\n        #self.logger.info(\'Called\')\r\n        noise = tf.random.normal([self._batch_size, self._noise_dim])\r\n\r\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as _:\r\n            # Generator creates synthetic _data\r\n            synthetic_data = self._generator(noise, training=True)\r\n            # Discriminator predicts on real _data\r\n            real_data_pred = self._discriminator(real_data, training=False)\r\n            # Discriminator predicts on synthetic _data\r\n            synth_data_pred = self._discriminator(synthetic_data, training=False)\r\n            # Calculate Generator loss based on Discriminator\'s predictions of synthetic _data\r\n            gen_loss = self.generator_loss(synth_data_pred)\r\n            # Calculate Discriminator loss based on Discriminator\'s predictions of real and synthetic _data\r\n            disc_loss = self.discriminator_loss(real_data_pred, synth_data_pred)\r\n\r\n        gradients_of_generator = gen_tape.gradient(gen_loss, self._generator.trainable_variables)\r\n        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self._generator.trainable_variables))\r\n\r\n        return gen_loss, disc_loss\r\n\r\n    # Notice the use of `tf.function`. This annotation causes the function to be ""compiled"".\r\n    @tf.function()\r\n    def _train_step(self, real_data):\r\n        #self.logger.info(\'Called\')\r\n        noise = tf.random.normal([self._batch_size, self._noise_dim])\r\n\r\n        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\r\n            # Generator creates synthetic _data\r\n            synthetic_data = self._generator(noise, training=True)\r\n            # Discriminator predicts on real _data\r\n            real_data_pred = self._discriminator(real_data, training=True)\r\n            # Discriminator predicts on synthetic _data\r\n            synth_data_pred = self._discriminator(synthetic_data, training=True)\r\n            # Calculate Generator loss based on Discriminator\'s predictions of synthetic _data\r\n            gen_loss = self.generator_loss(synth_data_pred)\r\n            # Calculate Discriminator loss based on Discriminator\'s predictions of real and synthetic _data\r\n            disc_loss = self.discriminator_loss(real_data_pred, synth_data_pred)\r\n\r\n        gradients_of_generator = gen_tape.gradient(gen_loss, self._generator.trainable_variables)\r\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, self._discriminator.trainable_variables)\r\n        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self._generator.trainable_variables))\r\n        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,\r\n                                                         self._discriminator.trainable_variables))\r\n\r\n        return gen_loss, disc_loss\r\n\r\n    def train(self, job: Job, epochs=None) -> None:\r\n        self.job = job;\r\n        start = time.time()\r\n        update_time = start + 60*5  # 5 minutes\r\n        if epochs is None:\r\n            epochs = job.db_job.epochs\r\n        update_calc = lambda x: int(epochs / 10) if (int(epochs / 10) > 0) else 1\r\n        update_rate = update_calc(epochs)\r\n        # assert not np.any(np.isnan(data.normalized.values))\r\n        self._batch_size = self._round_batch_size(job.db_job.batch_size)\r\n        if len(self.data.normalized) < self._batch_size:\r\n            self._batch_size = self._round_batch_size(len(self.data.normalized))\r\n            self.logger.info(f\'Specified batch_size greater than sample size: changing to {self._batch_size}\')\r\n        self.logger.debug(f\'Starting training of {epochs} epochs with batch size of {self._batch_size}\')\r\n        self.logger.info(f\'data.normalized.shape[0] = {self.data.normalized.shape[0]}\')\r\n        num_batches = int(len(self.data.normalized) / self._batch_size)\r\n        gen_count = 0\r\n        dis_count = 0\r\n        disc_loss = 0.0\r\n        gen_loss = 0.0\r\n\r\n        for epoch in range(epochs):\r\n            for batch_num in range(num_batches):  # Train on all data in each epoch\r\n                # indexes = randint(0, self.data.normalized.shape[0], batch_size)  # Generate non-contiguous random indexes\r\n                if self._batch_size > self.data.normalized.shape[0]:\r\n                    index = randint(0, self.data.normalized.shape[0] - self._batch_size)  # Generate contiguous random indexes\r\n                else:\r\n                    index = 0\r\n                real_data = self.data.normalized.values[index: index+self._batch_size]\r\n                # self.logger.debug(f\'Batch: {batch_num*batch_size} : {(batch_num*batch_size + batch_size)}\')\r\n                gen_loss, disc_loss = self._train_step(real_data)\r\n                # else:\r\n                #     if balance and gen_loss > disc_loss:  # Train just gen\r\n                #         gen_loss, disc_loss = self._train_gen(real_data)\r\n                #         gen_count += 1\r\n                #     else:  # Train both again\r\n                #         gen_loss, disc_loss = self._train_step(real_data)\r\n                #         dis_count += 1\r\n            # Save the model every periodically\r\n            if epoch % update_rate == 0 or time.time() > update_time:\r\n                # etime = (time.time() - start)\r\n                update_time = time.time() + 5 * 60  # 5 minutes\r\n                self.logger.debug(f\'Epoch: {epoch}   \\tgen loss: {gen_loss:.7f} \\tdisc loss: {disc_loss:.7f} \\t\'\r\n                                  f\'gen count: {gen_count} \\tdis_count: {dis_count}\')\r\n\r\n                self.update_status(f\'training epoch {epoch}\')\r\n                # self.logger.debug(f\'Epoch: {epoch}\\t time: {etime}\\tDLoss = {d_loss:.4f} GLoss = {g_loss:.4f}\')\r\n                # self.checkpoint.save(file_prefix=self.checkpoint_prefix)\r\n                if epoch > 100:\r\n                    self.save()\r\n        self.logger.debug(f\'Final gen loss: {gen_loss}  disc loss: {disc_loss}\')\r\n        # self.checkpoint.save(file_prefix=self.checkpoint_prefix)\r\n        job.db_job.training_time = int(time.time() - start)  # Round to nearest second\r\n        self.update_status(JobStatus.trained)\r\n        self.save()  # Save the generator and discriminator models\r\n        self._trained = True\r\n        # self.checkpoint.save(file_prefix=self.checkpoint_prefix)\r\n\r\nRun with batch size of 10240 or higher.\r\n\r\n\r\n', b'I think I found the problem. Looking through the TF source, the function CHECK_GT takes int32 as the arguments. For my model, the tensor size for batch of 1024 is [1024,250,250,64] which totals: 4,096,000,000. If I cast that to an int32, the resulting value is -198967296 which is what I see in the console when the program dies: .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (-198967296 vs. 0)\r\nSince modern GPUs can handle massive models (my Titan RTX has 24GB), you need to change the int arguments to long in CHECK_GT (and similar functions).', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'I would like some feedback on my suggestion. I can put together a standalone test case if necessary, but it should be easy to guess that using int32 for the parameter size checks is going to cause problems as models get larger and GPU memory continues to increase. ', b'Adding @rohan100jain from tf.core team for this issue. From the latest message, it might be a int32 overflow issue somewhere in TF.\r\n\r\nIn the meantime, please provide a some reproducible example. Thanks.', b'It is without question an int32 overflow problem. Please look at the function \r\n```\r\ninline GpuLaunchConfig GetGpuLaunchConfig(int work_element_count, const Eigen::GpuDevice& d) \r\n```\r\nin tensorflow/core/util/gpu_launch_config.h.\r\nIt takes an int as an argument. When work_element_count exceeds the system limit for int, the program aborts. You can check by calling it with a work_element_count of value 2200000000 or greater on an int32 system.', b""```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Reshape, Dense, BatchNormalization, Flatten, LeakyReLU, UpSampling2D, Conv2D\r\nfrom tensorflow.keras.initializers import he_uniform\r\nimport numpy as np\r\n\r\nnoise_dim = 128\r\ndim = 10\r\nupSamp = 5\r\nfeature_size = 328\r\nbatch_size = 1024\r\nkernel_init = he_uniform()\r\n\r\ninputs = tf.keras.Input(shape=(noise_dim,))\r\ndense = Dense(128 * dim * dim, kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02))\r\nx = dense(inputs)\r\nx = BatchNormalization(momentum=0.8)(x)\r\nx = LeakyReLU(0.2)(x)\r\nx = Reshape((dim, dim, 128))(x)\r\nx = UpSampling2D(size=(upSamp, upSamp))(x)\r\nx = Conv2D(64, kernel_size=(5, 5), padding='same', kernel_initializer = kernel_init)(x)\r\nx = BatchNormalization(momentum=0.8)(x)\r\nx = LeakyReLU(0.2)(x)\r\nx = UpSampling2D(size=(upSamp, upSamp))(x)\r\nx = Conv2D(1, kernel_size=(5, 5), padding='same', kernel_initializer = kernel_init)(x)\r\nx = BatchNormalization(momentum=0.8)(x)\r\nx = Flatten()(x)\r\noutputs = Dense(feature_size, activation='tanh', dtype=np.float32)(x)  # All output between -1 and 1\r\n\r\ngenerator = tf.keras.Model(inputs=inputs, outputs=outputs, name='Generator')\r\n\r\nnoise = tf.random.normal([batch_size, noise_dim])\r\n@tf.function()\r\ndef train_step(data):\r\n    with tf.GradientTape() as gen_tape:\r\n        synthetic_data = generator(noise, training=True)\r\ndata = tf.random.normal([batch_size, feature_size])\r\ntrain_step(data)\r\n```"", b""Make sure you run the code above on a GPU with enough memory that you don't get an OOM exception. This code was tested on an NVidia Titan RTX with 24GB of GPU memory."", b'Please let me know if the sample code above reproduced the problem and if it is helpful.']",open,2021-11-24 14:32:58,
https://api.github.com/repos/keras-team/keras/issues/15697,b'Changes a PSS test to use MinSizePartitioner instead of FixedShardsPartitioner to unblock tf.random.Generator rollout.',[],open,2021-11-24 01:08:03,
https://api.github.com/repos/keras-team/keras/issues/15694,b'Categorical Crossentropy Loss.',[],open,2021-11-23 17:54:21,
https://api.github.com/repos/keras-team/keras/issues/15685,b'Update permute.py',"[b'Hey @qlzh727 , thanks for reviewing the PR, I shall make the necessary commits. Thanks', b'Hey @gbaned , please review my comments and suggest me the necessary changes. Thanks', b""Hey @gbaned , please assign me another reviewee, as @qlzh727, hasn't reviewed my PR past 2 days. Thanks"", b""Hi @Bhavay192 ,\r\nThe example given in the documents is an example of permute in 3D tensor. What you saying is about a 4D tensor. Also when I say a rank, the batch is also included. \r\nIf you still can't follow please refer to [this notebook](https://colab.research.google.com/drive/1LcsSibKSVEoU0hHGhnblX9Bp18i7fDEt?usp=sharing)\r\nThank you"", b""@Bhavay192 Can you please check @old-school-kid's comments and keep us posted ? Thanks!""]",open,2021-11-22 09:50:47,
https://api.github.com/repos/keras-team/keras/issues/15680,b'Binary Focal Crossentropy Loss.',[],open,2021-11-19 20:17:15,
https://api.github.com/repos/keras-team/keras/issues/15678,"b""Mixed precision doesn't work properly on NVIDIA A10G GPUs""","[b'@goncinious,\r\n\r\nCan you try using `tensorflow/tensorflow:latest-gpu` docker image which uses stable version, instead of `nightly` and let us know if this is still an issue? Thanks!', b""@sanatmpa1, maybe my description wasn't fully clear - all tests were performed using `tensorflow/tensorflow:latest-gpu` image (see `a10g.log` and `t4_tesla.log`). I've only mentioned `tensorflow/tensorflow:nightly-gpu`, as it gave an additional `RESOURCE_EXHAUSTED` message (see `a10g_tf_nightly.log`), which might be useful to pinpoint the issue.\r\n\r\n"", b'@sachinprasadhs, please let me know if you need any more information about the issue found. If not, can you please change the label `stat:awaiting response` of the issue, so it gets the right visibility?', b'@reedwm could you take a look?', b""Can you try again with the latest `nightly-gpu` build? There's a decent chance https://github.com/tensorflow/tensorflow/pull/52337 fixed this, which was merged eight days ago."", b'Thanks @reedwm. I confirmed that the **error still occurs** on the latest `nightly-build`(`sha256:0a8364f4082cc51a1c2de05eb97e92e1d5d1ab8759387f0066d8ea700dec1b94`). Full log attached.\r\n[20211203_a10g_tf_nightly.log](https://github.com/keras-team/keras/files/7648425/20211203_a10g_tf_nightly.log)', b'With a Titan RTX, I can reproduce with CUDA 11.2 and cudnn 8.1.1. However, I cannot reproduce with CUDA 11.3 and cudnn 8.2.4. So presumably this will be fixed when TensorFlow upgrades CUDA and/or cudnn. Even if I limit the memory usage to 10 GB instead of 24 GB, it still runs with CUDA 11.3/cudnn 8.2.4, so there is clearly a memory issue with the earlier version of CUDA/cudnn.\r\n\r\n@sanjoy, do you know when we plan on upgrading CUDA and cudnn? Is this planed for TF 2.8?\r\n\r\n@awpr, any ideas what could be causing this, and know of any ways to fix it for CUDA 11.2? Seems strange that an algorithm requires 4.7 GB of memory.\r\n\r\n@goncinious, as a temporary workaround, you can manually upgrade CUDA and cudnn if you know how. Understandably, this is difficult however.', b""Thanks @reedwm. I've tested with [NVIDIA TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/running.html#running) official Docker images, which have newer versions of CUDA/cuDNN and I still hit the same error (see logs attached).\r\n\r\nI've tested 2 image versions that match your CUDA/cuDNN versions - both [21.04](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/rel_21-04.html#rel_21-04) (CUDA 11.3 / cuDNN 8.2.0) and [21.09](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/rel_21-09.html#rel_21-09) (CUDA 11.4 / cuDNN 8.2.4).\r\n\r\n### Steps to reproduce\r\n\r\n1. Pull TensorRT image\r\n```\r\ndocker pull nvcr.io/nvidia/tensorrt:21.04-py3\r\n```\r\n2. Start interactive Docker container and pass test.py (copy from [Colab](https://colab.research.google.com/drive/1Vg8LPNNrYJYAd4qjA_p_GuabzjQdqTwU?usp=sharing))\r\n```\r\ndocker run -it --gpus all --rm -v /home/radyc/test.py:/srv/test.py nvcr.io/nvidia/tensorrt:21.04-py3 /bin/bash\r\n```\r\n3. Install Tensorflow (2.7.0 gets installed)\r\n```\r\npip install tensorflow\r\n```\r\n4. Run script\r\n```\r\npython /srv/test.py\r\n```\r\n5. Repeat with 21.09 image\r\n\r\n### Logs\r\n[a10g_cuda11-3_cudnn8-2-0.log](https://github.com/keras-team/keras/files/7684000/a10g_cuda11-3_cudnn8-2-0.log)\r\n[a10g_cuda11-4_cudnn8-2-4.log](https://github.com/keras-team/keras/files/7684001/a10g_cuda11-4_cudnn8-2-4.log)\r\n\r\n\r\n"", b""Unfortunately, I still cannot reproduce with the newer CUDA/cudnn versions. I tried running the docker commands in your previous post on an A100 but could not reproduce, even when limiting the memory to 10GiB to try to reproduce the `RESOURCE_EXHAUSTED` error. I'm guessing this only happens on GPUs with compute capability 8.6, but I don't have access to such GPUs.\r\n\r\n@nluehr do you have access to GPUs with compute capability 8.6 that you can try to reproduce this issue on?\r\n"", b'Reproduced on a GTX 3090 (compute capability 8.6, 24GB of memory)\r\nWith CUDNN 8.2.0 (as provided in the referenced TRT container) I see the OOM for mixed_float16 while float32 runs without issue.\r\nUpdating to CUDNN 8.3.1, I can run mixed_float16 and float32 without issue.', b'Ok, so this will be fixed for all tested GPUs in cudnn 8.3.1. @sanjoy, do we plan to update cudnn to at least 8.3.1 anytime soon?', b""Thank you both for looking into it. I can confirm that updating cuDNN manually in the TensorRT 21.04 container to 8.3.1 or using [TensorRT 21.11](https://docs.nvidia.com/deeplearning/tensorrt/container-release-notes/rel_21-11.html#rel_21-11) (which use CUDA 11.5 / cuDNN 8.3.0) fixes the issue.\r\n\r\n\r\nI have two questions to @nluehr for further clarification:\r\n\r\n1. when you modify the version of cuDNN, do you just upgrade the library in the environment, or do you also need to recompile Tensorflow with the corresponding CUDA/cuDNN version?\r\n2.  > Reg. Updating to CUDNN 8.3.1, I can run mixed_float16 and float32 without issue.\r\n\r\nDid you also update CUDA here?\r\n\r\n\r\nNevertheless,I think this is still a workaround, as we need to manually update cuDNN version and this CUDA/cuDNN version combination isn't part of the TensorFlow's tested build configurations - https://www.tensorflow.org/install/source#gpu. Therefore, knowing if CUDA/cuDNN update in TensorFlow will happen (and having an ETA) would be very useful."", b'You can upgrade cudnn to newer minor versions (e.g., 8.3.0 over 8.2.x) without rebuilding TensorFlow.\r\n\r\nI did not update CUDA in my tests. It is generally safe to use a cuDNN built against a later CUDA of the same CUDA major version. (e.g., you can use a cudnn built against CUDA 11.5 with CUDA 11.3). If you also update the CUDA toolkit, I believe you would need to rebuild TensorFlow.\r\n\r\nAs you point out, ""generally works"" and officially tested and supported are different things. If you are looking for TensorFlow containers built and tested with the latest cuDNN/CUDA combinations, you might check out the [NGC TensorFlow releases](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tensorflow).', b'@nluehr - Thanks for your reply.\r\n\r\nAfter further investigation, I found that while upgrading cuDNN fixed the OOM issue observed with mixed precision on A10g GPUs (CC=8.6), the model output became non-deterministic when running on multi-GPU with mirrored strategy (i.e gives slightly different outputs every time I run it on the same input volume).\r\n\r\nCrucially, I found that the output was deterministic when using 1-GPU only or when I switch to full precision, suggesting that something is broken with mixed precision when used on multi-GPUs with the latest compute capability.\r\n\r\nNote that the behaviour was always deterministic when using mixed precision using GPUs with older compute capability (i.e Tesla T4, which have CC=7.5).\r\n\r\n## Steps to reproduce\r\n\r\nMy tests were performed on the latest official TensorFlow GPU Docker image (v2.7.0), using the cuDNN upgrade solution as suggested. See steps below to reproduce the results obtained:\r\n\r\n1. Start instance with 4 NVIDIA A10g GPUs\r\n2. Download latest TensorFlow Docker image (TensorFlow v2.7.0)\r\n```\r\ndocker pull tensorflow/tensorflow:latest-gpu\r\n```\r\n3. Download cuDNN\r\n- Follow https://developer.nvidia.com/cudnn-download-survey and download cuDNN v8.3.1 (for CUDA 11.5) - `Ubuntu20.04 x_86_64 (Deb)` file.\r\n4. Initialise container, passing cuDNN installer and test script\r\n- Get `test_identical.py` script by copying code from [Colab](https://colab.research.google.com/drive/12mv3Z4zn5zBc0kpnaxRv-9wn_aGqeY7Q?usp=sharing).\r\n ```\r\n$ docker run -it --gpus all --rm -v /home/radyc/cudnn-local-repo-ubuntu2004-8.3.1.22_1.0-1_amd64.deb:/srv/cudnn-local-repo-ubuntu2004-8.3.1.22_1.0-1_amd64.deb -v /home/radyc/test_identical.py:/srv/test_identical.py tensorflow/tensorflow:latest-gpu /bin/bash\r\n ```\r\n5. Upgrade cuDNN within the container\r\n- As per official instructions - https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installlinux-deb\r\n```\r\ndpkg -i /srv/cudnn-local-repo-ubuntu2004-8.3.1.22_1.0-1_amd64.deb\r\napt-key add /var/cudnn-local-repo-ubuntu2004-8.3.1.22/7fa2af80.pub\r\napt update\r\napt install libcudnn8=8.3.1.22-1+cuda11.5 -y\r\n``````\r\n6. Run `test_identical.py` (first time)\r\n- This will save the initialised model and save the first prediction\r\n```\r\npython /srv/test_identical.py\r\n`````\r\n7. Run `test_identical.py` again.\r\n- Loads the previously initialised model, runs prediction on the same volume and compares with output obtained in step 6\r\n- Here, you should get an `AssertionError`, as outputs will be different (see full log `a10g_cudnn_updated_test_identical_4_gpu.log` attached).\r\n\r\nFollow the steps below to check that it works on 1-GPU:\r\n1. remove previous model and output\r\n```\r\nrm /srv/output.npy /srv/model.h5\r\n```\r\n2. run `test_identical.py` forcing it to use 1-GPU (first time)\r\n```\r\nCUDA_VISIBLE_DEVICES=0 python /srv/test_identical.py\r\n```\r\n3. repeat step 2.\r\n- No `AssertionError` obtained (see full log `a10g_cudnn_updated_test_identical_1_gpu.log` attached)\r\n\r\n## Logs\r\n[a10g_cudnn_updated_test_identical_4_gpu.log](https://github.com/keras-team/keras/files/7763785/a10g_cudnn_updated_test_identical_4_gpu.log)\r\n[a10g_cudnn_updated_test_identical_1_gpu.log](https://github.com/keras-team/keras/files/7763786/a10g_cudnn_updated_test_identical_1_gpu.log)\r\n\r\n', b'Determinism is not guaranteed by default, and as you observed, nondeterminism might only occur in specific cases. Running [`tf.config.experimental.enable_op_determinism()`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) should fix it, but note this is only available in the nightly builds and also will likely reduce performance.', b""Thank you for your reply, @reedwm.\r\n\r\nI do understand why determinism is difficult to guarantee in training (e.g data sampling randomisation), but at inference it isn't that easy to understand, as the model and inputs are fixed.\r\n\r\nDo you mind expanding a bit on the sources for these non-determinism at inference time? My guess is the split of both data and model operations across GPUs that might cause discrepancies, but more detail would be very helpful."", b'> Do you mind expanding a bit on the sources for these non-determinism at inference time?\r\n\r\nThe split of data across GPUs can cause discrepancies, but these discrepancies can typically be removed by calling [`tf.keras.utils.set_random_seed`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed). A major source of nondeterminism comes from the fact that floating-point math is nonassociative, which means the order numbers are added can slightly affect the final result (unlike real numbers). GPU ops often use many threads to add numbers together, and so the order they are added is often nondeterministic.\r\n\r\nAnother source of nondeterminism comes from using a process in TensorFlow called ""autotuning"". For many ops, such as convolutions, there are multiple different algorithms that can be used to compute the op. For example, convolutions can be computed as FFTs, or using matrix multiplications, or with various other algorithms. With autotuning, TensorFlow tries each algorithm the first time the op is run, then uses the fastest algorithm for subsequent runs of the op. However, if multiple algorithms take approximately the same amount of time to run, it is nondeterministic which algorithm will be fastest, so the algorithm TensorFlow selects is nondeterministic. Different algoirthms may have slightly different results on the same inputs, so autotuning can cause nondeterminism. Autotuning is disabled with `tf.config.experimental.enable_op_determinism()`\r\n']",open,2021-11-19 10:20:08,
https://api.github.com/repos/keras-team/keras/issues/15669,b'Keras changes names of tensorflow operators',"[b'@dvtran221 I can reproduce the issue. I think the issue is more related to those layers (`tf.nn.space_to_depth`, `tf.nn.depth_to_space`, `tf.multiply`) used in the model. I checked with other layers like`Dense` which has no issue like the above.\r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/321928f8bc0f9683bf5602a6b98415b5/untitled1117.ipynb) is a gist for our reference. Thanks!\r\n\r\n![image](https://user-images.githubusercontent.com/46058173/144127899-82bb2b1e-652d-4606-91ca-3fac08d4fb7c.png)\r\n']",open,2021-11-18 17:33:53,
https://api.github.com/repos/keras-team/keras/issues/15667,b'Current implementation only supports equal length strides in the row and column dimensions.',"[b'It seems that when I upgrade my tensorflow version to the latest 2.7, similar exceptions still occurs, the error-info takes a bit changes, but the problem stay the same. Here is the erro info: \r\n![image](https://user-images.githubusercontent.com/94270103/142407564-5b7d1ebc-b29d-48fe-a4c4-4c9ca407ad04.png)\r\nAnd the link to original issue post on [tensorflow/tensorflow](https://github.com/tensorflow/tensorflow/) is:\r\nhttps://github.com/tensorflow/tensorflow/issues/53055.']",open,2021-11-18 11:30:00,
https://api.github.com/repos/keras-team/keras/issues/15666,"b""Details about tf.keras.layers.Conv2DTranspose params 'padding' and 'output_padding'""",[],open,2021-11-18 11:16:22,
https://api.github.com/repos/keras-team/keras/issues/15661,b'test: add xfail test for SavedModel restoring optimizer weights',"[b'@bhack could you run kokoro please?', b'Sorry, I cannot assign labels in this repository', b'Anything I can do here to get the ball rolling?', b'@k-w-w, can you take a look for this PR? Thanks.', b""Hi folks, just checking in so this doesn't go stale. It'd be great to make some progress on this long-standing footgun""]",open,2021-11-17 23:29:35,
https://api.github.com/repos/keras-team/keras/issues/15655,"b'[TPU, keras preprocessing layer] Some Op must be a compile-time constant.'","[b'I am able to reproduce the issue with TF 2.7.0 . Please find the [gist here.](https://colab.sandbox.google.com/gist/sanatmpa1/e43cd41c1f85bae012156b10311be44a/segmentation-augmentation-layer-with-tpu.ipynb)', b'Took a look. There may something we need to change in how we set up randomness for the RandomFlip layer, I will check about this.\r\n\r\nOverall, I think preferred approach here should be to apply the preprocessing layers inside a `tf.data.Dataset.map` **before** the training step. This would keep all preprocessing running on the CPU asynchronously, which should be more efficient in this case.\r\n\r\nSee this section of our guide which explains the choice:\r\nhttps://keras.io/guides/preprocessing_layers/#preprocessing-data-before-the-model-or-inside-the-model\r\n\r\nNote that the RandomRotation underlying op does not have TPU support (why you have `tf.config.set_soft_device_placement(True)`) so you will be running partially on the CPU anyway.\r\n\r\nThis blogpost also shows an example of running preprocessing separately tf.data and prefetching:\r\nhttps://blog.tensorflow.org/2021/11/an-introduction-to-keras-preprocessing.html', b""Thank you for your insightful guidance @mattdangerw ! I'll see the blog post and revise my code."", b'Adding @wangpengmit who works on tf.random.Generator.']",open,2021-11-16 08:05:45,
https://api.github.com/repos/keras-team/keras/issues/15643,b'Inconsistent results on every run with GPU',"[b'I could notice the change in final loss on each run, attaching the [gist](https://colab.research.google.com/gist/sachinprasadhs/5751a9c525624aa9132c3975d9dfba05/tensorflow-gradient-bug.ipynb) for reference. Thanks!', b'Thanks for reporting this issue! This is actually expected: if model is run on gpu, then cudnn imports this randomness. If you want to get deterministic results, you may check out this api: https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism\r\n\r\nBut getting a deterministic result is at the cost of performance.', b""@chenmoneygithub \r\nThank you for the reply.\r\nSo the `relu` is not a deterministic op in gpu mode? because I get the random results only when I use `relu` or other `relu`-like ops, e.g. `leaky_relu`, `tf.maximum`. Besides, I want to test this api on colab, but it seems it only available in tf-nightly, and maybe available after tensorflow 2.8 is released. Is there any api similar to this before tensorflow 2.8? I have already tested setting `os.environ['TF_CUDNN_DETERMINISTIC']='1'` but doesn't work.\r\n"", b'Adding here a note from the page.\r\n\r\n> Note: This API is new and only available via pip install tf-nightly.\r\n\r\nSo, if you want to test this, you need to install `tf-nightly`. For some reasons, some times colab has difficulty in accessing GPU. So, try testing on your local GPU or cloud GPU. Thanks', b'@jvishnuvardhan \r\nHey, thank you for noting me that. How about the version before `tf-nightly` (e.g. tf 2.4~2.7) . Can I have deterministic results on these versions on gpu?', b'@Ending2015a https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism This API is new and only available in `tf-nightly` and not available before (e.g. tf 2.4~2.7).', b'@jvishnuvardhan \r\nYes I know it is only available in `tf-nightly`. What I am looking for is the way other than using this api to get the deterministic results on the previous versions (tf 2.4~2.7).', b'I do not think we have a similar api in the stable version. \r\n\r\nOne question I am curious about is - does the small value fluctuation affect your work? We want to learn more about how severe the problem is, thanks!', b""Hello, @chenmoneygithub\r\nYou think that it's just a small fluctuation is because this example code uses a small network which only has 3 small conv layers and only trains for 1000 gradient steps. As I observed in this tiny scripts, each gradient update will cause 1e-6 scale of noises, resulting in 1e-3 scale of fluctuations. While in the real world application we use larger and deeper networks and needs much more training steps. So if we run more training steps, the compounding error becomes huge, and finally we have a completely different models with the same random seeds.\r\n\r\nThis problem is very severe, especially for researchers who want to reproduce the experiments results or want to debug algorithms. Since each time we get the completely different results, the bugs may not reproducible and we need to spend more time to find the cause. For software developers, it's hard to do CI and unittests since the networks output different results on each run even the random seeds are fixed."", b'So I think TensorFlow/Keras teams can not solve this issue. I decide to switch to PyTorch. Thank you.']",open,2021-11-13 09:11:55,
https://api.github.com/repos/keras-team/keras/issues/15631,b'Added DetermisticRandomTestTool (from www.tensorflow.org/guide/migrate/validate_correctness) to variable_scope_shim.py. This tool is used to make random number generation semantics match between TF1.x graphs/sessions and eager execution.',[],open,2021-11-10 23:10:01,
https://api.github.com/repos/keras-team/keras/issues/15608,b'feature request: Robbins-Monro type learning rate decay',[b'Should this be moved to tensorflow-addons? @bhack '],open,2021-11-08 18:09:29,
https://api.github.com/repos/keras-team/keras/issues/15606,b'tf.keras.callbacks.ModelCheckpoint Type Error : Unable to serialize 1.0000000656873453e-05 to JSON',"[b'@ravinderkhatri, In order to reproduce the issue reported here, could you please provide the complete code? Thanks!', b""Below is the code that I used. Please let me know in case you need further information\r\n```\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\r\n    strategy = tf.distribute.TPUStrategy(tpu)\r\nexcept ValueError:\r\n    strategy = tf.distribute.MirroredStrategy()\r\n\r\n\r\nprint('REPLICAS', strategy.num_replicas_in_sync)\r\n\r\n\r\nimg_dataset_path = 'images'\r\n\r\nIMG_HEIGHT = 224\r\nIMG_WIDTH = 224\r\nIMAGE_SIZE = [IMG_HEIGHT, IMG_WIDTH]\r\nIMG_CHANNELS = 3\r\nBATCH_SIZE = 16\r\n\r\nlabel_map_pbtxt_path = 'label_map.pbtxt'\r\n# get  total image class\r\nCLASS_NAMES = get_label_from_label_map_pbtxt(label_map_pbtxt_path)\r\nn_labels = len(CLASS_NAMES)\r\n# read tf record dataset\r\ntrain_dataset_path = 'train.record'\r\ntest_dataset_path = 'test.record'\r\n# changes in tf record dataset\r\ntrain_data = create_dataset(train_dataset_path, batch_size=16, IMG_WIDTH=224, IMG_HEIGHT=224, n_labels=n_labels)\r\ntest_data = create_dataset(test_dataset_path, batch_size=16, IMG_WIDTH=224, IMG_HEIGHT=224, n_labels=n_labels)\r\n\r\nN_EPOCHS = 200\r\n\r\nstep = tf.Variable(0, trainable=False)\r\n\r\ninitial_learning_rate = 0.001\r\nschedule = tf.keras.optimizers.schedules.ExponentialDecay(\r\n    initial_learning_rate,\r\n    decay_steps=14,\r\n    decay_rate=0.8,\r\n    staircase=True)\r\nlr = 1e-0 * schedule(step)\r\nwd = lambda: 1e-2 * schedule(step)\r\n\r\nwith strategy.scope():\r\n    pretrained_model = tf.keras.applications.MobileNetV2(\r\n                                                    weights='imagenet',\r\n                                                    include_top=False,\r\n                                                    input_shape=[*IMAGE_SIZE, IMG_CHANNELS])\r\n    pretrained_model.trainable = True #fine tuning\r\n    model = tf.keras.Sequential([\r\n                            tf.keras.layers.Lambda(# Convert image from int[0, 255] to the format expect by this model\r\n                            lambda data:tf.keras.applications.mobilenet.preprocess_input(\r\n                                tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3]),\r\n                            pretrained_model,\r\n                            tf.keras.layers.GlobalAveragePooling2D()])\r\n    model.add(tf.keras.layers.Dense(64, name='object_dense',kernel_regularizer=tf.keras.regularizers.l2(l2=0.001)))\r\n    model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))\r\n    model.add(tf.keras.layers.Activation('relu', name='relu_dense_64'))\r\n    model.add(tf.keras.layers.Dropout(rate=0.2, name='dropout_dense_64'))\r\n    model.add(tf.keras.layers.Dense(32, name='object_dense_2',kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)))\r\n    model.add(tf.keras.layers.BatchNormalization(scale=False, center = False))\r\n    model.add(tf.keras.layers.Activation('relu', name='relu_dense_32'))\r\n    model.add(tf.keras.layers.Dropout(rate=0.2, name='dropout_dense_32'))\r\n    model.add(tf.keras.layers.Dense(16, name='object_dense_16', kernel_regularizer=tf.keras.regularizers.l2(l2=0.01)))\r\n    model.add(tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax', name='object_prob'))\r\n    m1 = tf.keras.metrics.CategoricalAccuracy()\r\n    m2 = tf.keras.metrics.Recall()\r\n    m3 = tf.keras.metrics.Precision()\r\n\r\n\r\n\r\noptimizers = [\r\n    tfa.optimizers.AdamW(learning_rate=lr * .001 , weight_decay=wd),\r\n    tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\r\n           ]\r\n\r\n\r\noptimizers_and_layers = [(optimizers[0], model.layers[0]), (optimizers[1], model.layers[1:])]\r\n\r\noptimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\r\n\r\nmodel.compile(\r\n    optimizer= optimizer,\r\n    loss = 'categorical_crossentropy',\r\n    metrics=[m1, m2, m3],\r\n    )\r\ncheckpoint_path = os.getcwd() + os.sep + 'keras_model'\r\n\r\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \r\n                                                    monitor = 'categorical_accuracy',\r\n                                                    save_best_only=True,\r\n                                                    save_weights_only=False)\r\n\r\nhistory = model.fit(train_data, validation_data=test_data, epochs=N_EPOCHS, callbacks=[checkpoint_cb])\r\n\r\n```"", b'@ravinderkhatri, Can you please share `label_map.pbtxt` and other supporting files if any to replicate above issue? Thanks!', b'@chunduriv Sharing all the required files to replicate the above issue.\r\n\r\nBelow is the link to label_map.pbtxt file \r\nhttps://drive.google.com/file/d/1Qqk3jtJLwA-h3G3sgjMMtvr5JWmPJGcC/view?usp=sharing\r\n\r\nLink to tfrecord training file\r\nhttps://drive.google.com/file/d/1WSuFYHiYKN5AXlRmtMfNhQ_gmfsnK-BE/view?usp=sharing\r\n\r\nLink to tfrecord testing file\r\nhttps://drive.google.com/file/d/1NWOMQ9QnVA5txXwtWVPgZ4oT0Ul7fLyv/view?usp=sharing\r\n\r\nFunction to get labels from label_pbtxt file\r\n```\r\ndef get_label_from_label_map_pbtxt(label_map_pbtxt_path):\r\n    CLASS_NAMES = []\r\n    with open(label_map_pbtxt_path, ""r"") as file:\r\n        for line in file:\r\n            if \'name:\' in line:\r\n                img_class_name = line.split(\'name:\')[-1].replace(""\'"", """").strip()\r\n                CLASS_NAMES.append(img_class_name)\r\n    return CLASS_NAMES\r\n\r\n```\r\n\r\n\r\n\r\n\r\nFunction to read tfrecord file\r\n```\r\ndef read_tfrecord(example):\r\n    read_features = {\'image/encoded\' : tf.io.FixedLenFeature([], dtype = tf.string),\r\n                    \'image/object/class/label\': tf.io.FixedLenFeature([], dtype = tf.int64)}\r\n\r\n    parsed_s_example = tf.io.parse_single_example(serialized=example,\r\n                                                features=read_features)\r\n    encoded_image = parsed_s_example[\'image/encoded\']\r\n    decoded_image = tf.io.decode_image(encoded_image)\r\n    image_label = parsed_s_example[\'image/object/class/label\']\r\n    return decoded_image, image_label\r\n\r\n\r\ndef read_tfrecord_parallelize(tfrecord_data_name):\r\n    """"""\r\n    Perform parallel reading of data from TFRecordDataset generated by using example_proto\r\n    For more info see: to_gpt_tfrecord_example and get_data_and_write_gpt_tf_record_format\r\n    It uses read_gpt_tfrecord function to read data.\r\n    Args:\r\n        tfrecord_data_name(str): name of tfrecord dataset to read\r\n    Returns:\r\n        dataset(tensorflow.python.data.ops.dataset_ops.ShuffleDataset): a shuffled dataset read from tfrecord_data_name \r\n    """"""\r\n    dataset = tf.data.TFRecordDataset(tfrecord_data_name, num_parallel_reads=AUTO)\r\n    dataset = dataset.with_options(option_no_order)\r\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\r\n    dataset = dataset.shuffle(300)\r\n    return dataset\r\n\r\n```\r\nBy using above function dataset can be called out as \r\n```\r\ntrain_data = read_tfrecord_parallelize(train_tfrecord_file_path)\r\ntest_data = read_tfrecord_parallelzie(test_tfrecord_file_path\r\n\r\n```\r\n\r\nPlease let me know in case further information is required.']",open,2021-11-08 10:54:02,
https://api.github.com/repos/keras-team/keras/issues/15593,b'Model consuming RaggedTensors fails during evaluation in a distributed setting',"[b'Adding to the above. I noticed that this error can be avoided by using infinite datasets for both train and validation (`.repeat()`). If either or both of the train and validation datasets are non-infinite, the same or a similar error pops up.\r\n\r\nIf training is done with with a non-infinite dataset (irrespective of the nature of the validation set), the same error pops up, but during the training phase.\r\n\r\nAll of the above holds for `tf.distribute.MirroredStrategy()` at least. Hope this helps.\r\n\r\n', b'I am able to reproduce the reported issue in colab with `TF 2.6` and `TF 2.7` as well. Please find the [gist here](https://colab.sandbox.google.com/gist/sanatmpa1/dc4658aa21884250a99eb60f9f0b0ddb/dummy-model-ragged.ipynb). Thanks!', b""Could you help make this repro a little more minimal to be able to triangulate what's going wrong? Do we know the minimum piece of code to trigger this error?\r\n\r\nOne thing just browsing that did look suspicious is that the ragged `tf.keras.Input` was not actually created with `ragged=True`, though that may not be related to the error being seen here."", b""> Could you help make this repro a little more minimal to be able to triangulate what's going wrong? Do we know the minimum piece of code to trigger this error?\r\n\r\nActually, we minimalized our implementation to a great extent to produce the current snippet. In our understanding, this is the minimum piece of code that triggers the error. "", b'@mattdangerw @sayakpaul \r\n\r\nI created another [notebook](https://colab.research.google.com/drive/1yNZlIuTnQagLwULt7BJyMHPn_brk7V0Q) that has the bare minimum (or as close as I could get to bare minimum) code needed to reproduce the issue. Note that the notebook is fetching a different set of TFRecords from the previous notebook.\r\n\r\n\r\nI hope this helps!', b'@Nilabhra , Could you please check the links of the files you have attached, both the links giving 404 error.', b'@sachinprasadhs Thanks for letting me know, I have updated the link in the comment.', b'@sachinprasadhs Could you check the updated link?', b""@mattdangerw\r\n\r\n@Nilabhra has further simplified the notebook reducing the amount of code needed to reproduce the said issue. Let us know if you'd need anything else. "", b'It looks like there\'s a bug in the `create_dummy_tensor` function in `distribute/input_lib.py`, where it does something fairly nonsensical if the rank of the feature is unknown.  I\'m not entirely clear on how these ""dummy tensors"" get used, but my best guess is that this could be fixed on TensorFlow\'s end by a change such as this (new lines marked with ""NEW""):\r\n\r\n```\r\n[tensorflow/python/distribute/input_lib.py, in create_dummy_tensor]\r\n    if isinstance(spec, ragged_tensor.RaggedTensorSpec):\r\n      if not dims:                                               ## NEW\r\n        dummy_tensor = tf.zeros([0], feature_type)               ## NEW\r\n      row_splits = array_ops.zeros(1, spec._row_splits_dtype)\r\n      dummy_tensor = ragged_tensor.RaggedTensor.from_nested_row_splits(\r\n          dummy_tensor, (row_splits,) * spec._ragged_rank, validate=False)\r\n```\r\n\r\nAlternatively, you could modify your data-loading code to ensure that at least the rank of the input feature is known.  E.g., if I change `read_ragged_feature` in the linked colab to the following definition, then the colab works:\r\n\r\n```\r\ndef read_ragged_feature(feature_name, feature, ragged_rank):\r\n    ragged_feature = {}\r\n    ragged_feature[feature_name] = deserialize_composite(\r\n        feature, tf.RaggedTensorSpec(dtype=tf.int32, ragged_rank=ragged_rank),\r\n    )\r\n    ragged_feature[feature_name].flat_values.set_shape([None])  # NEW\r\n    return ragged_feature\r\n```\r\n\r\n(This assumes that you statically know the rank of your input tensors -- in this case, I was assuming that there are no ""uniform inner"" dimensions beyond the ragged dimensions, but you could adjust it if that\'s not the case for you.  If you don\'t statically know the rank of your input tensors, then this won\'t help, but I think having unknown ranks for input tensors is fairly rare.)\r\n', b'@edloper Thank you so much for taking the time to work on this bug. I guess I can incorporate either of the solutions your provided, the second one being the easier one to do so. I hope the dev team takes notice of this and patches the bug soon.']",open,2021-11-05 11:35:56,
https://api.github.com/repos/keras-team/keras/issues/15592,"b""tf.keras.layers.BatchNormalization returns output that's significantly different from batch norm formula""","[b'If its any help, I dug deeper into the Keras code and found the source of discrepancy\r\n\r\nif the input shape is `(N, C)` or `(N, T, C)`, then `tf.nn.batch_normalization` is used within `tf.keras.layers.BatchNormalization` to calculate the batch normalization. The output value here is in line with `numpy` outputs. \r\n\r\nHowever, if the input shape is longer such as `(N, H, W, C) ` or `(N, T, H, W, C)`, then `tf.compat.v1.nn.fused_batch_norm` is used and the large discrepancy in output occurs', b'https://github.com/keras-team/keras/issues/15571 might be related to this issue', b'this issue can be temporarily fixed by `tf.keras.layers.BatchNormalization(<other args>, fused=False)`', b'@jvishnuvardhan, I was able to replicate issues in `TF2.7` and `tf-nightly`. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/98496019a2e16e5ab45dd03636c8ea61/15592.ipynb). Thanks!', b""Took a look, it does look like this difference is directly from `tf.compat.v1.nn.fused_batch_norm`, which won't guarantee the exact same computation. If the difference is important, I think the fix you listed (passing `fused=False`) is the correct for now.\r\n\r\nThe long term fix we'd like is to move a lot of the complexity out of batch norm, use XLA compilation (`jit_compile=True`) to keep things efficient, and remove the fused op entirely. Until we do that, we will keep the fused option default to avoid a performance regression.""]",open,2021-11-05 09:37:32,
https://api.github.com/repos/keras-team/keras/issues/15586,b'Initializers with the same seed produce different output in Keras 2.7',"[b'@jvishnuvardhan, I was able to replicate issues in `TF2.7` and `tf-nightly`. Please find the [gist here](https://colab.research.google.com/gist/chunduriv/efb911b97d1eb2a288afb4ec73db7f3a/15586.ipynb). Thanks!', b'I can reproduce the issue with `tf-nightly`. However, the following workaround may be helpful. [Here](https://colab.research.google.com/gist/jvishnuvardhan/0bbbc0cd98ae79ac135cb3ef0849df1d/15586.ipynb) is a gist for reference. Thanks\r\n\r\n```\r\ng = tf.random.Generator.from_seed(0)\r\nw0 = g.normal(shape=(2,2))\r\n\r\ng1 = tf.random.Generator.from_seed(0)\r\nw1 = g1.normal(shape=(2,2))\r\n\r\nprint(w0)\r\nprint(w1)\r\n```\r\n\r\n```\r\ntf.Tensor(\r\n[[-1.3544159   0.70454913]\r\n [ 0.03666191  0.86918795]], shape=(2, 2), dtype=float32)\r\ntf.Tensor(\r\n[[-1.3544159   0.70454913]\r\n [ 0.03666191  0.86918795]], shape=(2, 2), dtype=float32)\r\n```', b'@drasmuss This is working as mentioned in the docs. Please check [here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/RandomUniform). It was mentioned that the initializer will not produce same random values across multiple calls but produces the same sequence.\r\n\r\n\r\nseed | A Python integer. Used to create random seeds. See\xc2\xa0tf.compat.v1.set_random_seed\xc2\xa0for behavior. Note that seeded initializer will not produce same random values across multiple calls, but multiple initializers will produce same sequence when constructed with same seed value.\r\n-- | --\r\n\r\n\r\nI ran three times with the seed=0 and verified that the produces the same sequence.  \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/c6271cf200f80b5b0e04ede36c2c49c6/15586.ipynb) is a gist for reference. Thanks!', b""I don't believe this is working as described in the docs\r\n>Note that seeded initializer will not produce same random values across multiple calls\r\n\r\nThis means that if I construct a given Initializer (e.g. `init0 = tf.keras.initializers.RandomUniform(seed=0)`), and call it multiple times, like\r\n```\r\ninit0 = tf.keras.initializers.RandomUniform(seed=0)\r\nw0 = init0((2, 2))\r\nw1 = init0((2, 2))\r\nw2 = init0((2, 2))\r\n```\r\nI should expect `w0`, `w1`, and `w2` to be different. That part is working as expected.\r\n\r\n> but multiple initializers will produce same sequence when constructed with same seed value.\r\n\r\nThis is the part that is not working. This says that multiple initializers, with the same seed value, will produce the same sequence. But that is not the case. If I construct a second initializer with the same seed, and call it multiple times\r\n```\r\ninit1 = tf.keras.initializers.RandomUniform(seed=0)\r\ny0 = init1((2, 2))\r\ny1 = init1((2, 2))\r\ny2 = init1((2, 2))\r\n```\r\nThe documentation says that this initializer should produce the same sequence. I.e., `w0 == y0`, `w1 == y1`, and `w2 == y2`. But that is not the case. The two initializers, despite having the same seed value, produce a different sequence.\r\n"", b'@drasmuss Before running `init1 = tf.keras.initializers.RandomUniform(seed=0)`, if you reset the colab runtime, then the sequence is same (i.e. w0 == y0, w1 == y1, and w2 == y2).\r\n\r\nIn the above, you are not restarting the runtime. May be we need to update the docs to reflect it. Thanks!', b""If you reset the colab runtime then you effectively just have a single initializer (because resetting gets rid of the previous one). The documentation explicitly says that _multiple_ initializers should produce the same sequence with the same seed. You could adjust the documentation to remove that claim entirely, but that's a pretty significant change in the behaviour.\r\n\r\nAs a broader point, the benefit of setting the seed to a specific value on an object with randomness is to set that randomness to a reliable, reproducible value.  With the current behaviour, the actual seed value passed doesn't really matter, because all the seed does is fix the RNG to some arbitrary point. For example, consider this code snippet:\r\n```\r\nw0 = tf.keras.initializers.RandomUniform(seed=0)((2, 2))\r\nw1 = tf.keras.initializers.RandomUniform(seed=0)((2, 2))\r\nw2 = tf.keras.initializers.RandomUniform(seed=1)((2, 2))\r\n```\r\nThere is no effective difference here between setting `seed=0` and `seed=1`. In both cases `w0 != w1` and `w0 != w2`. That seems like unexpected behaviour for a seed.\r\n\r\nFor a more real-world example, consider the case where I want to build a model that has two layers with the same initial weights in each layer. A natural way to go about that would be the following:\r\n```\r\nimport tensorflow as tf\r\n\r\nmy_model = tf.keras.Sequential(\r\n    [\r\n        tf.keras.layers.Dense(\r\n            2,\r\n            kernel_initializer=tf.keras.initializers.RandomUniform(seed=0),\r\n            input_shape=(2,),\r\n        ),\r\n        tf.keras.layers.Dense(\r\n            2,\r\n            kernel_initializer=tf.keras.initializers.RandomUniform(seed=0),\r\n        ),\r\n    ]\r\n)\r\n\r\nmy_model.build()\r\nprint(my_model.weights)\r\n```\r\nI think most users would be pretty surprised to find that that their two layers actually have completely different initial weights, even though they explicitly seeded them to the same value."", b'@qlzh727 could you take a look at this when you have a second?', b'Let me add @wangpengmit who is the owner of random ops in tf.\r\n\r\nCurrently it is using stateful random ops under the hood, which might have some relationship with the tf global seed. In 2.8, we will enable the tf.random.Generator for the initializer, which I think will produce the behavior describe in the docstring.', b'Btw, you can take a look for the example in https://www.tensorflow.org/api_docs/python/tf/random/set_seed. There is actually some behavior details. \r\n\r\nIn the meantime, you can also try:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninit0 = tf.keras.initializers.RandomUniform(seed=0)\r\ninit0._random_generator._force_generator = True\r\nw0 = init0((2, 2))\r\n\r\ninit1 = tf.keras.initializers.RandomUniform(seed=0)\r\ninit1._random_generator._force_generator = True\r\nw1 = init1((2, 2))\r\n\r\nprint(w0)\r\nprint(w1)\r\n\r\nassert np.allclose(w0, w1)\r\n```', b'@qlzh727 Depending on the final code change, we also need to update the docs https://www.tensorflow.org/api_docs/python/tf/keras/initializers/RandomUniform\r\n\r\nseed | A Python integer. Used to create random seeds. See\xc2\xa0tf.compat.v1.set_random_seed\xc2\xa0for behavior. Note that seeded initializer will not produce same random values across multiple calls, but multiple initializers will produce same sequence when constructed with same seed value.\r\n-- | -- ', b'Btw, with the current code, it will also reproduce same result if you have fixed tf.random.seed. It can be removed once have the generator code path turn on by default.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(1)\r\ninit0 = tf.keras.initializers.RandomUniform(seed=0)\r\nw0 = init0((2, 2))\r\n\r\ntf.random.set_seed(1)\r\ninit1 = tf.keras.initializers.RandomUniform(seed=0)\r\nw1 = init1((2, 2))\r\n\r\nprint(w0)\r\nprint(w1)\r\n\r\nassert np.allclose(w0, w1)\r\n```', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""I'd wait until TF 2.8 is released to confirm that the behaviour then matches the docs before closing this, but up to you. But the `_force_generator` workaround does work until then.""]",open,2021-11-04 17:38:15,
https://api.github.com/repos/keras-team/keras/issues/15577,b'InvalidArgumentError: 9 root error(s) found.',"[b'cc @ksalama ', b'Is this going to happen on training?  As I cannot reproduce your error on Colab/TPU with the vanilla notebook.\r\nCan you share your Colab?', b""@bhack hi, I've shared a reproducible colab link, mentioned above. If it's not working, please try [this](https://colab.research.google.com/drive/1DAQpG9YD-5vH-A-jfaISwFzMkE6oAlm9?usp=sharing). "", b""Ok now I can reproduce it I don't think it is related `tf.image.extract_patches` as it is in the official list without special notes:\r\nhttps://cloud.google.com/tpu/docs/tensorflow-ops\r\n\r\nHave you tried to check the TPU compatiblity graph:\r\nhttps://cloud.google.com/tpu/docs/cloud-tpu-tools#tpu_compatibility_graph\r\n\r\nI don't if on colab it could be retrieved with:\r\nhttps://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/profiling_tpus_in_colab.ipynb"", b""OK, I just replace class `Patches` and `PatchEncoder` layer with a single class `PatchEmbed` layer that does a similar thing but doesn't use `tf.image.extract_patches` and it runs successfully. [Colab](https://colab.research.google.com/drive/1DAQpG9YD-5vH-A-jfaISwFzMkE6oAlm9?usp=sharing).\r\n\r\n```python\r\ndef create_vit_classifier():\r\n    inputs = layers.Input(shape=input_shape)\r\n    # # Create patches.\r\n    # patches = Patches(patch_size)(inputs)\r\n    # # Encode patches.\r\n    # encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\r\n\r\n    encoded_patches = PatchEmbed(img_size=(72, 72), \r\n                                 patch_size=(patch_size, patch_size), in_chans=3, \r\n                                 embed_dim=projection_dim)(inputs)\r\n```"", b""Are you sure that it was about `tf.image.extract_patches`? As I see that you have also removed the keras embedding layer. I don't know what is the behavior inside the TPU strategy_scope:\r\n\r\nhttps://github.com/keras-team/keras/blob/42bf9972492f47c3d3c249de9c20942ba217937d/keras/layers/embeddings.py#L83-L99"", b""I'm not sure if it causes for `tf.image.extract_patches`. I guess came from seeing the error logs, i.e. `patch_encoder_3`; maybe from layer `PatchEncoder`. \r\n\r\n```\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_43005}}\r\n    Incompatible shapes: [100,25,64] vs. [144,64]\r\n\r\n\t# ---------------------------------------  HERE ------------------------\r\n\t [[{{node gradient_tape/model_1/patch_encoder_3/add/BroadcastGradientArgs}}]]\r\n\t [[TPUReplicate/_compile/_58268965962327260/_4]]\r\n\t [[tpu_compile_succeeded_assert/_14858208428749173052/_5/_197]]\r\n```\r\n\r\nAlso in the following docstring, it says if my embedding matrix is too large for the GPU, I may consider placing the operation on CPU.  \r\n\r\nhttps://github.com/keras-team/keras/blob/42bf9972492f47c3d3c249de9c20942ba217937d/keras/layers/embeddings.py#L83-L99\r\n\r\nBut note that here I'm using TPU. Even if I set `tf.config.set_soft_device_placement(True)`, it doesn't work either."", b'What I see is:\r\n`tf.keras.layers.Add`\r\n> It takes as input a list of tensors, **all of the same shape**, and returns a single tensor (also of the same shape).\r\n\r\nIn your example if you add a check in your `PatchEncoder`\r\n```\r\n    def call(self, patch):\r\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\r\n        pos_emb = self.position_embedding(positions)\r\n        proj =  self.projection(patch)\r\n        tf.test.TestCase().assertEqual(proj.shape,pos_emb.shape)\r\n        encoded = pos_emb + proj\r\n        return encoded\r\n```\r\n\r\n\r\n`AssertionError: TensorShape([None, None, 64]) != TensorShape([144, 64])`\r\n\r\n']",open,2021-11-02 17:47:21,
https://api.github.com/repos/keras-team/keras/issues/15573,"b""Can't set sprite in Keras Tensorboard Callback""",[b'Assign to @rchao  who works on callbacks.'],open,2021-11-01 20:20:39,
https://api.github.com/repos/keras-team/keras/issues/15565,b'TensorFlow Keras SavedModel with custom layers throws a TypeError after being saved and loaded twice',"[b'I am able to reproduce the issue reported in colab, with `TF 2.6.0` and also `tf-nightly`. Please find the [gist here](https://colab.research.google.com/gist/sanatmpa1/3a86907e2247c80578d6ee4a0616e562/15565.ipynb)', b'I would like to work on this issue if no one else is currently working on it. ', b""Thanks MrinalTyagi! Let us know if you need any help from our side. Also cc'ed a crew member in Keras."", b'@k-w-w ', b'Is there an easy way to make it work for an existing saved model?', b'@adam-kral Easy and recommended approach is to pass `custom_objects` dictionary.\r\n\r\n`model3 = tf.keras.models.load_model(""model2"", custom_objects={\'CustomLayer\': CustomLayer)`\r\n\r\nWithout that `custom_objects`, `load_model` is able to load the model with custom_layer once. Second time, loading the saved model (that was loaded and saved from original model1) is throwing an error because it is missing one of the `training` argument. \r\n\r\nCheck https://www.tensorflow.org/guide/keras/save_and_serialize for more details on saving and loading a model with custom objects. Thanks', b'@jvishnuvardhan Thank you, but that didn\'t work for me. (Hope I\'m doing it right). Probably because my saved model is already saved, loaded and saved again (indeed `""model2""` file as you write). So as jamesmishra writes in his [SO answer](https://stackoverflow.com/questions/69762318/tensorflow-keras-savedmodel-throws-a-typeerror-after-being-saved-and-loaded-twic) when I implement `to_config`, it has no effect on `model2` as it was already saved (without `to_config`). \r\n\r\nSo I get the same error even when I implement both `from_config`, `to_config` and use the line you provided. \r\n\r\nIt is nothing important, a model for school assignment I just wanted to know how it performed, but I wonder if there is a simple way to load it (add `training` argument to the saved model bytecode), for me, or others that stumble upon this.', b'@adam-kral I am not sure about adding another `training` argument to the save model. \r\n\r\n@k-w-w is the expert and would wait for a response from @k-w-w ', b""@adam-kral @jvishnuvardhan @chenmoneygithub Just a suggestion, while creating model with custom layer, shouldn't adding these as warnings would be helpful for users.\r\n\r\n1. passing it in the `custom_objects` dictionary passed to `tf.keras.models.load_model()`.\r\n2. adding the layer to the Keras registry with the `@tf.keras.utils.register_keras_serializable` decorator.\r\n3. installing the layer with the `tf.keras.utils.custom_object_scope()` context manager."", b""same.\r\nIt's ok on tf2.4"", b'The same issue was encountered on TF 2.7.0']",open,2021-10-29 00:57:49,
https://api.github.com/repos/keras-team/keras/issues/15550,b'Google only changes',[],open,2021-10-25 22:40:02,
https://api.github.com/repos/keras-team/keras/issues/15547,b'Cannot load back model with no-op Concatenate layer',"[b'@stefanistrate,  In general, `layers.concatenate` can be used to merge all available features into a single large vector. \r\n\r\nWhy are using `tf.keras.layers.Concatenate()` layer? Can you please brief about your use case? \r\n', b'In my case, I control the various layers I want to concatenate via flags to the python script. Although in general I concatenate more than one layer, the baseline is with one single layer and it would be helpful (and expected I would say) that `Concatenate()` is a no-op for only one input layer.', b'For functional API you should use concatenate instead of Concatenate, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/7083046f47862f0859e2e2b231b341ff/52968.ipynb) for the same with multiple inputs in concat layer and in your case there is no need of concatenating the layers, it can be used when multiple Input layers are used like below [example](https://www.tensorflow.org/guide/keras/functional#manipulate_complex_graph_topologies).\r\n', b""I know there's no need to concatenate 1 layer but, as I was saying, in case I still do, `Concatenate()` should be a no-op. Same for `concatenate()`. My point is that instead of having something like:\r\n\r\n```python\r\nlayers = [...]\r\nif len(layers) == 1:\r\n  concatenated = layers[0]\r\nelse:\r\n  concatenated = tf.keras.layers.concatenate(layers)\r\n```\r\n\r\nI'd like to simply use:\r\n\r\n```python\r\nlayers = [...]\r\nconcatenated = tf.keras.layers.concatenate(layers)\r\n```"", b'And coming back to the bug I reported: I can do the above at training time, but loading back the model produces the crash.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b'Triage notes: we took a look and think that we should actually have concatenate throw an error with only a single input. We will try out a change and see how safe that is.\r\n\r\nFor now, the solution proposed in https://github.com/keras-team/keras/issues/15547#issuecomment-966954665 is probably best.']",open,2021-10-25 18:15:59,
https://api.github.com/repos/keras-team/keras/issues/15545,b'Add Swin-Transformer to keras.applications',"[b'Thanks for this FR, could you provide the link to the original paper so that we can take a look?\r\n\r\nIn general, I think we are open accept new application models if they are widely recognized. Currently we might host it in keras.application, but will very likely to move them to keras-cv repository, which is targeted for CV specific components. ', b'Just FYI, you can take a look for the new guidance for adding application to keras in https://github.com/keras-team/keras/pull/15447.', b""@qlzh727 \r\nThe paper link is provided. \r\n\r\nI'm really excited about the [keras-cv](https://github.com/keras-team/keras-cv) repo. Any good news? "", b'So far we are scoping it and draft the roadmap for it. Will have more details later this year.', b'@Rishit-dagli are you interested to contribute here?\n\nContext:\nhttps://github.com/tensorflow/models/issues/10269']",open,2021-10-25 10:51:51,
https://api.github.com/repos/keras-team/keras/issues/15541,b'Periodic delays of TF inference time in batch process',"[b'@mayantd,\r\n\r\nCan you share a simple standalone code to reproduce the reported issue from our end? Thanks!', b'My model is a ResNet50-V2 model. You can see a standalone code below.\r\n\r\n```\r\nimport os\r\nimport sys\r\nfrom PIL import Image\r\nimport time\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow.keras.preprocessing import image\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\r\nimport cv2\r\nfrom datetime import datetime\r\nimport gc\r\n\r\nIMG_SIZE = 224\r\n\r\n\r\ndef setpriority(pid=None, priority=1):\r\n    """""" Set The Priority of a Windows Process.  Priority is a value between 0-5 where\r\n        2 is normal priority.  Default sets the priority of the current\r\n        python process but can take any valid process ID. """"""\r\n\r\n    import win32api, win32process, win32con\r\n\r\n    priorityclasses = [win32process.IDLE_PRIORITY_CLASS,\r\n                       win32process.BELOW_NORMAL_PRIORITY_CLASS,\r\n                       win32process.NORMAL_PRIORITY_CLASS,\r\n                       win32process.ABOVE_NORMAL_PRIORITY_CLASS,\r\n                       win32process.HIGH_PRIORITY_CLASS,\r\n                       win32process.REALTIME_PRIORITY_CLASS]\r\n    if pid is None:\r\n        pid = win32api.GetCurrentProcessId()\r\n    handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\r\n    win32process.SetPriorityClass(handle, priorityclasses[priority])\r\n\r\n\r\ndef main():\r\n    setpriority(None, 5)\r\n    config = tf.compat.v1.ConfigProto(gpu_options=\r\n                                      tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\r\n                                      # device_count = {\'GPU\': 1}\r\n                                      )\r\n    config.gpu_options.allow_growth = True\r\n    session = tf.compat.v1.Session(config=config)\r\n    tf.compat.v1.keras.backend.set_session(session)\r\n\r\n    im_mask = cv2.imread(""mask.png"", cv2.IMREAD_GRAYSCALE)\r\n\r\n    model = tf.keras.models.load_model(\'model_gpu_Dataset11.h5\')\r\n    predictionThreshold = -2\r\n\r\n    logFile = open(\'CycleTime.txt\', \'a\')\r\n\r\n    folder_raw = ""FOLDER_RAW\\\\""\r\n    folder_nok = ""FOLDER_NOK\\\\""\r\n    folder_ok = ""FOLDER_OK\\\\""\r\n\r\n    image_files = os.listdir(folder_raw)\r\n    file_number = len(os.listdir(folder_raw))\r\n\r\n    for index in range(file_number):\r\n        print(index)\r\n        filename = folder_raw + image_files[index]\r\n        img = cv2.imread(filename, 0)\r\n\r\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\r\n\r\n        im_masked1 = cv2.bitwise_and(img_rgb, img_rgb, mask=im_mask)\r\n        dim = (IMG_SIZE, IMG_SIZE)\r\n        img_resized = cv2.resize(im_masked1, dim, interpolation=cv2.INTER_LANCZOS4)\r\n        img_filtered = im_masked1.astype(\'uint8\')\r\n\r\n        my_image_arr = image.img_to_array(img_resized)\r\n        my_image_arr_norm = my_image_arr / 127.5 - 1\r\n\r\n        my_image_arr_norm_expand = np.expand_dims(my_image_arr_norm, axis=0)\r\n\r\n        mytime = datetime.now().strftime(""%m_%d_%Y_%H_%M_%S_%f"")\r\n        logFile.write(""Prediction Time Start          :{ptime}\\n"".format(ptime=mytime))\r\n\r\n        prediction_cam1 = model.predict(my_image_arr_norm_expand)\r\n\r\n        print(prediction_cam1)\r\n\r\n        if prediction_cam1 < predictionThreshold:\r\n            save_file_name = folder_nok + image_files[index]\r\n        else:\r\n            save_file_name = folder_ok + image_files[index]\r\n\r\n        mytime = datetime.now().strftime(""%m_%d_%Y_%H_%M_%S_%f"")\r\n        logFile.write((""Prediction Time End          :{ptime}\\n"").format(ptime=mytime))\r\n\r\n        font = cv2.FONT_HERSHEY_SIMPLEX\r\n        org = (10, 20)\r\n        fontscale = 0.5\r\n        color = (0, 255, 255)\r\n        thickness = 1\r\n        cv2.putText(img_filtered, str(prediction_cam1), org, font, fontscale, color, thickness, cv2.LINE_AA)\r\n\r\n        # collected=gc.collect()\r\n        # img.save(save_file_name)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    if main():\r\n        sys.exit(0)\r\n    else:\r\n        sys.exit(1)\r\n```', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b""Closing as stale. Please reopen if you'd like to work on this further.\n""]",open,2021-10-24 08:08:02,
https://api.github.com/repos/keras-team/keras/issues/15540,b'Improvement for tf.keras.utils.get_file',"[b'Triage note: Is it possible to simply use the python post api to retrieve the file before using it?', b'@bizzyvinci Can you please respond to the above comment? Thanks!', b""I don't really understand the question. \r\n\r\nIf you meant using `requests.post` method. No, we can't use that because a response object is returned instead of downloading the file like `urlretrieve`.\r\n\r\n`urllib.request.urlretrieve` method is currently used in `keras.utils.get_file`. `urllib.request.urlretrieve` has a `data` parameter which turns the request type from GET to POST and that parameter is not utilized."", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b'Reopening the issue, as the bot has mistakenly closed this issue even after the user has commented.']",open,2021-10-22 21:26:41,
https://api.github.com/repos/keras-team/keras/issues/15536,b'TypeError in VQ-VAE example',"[b""I am able to reproduce the issue reported, in colab running with TF 2.6.0. Here's the colab [gist](https://colab.research.google.com/gist/sanatmpa1/a16ec9f9d1d5454d658f7fda519a5e04/15536.ipynb)."", b'Triage notes: Leave this to the meeting and decide the owner of the keras.io notebooks.', b'Triage notes: We probably should run tests for guides and examples as a pre release check.', b""Will assign to the original author, and when they not respond, keras team will try to check if it is fixable. Will probably take the example offline if we can't get response from original author for a long period."", b'Assign to @sayakpaul who created in original example in https://github.com/keras-team/keras-io/pull/556', b""I will take a look at this and get back soon. \r\n\r\n> Triage notes: We probably should run tests for guides and examples as a pre release check.\r\n\r\nThe standalone Python script that is initially submitted for PR needs to be actually run successfully to produce the notebook, md, and figures. Doesn't that partially solve the purpose? @qlzh727 "", b""Hi again. \r\n\r\nIt's unfortunate that this issue is happening and as bad it may sound I currently don't have any concrete clue as to why this might be happening. \r\n\r\nThe example was created using TensorFlow 2.5.0 and it ran perfectly fine with that as we can see in the example (otherwise, the `keras.io` scripts won't have generated the complete notebook and figures in the first place). \r\n\r\n@qlzh727 could this be a version problem? Currently, my schedule is a bit swamped to try it out on TensorFlow 2.5.0 so I will really appreciate any help. "", b'@sayakpaul I have tried this notebook in the environment with tensorflow==2.5.0 and tensorflow-probability==0.13.0 and the code ran without any bug, so I guess it\'s the version problem.\r\n\r\nEdit: I tried also for tensorflow==2.7.0 and there is different error:\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-16-a6e2591462e2> in <module>()\r\n      9         # Feed the whole array and retrieving the pixel value probabilities for the next\r\n     10         # pixel.\r\n---> 11         probs = sampler.predict(priors)\r\n     12         # Use the probabilities to pick pixel values and append the values to the priors.\r\n     13         priors[:, row, col] = probs[:, row, col]\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)\r\n     65     except Exception as e:  # pylint: disable=broad-except\r\n     66       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n---> 67       raise e.with_traceback(filtered_tb) from None\r\n     68     finally:\r\n     69       del filtered_tb\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in autograph_handler(*args, **kwargs)\r\n   1127           except Exception as e:  # pylint:disable=broad-except\r\n   1128             if hasattr(e, ""ag_error_metadata""):\r\n-> 1129               raise e.ag_error_metadata.to_exception(e)\r\n   1130             else:\r\n   1131               raise\r\n\r\nTypeError: in user code:\r\n\r\n    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1621, in predict_function  *\r\n        return step_function(self, iterator)\r\n    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1611, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1604, in run_step  **\r\n        outputs = model.predict_step(data)\r\n    File ""/usr/local/lib/python3.7/dist-packages/keras/engine/training.py"", line 1572, in predict_step\r\n        return self(x, training=False)\r\n    File ""/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n    File ""<string>"", line 3, in raise_from\r\n        \r\n\r\n    TypeError: Exception encountered when calling layer ""model"" (type Functional).\r\n    \r\n    Dimension value must be integer or None or have an __index__ method, got value \'<attribute \'shape\' of \'numpy.generic\' objects>\' with type \'<class \'getset_descriptor\'>\'\r\n    \r\n    Call arguments received:\r\n      \xe2\x80\xa2 inputs=tf.Tensor(shape=(None, 7, 7), dtype=float32)\r\n      \xe2\x80\xa2 training=False\r\n      \xe2\x80\xa2 mask=None\r\n```', b'Interesting. Thanks so much for checking. @qlzh727 FYI. ', b""From what I investigated, the error is raised by `Dimension` class. It is called with `dim` argument of value `<attribute 'shape' of 'numpy.generic' objects>` of type `<class 'getset_descriptor'>` instead of `int`, `None` or `Dimension` object. I'm not really sure where it comes from but I think it may be some code in `__call__()` method of `base_layer.Layer` - the stack goes to there and the method was changed between 2.5.0 and 2.7.0, but I may be wrong.\r\n\r\nI'm not really advanced in debugging such a big library but I hope it will help someone in finding the problem."", b'I have the same problem, but I really need it to run in colab, maybe you could recommend how to implement that part of the code?']",open,2021-10-21 19:59:18,
https://api.github.com/repos/keras-team/keras/issues/15529,b'Add compute_output_shape in TFOpLambda',"[b'@fchollet Thanks for your advice in https://github.com/keras-team/keras/pull/15381, I create a new one for that.', b'I wonder why this pr merging is blocked?', b""@fancyXun unfortunately, it appears that a test is failing:\r\n\r\n```\r\nkeras/layers/core:core_test\r\n```\r\n\r\nSpecifically\r\n\r\n```\r\nTFOpLambdaTest.test_compute_output_shape_v1_session\r\n```\r\n\r\nPlease fix the failing test and push your changes. We'll re-approve the PR and attempt to merge it. Thank you!"", b""@FancyXun Can you please check @fchollet's comments and keep us posted ? Thanks!"", b'@FancyXun Any update on this PR? Please. Thanks!']",open,2021-10-21 02:55:09,
https://api.github.com/repos/keras-team/keras/issues/15528,b'Mixed precision training incompatible with BinaryCrossentropy label smoothing',"[b'@ghup1 \r\n\r\nIn such a case, cast the last layer output as follows (set `tf.float32`). \r\n\r\n```python\r\n# your last layer of the model \r\ntf.keras.layers.Conv2D(..., dtype=tf.float32)\r\n```', b'Hi @ghup1 , would you be open to opening a pr that adds a cast to:\r\n```\r\n    F:\\conda_env\\[redacted]\\lib\\site-packages\\keras\\losses.py:1803 _smooth_labels\r\n        return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\r\n```\r\n(probably cast the output of the label_smoothing array to the type of y_true, but should be checked)\r\nAnd add a unit test?', b'@innat This solution is a nice workaround, but not sure if this is expected behaviour. However, I can continue with my work. Thanks!\r\n\r\n@tomerk I do not have such knowledge of github, pull requests and keras. Not sure how to solve this case specifically. Unit testing should be pretty straight forward though.']",open,2021-10-20 15:51:03,
https://api.github.com/repos/keras-team/keras/issues/15522,b'InvalidArgumentError: slice index 22 of dimension 0 out of bounds. (reproducible code is given / colab).',"[b'It seems I need to ensure this `number of samples % batchsize = 0`. But it limits the batch size. For 3670 samples I need to set batch size 10 where a higher batch size can be set. ', b""Hello @innat , I don't seem to fine the error quoted in the reproducible colab. Can you check into this?"", b""@rchao did u mean yoy couldn't find the error?  Can you check again, if so. Just run the code,  it should appear. Note, it's not exactly an error but most likely a situation."", b'I can now see the error. My guess is that your map function has an error inside when processing the input. Can you first try adding `tf.print` statements inside your map function and attempt to see at which point it errors out?', b'`tf.data.experimental.enable_debug_mode` may also be worth trying out: https://www.tensorflow.org/api_docs/python/tf/data/experimental/enable_debug_mode', b""@rchao I'm not sure how to solve this, it's repeating anyway. Could you please assist? "", b'I would do `tf.data.experimental.enable_debug_mode` and `tf.print` inside the map function, and see the logs to find out more. Hope this helps!', b""@rchao thanks. I tried that already but had no luck finding any precise information. \r\nHowever, I think the issue might cause of `tf.keras.utils.image_dataset_from_directory`.  \r\n\r\nWe did one test. As my suspicion for `tf.keras.utils.image_dataset_from_directory` API which already gives batch samples, I used the same different training pipelines (without this API and didn't face any issue). In the colab notebook, [here](https://colab.research.google.com/drive/1T-8bgNWGq6ZJ_etvVKXCsXq7TDvcTVhd?usp=sharing), we used `tf.keras.utils.image_dataset_from_directory` but the in the kaggle-notebook, [here](https://www.kaggle.com/ipythonx/github-issue-15522-temp-notebook/notebook), we didn't face such problem. Precisely, in colab notebook, you would find:\r\n\r\n```\r\ndef cutmix(): <- custom function, it takes a batch of images and corresponding samples. \r\n\r\ntrain_ds = tf.keras.utils.image_dataset_from_directory( ... ) \r\ntrain_ds = train_ds.map(lambda x, y: cutmix(x,y)) <--- Raise PROBLEM \r\n```\r\n\r\nIt causes the above-stated issue (title). But if we use (which is shared in the kaggle notebook above) as follows, it works just fine, \r\n\r\n```\r\ndataset = tf.data.Dataset.from_tensor_slices((df['image_id'].values, \r\n                                                      df['label'].values)\r\ndataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\r\ndataset = dataset.shuffle(1024, reshuffle_each_iteration = True)\r\ndataset = dataset.batch(batch_size, drop_remainder=True)\r\ndataset = dataset.map(lambda x, y: cutmix(x,y)) <- same function  < ---- WORKS FINE \r\n```""]",open,2021-10-19 11:07:59,
https://api.github.com/repos/keras-team/keras/issues/15513,b'Performance of a model loading the same weights changes depending on the version of tensorflow ',"[b'@Jon573, In order to reproduce the issue reported here, could you please provide the complete code and supporting files? Thanks!', b""Here is some quick code I can reproduce the issue with. Set the CHECKPOINT_PATH to where you want, set EVALUATE to False, and run with TensorFlow 2.4.3. You should get an accuracy of ~0.65 . Change EVALUATE to True and run with with TensorFlow 2.4.3 and you should get the same accuracy (no surprises there). Then, with EVALUATE still True, run with TensorFlow 2.6.0 and I get an accuracy of ~0.1 . \r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nCHECKPOINT_PATH = ''\r\nEVALUATE = True\r\n\r\ndef get_dataset():\r\n    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\r\n    train_images, test_images = tf.image.per_image_standardization(train_images) , tf.image.per_image_standardization(test_images)\r\n    return (train_images, train_labels, test_images, test_labels)\r\n\r\ndef get_model(load_weights):\r\n    model = tf.keras.models.Sequential()\r\n    base_model = tf.keras.applications.EfficientNetB0(\r\n                                            input_shape=(32,32,3),\r\n                                            weights='imagenet',\r\n                                            include_top = False,\r\n                                            ) \r\n    model.add(base_model)\r\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\r\n    model.add(tf.keras.layers.Dropout(0.5))\r\n    model.add(tf.keras.layers.Dense(10, activation = 'softmax', dtype='float32'))\r\n\r\n    model.compile(optimizer='adam',\r\n                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n                metrics=['accuracy'])\r\n    \r\n    if load_weights:\r\n        model.load_weights(CHECKPOINT_PATH)\r\n    \r\n    return(model)\r\n\r\ndef train_model(model, train_images, train_labels,test_images, test_labels):\r\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=CHECKPOINT_PATH,\r\n        save_weights_only=True,\r\n        monitor='val_accuracy',\r\n        mode='max',\r\n        save_best_only=True)\r\n    model.fit(train_images, \r\n                train_labels, \r\n                epochs=1, \r\n                validation_data=(test_images, test_labels),\r\n                callbacks=[model_checkpoint_callback]\r\n                )\r\n\r\ndef evaluate(model, train_images, train_labels,test_images, test_labels):\r\n    _, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\r\n    print(test_acc)\r\n\r\ndef main():\r\n    data = get_dataset()\r\n    if EVALUATE:\r\n        model = get_model(load_weights = True)\r\n        evaluate(model, *data)\r\n    else:\r\n        model = get_model(load_weights = False)\r\n        train_model(model, *data)\r\n        evaluate(model, *data)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n"", b'@chunduriv Any idea what could be causing the difference in performance?', b'@Jon573, While trying to execute the code, facing `OSError: Unable to open file` error. \r\n\r\nCould you please provide colab gist, it helps to analyze the issue?', b""@chunduriv\r\n\r\nAre you setting EVALUATE to False first, running the script in 2.4.3 and then subsequently setting it to True and running in 2.4.3 and then 2.6.0? If EVALUATE is set to True and you run it then there wont be a file for it to load and you will get that error.\r\n\r\nAlso, are you sure you set the CHECKPOINT_PATH correctly? for example on Linux, it could be something like '/home/username/path/to/wherever/checkpoint' for an absolute path.\r\n\r\n"", b'@Jon573, In general, the models improve with more epochs of training. Did you checked performance of model by increasing the number of epochs from `1` to `10`?', b""@chunduriv, as described in the title and my description of the problem, the problem isn't the performance resulting from the number of epochs trained. I used 1 epoch here so that the code used to reproduce the issue would not take long to run. The problem is that when you train a network in TensorFlow 2.4.3 and then in TensorFlow 2.6.0 use the exact same model architecture (in this case a keras sequential model based on EfficientNetB0) with the weights from the training in TensorFlow 2.4.3, the performance is terrible. This, to my understanding, should not happen.\r\n\r\n"", b'Hi @Jon573, would you be able to do a bisection with running the published tf nightlies:\r\nhttps://pypi.org/project/tf-nightly/\r\n\r\n(And run against tf 2.5 as well)\r\nTo see if you can identify at what point it breaks? We only have some of the later tf 2.6 nightlies available on pypi, so let us know if even the oldest nightlies available are broken.', b""Hi @tomerk, I have managed to get a bit of time to do a bit of investigation (I didn't get around to testing the nightly pip installs as I found issues with 2.5.0). I pulled docker images of 2.4.3-gpu, 2.5.0-gpu, and 2.6.0-gpu from docker to make sure it was in as clean environment as possible. I reconfirmed my findings (between 2.4.3 and 2.6.0). Then using the weights from 2.4.3 environment training, I got the same results in 2.5.0 i.e. performance wasn't good. I then trained (i.e. set EVALUATE to false in my script) in 2.5.0 and got an accuracy of 0.22 (in 2.5.0). Trained and tested in 2.4.3, the accuracy was ~0.65. So there is some change between 2.4.3 and 2.5.0 that causes performance issues when using the code above. What is quite confusing is that performance when trained and tested in the same version is not the same for 2.4.3 and 2.5.0. Below is a summary of the accuracy I got with the different combinations of trained versions and the versions the weights are evaluated in.\r\n\r\n(weights from version, version evaluated in) = ~accuracy\r\n(2.4.3, 2.4.3) = ~0.65\r\n(2.4.3, 2.5.0) = ~0.10\r\n(2.5.0, 2.5.0) = ~0.22\r\n(2.5.0, 2.4.3) = ~0.08                              \r\n\r\nFor all of them I get the warnings if I am just loading the weights and evaluating (which is a bit weird):\r\n```\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\r\n```\r\n\r\nBut loading 2.4.3 weights into 2.5.0, I get the additional warning:\r\n```\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.layer_with_weights-0.state_variables\r\n```\r\n\r\nAnd loading 2.5.0 weights into 2.4.3 I get an additional:\r\n```\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.layer_with_weights-0._keep_axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.layer_with_weights-0._reduce_axis\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.layer_with_weights-0._reduce_axis_mask\r\n```\r\n\r\nDid the way weights get loaded change between 2.4.3 and 2.5.0? Was there some fundamental change in training between 2.4.3 and 2.5.0 (I don't know why there is a performance difference between the two)? Can you reproduce any of this or am I using it wrong?\r\n\r\n"", b""@tomerk It was annoying me why performance when trained and tested in the same version is not the same for 2.4.3 and 2.5.0. Turns out there was a change in behaviour between 2.4.3 and 2.5.0 of tf.image.per_image_standardization. In 2.4.3, tf.image.per_image_standardization returns a tensor of the same shape and dtype as the input, but in 2.5.0 and onward, it returns a tensor of the same shape and dtype float32. This completely changes the input tensor to the network and explains all of the behaviour I was seeing. I'm guessing this is the wrong repo to ask for a note to be put in the docs making it more obvious that the behaviour of the function has changed?"", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""That's a really good catch, thanks @Jon573 !\r\nI'm guessing it was probably https://github.com/tensorflow/tensorflow/commit/b6be9714e878a7dd0d1405bd7a83e021ba4b561a\r\n\r\nBut it looks like the release notes release.md was not updated at the time even though it should have been.\r\nYou're right this issue would probably be better filed against the base tensorflow repo:\r\nhttps://github.com/tensorflow/tensorflow\r\n\r\nI'm not sure the behavior will be changed if it was originally a fix of an incorrect behavior change. But, the team can at least try updating release.md for past versions (not sure if it would get updated on already-published release notes though).\r\n\r\n"", b'@chunduriv is there a good process to move the issue over or cc teammates from the other tf repo here? (E.g. rohan100jain@ )']",open,2021-10-14 11:15:48,
https://api.github.com/repos/keras-team/keras/issues/15512,"b""keras.models.load_model resets the optimizer's state""","[b'@SiLiKhon I think the error is expected because when you print the following\r\n\r\n```\r\nreconstructed_model = tf.keras.models.load_model(""my_model"", compile=False)\r\nfor w in reconstructed_model.optimizer.weights:\r\n    print(w.shape)\r\n\r\nprint(reconstructed_model.optimizer)  # outputs <keras.optimizer_v2.optimizer_v2.RestoredOptimizer at 0x7fdebd716950>\r\n```\r\nThe above optimizer is unknown to keras and it throws `NotImplementedError`.\r\n\r\nAlternatively, you can save weights along with `model`. During model loading, you can load  model and then load weights as shown below\r\n\r\n```\r\n# Calling `save(\'my_model\')` creates a SavedModel folder `my_model`.\r\nmodel.save(""my_model"")\r\nmodel.save_weights(\'my_weights\')\r\n\r\n# It can be used to reconstruct the model identically.\r\nreconstructed_model = tf.keras.models.load_model(""my_model"")\r\nreconstructed_model.load_weights(\'my_weights\')\r\n\r\nreconstructed_model.compile(reconstructed_model.optimizer, loss=""mean_squared_error"")\r\n# reconstructed_model.compile(optimizer=""adam"", loss=""mean_squared_error"")\r\nreconstructed_model.fit(test_input, test_target)\r\n```\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/96f74a40762db16fdd873baf42eb674d/untitled1084.ipynb)\r\n\r\nI am not sure about your use-case. If you want to retrain the model from where it was left, you can load model and retrain (without recompiling). Thanks!', b""@jvishnuvardhan thanks for the reply!\r\n\r\ntbh, I don't quite understand: I thought that a call to `tf.keras.models.load_model` should be enough to restore both the model and the optimizer. It's what seems to be implied in the example from [this section of the tutorial](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading), e.g. there is the following comment in that code example:\r\n```python\r\n# The reconstructed model is already compiled and has retained the optimizer\r\n# state, so training can resume:\r\n```\r\n\r\nIf, however, you run the example code, the optimizer's state will not be restored. Adding separate `save_weights` and `load_weights` calls as in your snippet does fix the issue, but to me its super counterintuitive as to why both of `save`/`load_model` and `save_weights`/`load_weights` are needed.\r\n\r\n> I am not sure about your use-case. If you want to retrain the model from where it was left, you can load model and retrain (without recompiling).\r\n\r\nThat's exactly my use-case. And I'm saying that just a pair of `save`/`load_model` calls doesn't do its job in this :)"", b'For Adam and optimizers with SLOTS I think that we have still this https://github.com/tensorflow/tensorflow/issues/44670', b'@k-w-w Please take a look at this issue. Thanks', b'https://github.com/keras-team/keras/blob/0b1d504ba21cf917a4f42055aba6bdf91fa5e5d6/keras/saving/saved_model/load.py#L161-L166', b'As per https://github.com/tensorflow/tensorflow/issues/44670#issuecomment-725215383 this _is_ solvable. @bhack what is holding up this being fixed? Is the implementation really that hard?', b'@adriangb If you want to know my opinion as a first step I will try write a first PR with some expecting failing test to cover this missing feature. \r\n\r\nE.g. like in https://github.com/tensorflow/tensorflow/pull/51538/files\r\n\r\nWhen the test PR is approved and so the team agree on the use cases coverage of this feature I think that we could wait for another user contributed PR that invalidate these test so we can remove the expecting failure annotation and close this bug. \r\n\r\nSometimes the feature are also implemented by the natural internal coding activity so the failing tests IMHO are still useful to fail in the case it will be solve by some internal development as a natural way to monitor open confirmed tickets.\r\n\r\nThis is just my own view as probably someone else might not like the fact of making a feature request (or bug) with just expecting failing tests.\r\n', b'Looks like I created a dupe of this by accident here: https://github.com/tensorflow/tensorflow/issues/53064.\r\nSince this might not get fixed soon, can you please put a disclaimer into the docs that this does not work? It was a big surprise to me, especially since it works flawlessly in TF1. Due to it failing silently, it\'s very hard to catch and can have really bad consequences if it\'s not caught.\r\nI also couldn\'t find anything in the docs that states anything similar to (""you cannot restore an optimizer\'s state""). I think this is a core functionality of TF and should not fail silently.', b'@janhartman Is this warning not working in your case:\r\nhttps://github.com/keras-team/keras/blob/0b1d504ba21cf917a4f42055aba6bdf91fa5e5d6/keras/saving/saved_model/load.py#L161-L166\r\n', b""@bhack Check out the [notebook I linked in my issue](https://colab.research.google.com/drive/1WyRoHFnNvoFocqm7jKC1CiH0KzzKh2lk?usp=sharing):  I don't see the warning in Colab or on my machine. Regardless of the warning, this should still be put into the docs."", b""+1 for plastering this warning all over the docs. I would even go so far as to making this an error (only if fit is called). TensorFlow emits all sorts of warnings left and right; even if this did emit 1 warning there's a lot of noise. It's a relatively obscure and hard to detect bug since you can only see if via the resultant data/weights/training results. I could easily see this causing great harm to research projects or real world applications."", b'@adriangb By an historical point of view you can read a little bit the discussion of this warning at https://github.com/tensorflow/tensorflow/pull/42846 \r\n\r\nI still believe that an expected failing test PR like in https://github.com/tensorflow/tensorflow/pull/51538/files could really help and also support a community fix. At least when failing test are merged we could have a good overview of what tests will need to pass to implement/resolve this missing feature.\r\n\r\nAre you interested to try to contribute a PR to extend the tests with this (expected) failing case?', b'https://github.com/keras-team/keras/pull/15661', b'> Looks like I created a dupe of this by accident here: [tensorflow/tensorflow#53064](https://github.com/tensorflow/tensorflow/issues/53064). Since this might not get fixed soon, can you please put a disclaimer into the docs that this does not work? It was a big surprise to me, especially since it works flawlessly in TF1. Due to it failing silently, it\'s very hard to catch and can have really bad consequences if it\'s not caught. I also couldn\'t find anything in the docs that states anything similar to (""you cannot restore an optimizer\'s state""). I think this is a core functionality of TF and should not fail silently.\r\n\r\nSame situation here. But I find if the model is saved in H5 format, the optimizer states will be restored. Is it a bug in SavedModel format?', b'Yes, it is primarily a bug in SavedModel. I believe that, as you say, H5 works fine.']",open,2021-10-14 09:47:03,
https://api.github.com/repos/keras-team/keras/issues/15509,b'Allow for a customized loss reduction when training with custom train_step in a MirroredStrategy',"[b""I'll bring up with the team whether we want to add a custom loss reduction as an argument to fit or add_loss. Pre-emptively cc'ing @rchao or @yuefengz who may have some thoughts on this.\r\n\r\nIn the meantime, as you've seen you are welcome to override make_train_function to use your own custom reduction. We're currently working on some training guides that elaborate a bit more on how you can override make_train_function and fit in addition to train_step. (@LukeWood )"", b""Thanks for reaching out @kretes! This makes sense to me and we're happy to provide the flexibility of specifying the reduction method used in the `step_function`. Would you be interested in sending us a PR to review?""]",open,2021-10-14 05:59:50,
https://api.github.com/repos/keras-team/keras/issues/15508,b'keras.layers.IntegerLookup fails to deserialize vocubulary',"[b'@mattdangerw I think this is a preprocessing layer? Could you take a quick look?', b""I've also run into this problem. Any temporary patch for this?"", b'Took a look. We currently don\'t support `saving -> loading -> cloning` as path with the lookup layers because of saving vocabulary files. In this case, we don\'t want to leak the old config file name to new models which might be restored in a new environment where the vocab file name is not available.\r\n\r\nI think there\'s a few we should probably do here:\r\n\r\n 1) We could support the `saving -> loading -> cloning` workflow for non-file vocabularies.\r\n 2) Ideally we can error out helpfully when attempting to get run `get_config` on a lookup layer with a file vocabulary that has been saved and restored.\r\n 3) Lastly, an ""everything works"" solution would involve updating any vocabulary file paths in a lookup layer config to the saved model asset path for the vocab file on save/load. I\'m not sure we have all the APIs we would need to support this today.\r\n\r\nI\'m not sure if 3) will be immediately actionable, but 1) and 2) should be things we could start on.', b""I don't believe there is a vocab file. It's a python list. "", b'Yeah, in the colab you posted, 1) would be a sufficient fix.', b'Oops --- I completely misread your message.  It all makes sense.  I apologize for my errant comment.']",open,2021-10-13 23:56:04,
https://api.github.com/repos/keras-team/keras/issues/15494,b'Addition of Resnet18 Model in Application',"[b""@MrinalTyagi \nIt's already been asked, [here](https://github.com/keras-team/keras/issues/15269)"", b'Duplicate of #https://github.com/keras-team/keras/issues/15269\r\n\r\nIs it possible for you and @innat to add this to `tf-addons`. Based on usage, this may be moved under `tf.keras.applications`. Thanks!', b""@jvishnuvardhan \r\nSorry, it's not clear to me the above comment. Is there any model application in `tf-addons`? I think it's [well used](https://github.com/keras-team/keras-applications/issues/151) and worthy to add under `tf.keras.applicaitons`. "", b'> Duplicate of ##15269\r\n> \r\n> Is it possible for you and @innat to add this to `tf-addons`. Based on usage, this may be moved under `tf.keras.applications`. Thanks!\r\n\r\nyes I can surely try that but would require some more information on how to submit the required pr for that.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'> Duplicate of #https://github.com/keras-team/keras/issues/15269\n> \n> \n> \n> Is it possible for you and @innat to add this to `tf-addons`. Based on usage, this may be moved under `tf.keras.applications`. Thanks!\n\nany update on this?', b'any update guys?\r\n']",open,2021-10-12 11:16:33,
https://api.github.com/repos/keras-team/keras/issues/15490,b'[Feature Request] ConvND and ConvNDTranspose layers',"[b""Triage note: it doesn't seem to be a good idea to have N >= 4 since the cost is too high. Wondering what cases are you thinking about that would need N >= 4?"", b'In the particular case I was working after analyzing it, I came to the conclusion that I can solve it with other methods. However I still think 4d convolution can be useful despite the computational cost.']",open,2021-10-11 21:16:24,
https://api.github.com/repos/keras-team/keras/issues/15488,b'Subclass StringLookup directly from TextVectorization',[],open,2021-10-11 19:26:02,
https://api.github.com/repos/keras-team/keras/issues/15477,"b'Keras Optimizer: Move the logic of distributed_apply and whether a strategy support merge call to tf.distribute, and reference from Keras via a tf.__internal__ endpoint.'",[],open,2021-10-08 18:50:25,
https://api.github.com/repos/keras-team/keras/issues/15475,b'Using the same seed kwarg returns different values between GlorotUniform and GlorotUniformV2',"[b'@sachinprasadhs Was able to reproduce the issue on colab using `TF v2.6.0` ,Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/b14b4e3b9e46ac918dd4d367404a6de2/glorotuniforminitializationwithseedissue.ipynb) for reference.Thank you!', b'Is this the old https://stackoverflow.com/questions/61960609/why-is-rng-different-for-tensorflow-2-and-1 ?', b""This SO article (which I included in the gist) tells us that including the operation seed causes the backend to call a different RNG function.\r\n\r\nThe problem is that GlorotUniform generates totally different values between TF1 and TF2 versions, even when using the same global and operation seed. Even If the backend is different, the result should be the same if I set up the global and operation seeds in both cases.\r\n\r\nIf I'm migrating from TF1 and TF2, then this behavior is unexpected and might lead me to think that my migration went wrong."", b'Yes in TF2 `random_uniform` is conditional https://github.com/keras-team/keras/blob/5817ec5745a844a924065c19f4a9ee1cc4a5c66e/keras/backend.py#L1825-L1833\r\n\r\nYou can reproduce the same behavior in TF2 with `keras.backend.disable_generator_for_rng()`', b""@qlzh727 This symbol is not exported right? So isn't available in `tensorflow.keras` but only in `keras` right?"", b""Hi @fernandobperezm,\r\nThis is expected behavior for the initializers themselves. So it's not a bug in our view, esp. that the random seed in not part of the official public API. However, your use case is an important one and we'll look into providing a workaround.\r\n@tomerk, any thoughts on this? Is there a known work around or can we provide one to migration users?"", b""As I've mentioend `keras.backend.disable_generator_for_rng` is not part of the public API `from tensorflow import keras` but it is still part of `import keras`.\r\n\r\nBuf if we search in the source code it is not used anywhere."", b'The `disable_generator_for_rng ` is not intended for public API usage, and this is only a flag to hide the feature that is under development. We will try to turn on the feature when we are ready, and the flag is used for temp/debug usage at the moment.', b""`import keras` doesn't give you public API, it directly access the Keras package python code. User should always prefer `from tensorflow import keras`."", b'Yes but currently I think it is the only workaround that we have also if it is still not officially supported', b""Quick note: We're going to check the viability of including this in the migration correctness validation tooling that is already intended to aid migrating random number generation between tf1 and tf2:\r\nhttps://www.tensorflow.org/guide/migrate/validate_correctness"", b""Hi all, these week's I've been super busy and haven't had the time to see the updates on this issue.\r\n\r\n> Yes in TF2 `random_uniform` is conditional\r\n> \r\n> https://github.com/keras-team/keras/blob/5817ec5745a844a924065c19f4a9ee1cc4a5c66e/keras/backend.py#L1825-L1833\r\n> \r\n> You can reproduce the same behavior in TF2 with `keras.backend.disable_generator_for_rng()`\r\n\r\nThanks! I'll have a look at that. On the other hand, this made me think about a possible problem to solve, i.e., is there any way to make `tf.random.Generator.from_seed(...).uniform(...)` to return the same random numbers than `tf.random.uniform(...)`? Are the underlying algorithms different?\r\n\r\n> Hi @fernandobperezm,\r\n> This is expected behavior for the initializers themselves. So it's not a bug in our view, esp. that the random seed in not part of the official public API. However, your use case is an important one and we'll look into providing a workaround.\r\n> @tomerk, any thoughts on this? Is there a known work around or can we provide one to migration users?\r\n\r\nThanks for considering my use case. It seems a little odd that using random seeds is not part of the official public API of Keras when Tensorflow have them in their public API (I know that Keras is different than TF, but from the docs perspective they're are same). Is there any way I can help to provide a workaround?\r\n\r\n> Quick note: We're going to check the viability of including this in the migration correctness validation tooling that is already intended to aid migrating random number generation between tf1 and tf2:\r\n> https://www.tensorflow.org/guide/migrate/validate_correctness\r\n\r\nHow do you guys plan to include this in the migration tutorial? I may try the same steps if they do not involve using tf-nightly (sadly, I cannot use non-released TF versions in my pipeline).\r\n\r\n"", b""> Hi all, these week's I've been super busy and haven't had the time to see the updates on this issue.\r\n> \r\n> > Yes in TF2 `random_uniform` is conditional\r\n> > https://github.com/keras-team/keras/blob/5817ec5745a844a924065c19f4a9ee1cc4a5c66e/keras/backend.py#L1825-L1833\r\n> > \r\n> > You can reproduce the same behavior in TF2 with `keras.backend.disable_generator_for_rng()`\r\n> \r\n> Thanks! I'll have a look at that. On the other hand, this made me think about a possible problem to solve, i.e., is there any way to make `tf.random.Generator.from_seed(...).uniform(...)` to return the same random numbers than `tf.random.uniform(...)`? Are the underlying algorithms different?\r\n\r\nThe tf.random.Generator are using stateless ops, which is very different from tf.random.uniform. So it is not possible to produce same result with same seed.\r\n> \r\n> > Hi @fernandobperezm,\r\n> > This is expected behavior for the initializers themselves. So it's not a bug in our view, esp. that the random seed in not part of the official public API. However, your use case is an important one and we'll look into providing a workaround.\r\n> > @tomerk, any thoughts on this? Is there a known work around or can we provide one to migration users?\r\n> \r\n> Thanks for considering my use case. It seems a little odd that using random seeds is not part of the official public API of Keras when Tensorflow have them in their public API (I know that Keras is different than TF, but from the docs perspective they're are same). Is there any way I can help to provide a workaround?\r\n> \r\n> > Quick note: We're going to check the viability of including this in the migration correctness validation tooling that is already intended to aid migrating random number generation between tf1 and tf2:\r\n> > https://www.tensorflow.org/guide/migrate/validate_correctness\r\n> \r\n> How do you guys plan to include this in the migration tutorial? I may try the same steps if they do not involve using tf-nightly (sadly, I cannot use non-released TF versions in my pipeline).\r\n\r\nI will let @tomerk to comment more since we are working on some migration tool to help user have a fixed seed behavior.\r\n""]",open,2021-10-08 14:05:55,
https://api.github.com/repos/keras-team/keras/issues/15474,b'Reuse of layers with mixed precision produce multiple float16 copies of weights',"[b'@tringholm I tried to reproduce the issue on colab  using TF v2.6 ,Could Please find the [gist](https://colab.research.google.com/gist/sushreebarsa/68e000c8f52582cc78841f29206e9745/untitled455.ipynb) and confirm the issue reported here?Thank you!', b'The gist confirms the issue. With num_iterations=100 I get\r\n```\r\nResourceExhaustedError:  failed to allocate memory\r\n\t [[node gradient_tape/model_4/dense_4/MatMul_22/Cast/Cast (defined at <ipython-input-5-aa60ff95d6fb>:33) ]]\r\n```\r\n\r\nAlso, removing the need for casting the layer\'s variables by setting\r\n`dense_layer = Dense(layer_dim, dtype=tf.float16)`\r\nresolves the memory issue although it decreases the precision of the layer\'s stored variables, which is not the intent of mixed precision.\r\n\r\nA workaround that has helped for our (subclassed) layers is to keep a float32 master copy of weights, and then explicitly cast these before computations, something like this:\r\n\r\n```\r\nclass OuterLayer(tf.keras.layers.Layer):\r\n    def __init__(self, inner_size, loop_size):\r\n        self.inner_layer = InnerLayer(inner_size)\r\n        self.loop_size = loop_size\r\n\r\n    def __call__(self, inputs):\r\n        self.inner_layer.setup()\r\n        x = self.inner_layer(inputs)\r\n        for i in range(self.loop_size):\r\n            x = self.inner_layer(x)\r\n        return x\r\n\r\n\r\nclass InnerLayer(tf.keras.layers.Layer):\r\n    def __init__(self, size):\r\n        self.variable_master = self.add_weight(""variable"", shape=[size, size])\r\n        self.variable = None\r\n\r\n    def setup(self):\r\n        self.variable = tf.cast(self.variable_master, self.compute_dtype)\r\n\r\n    def call(self, inputs):\r\n        return tf.linalg.matmul(self.variable, inputs)\r\n```\r\n\r\nbut this is somewhat verbose and feels like it shouldn\'t be necessary. Perhaps the AutoCastVariable class could maintain the float16 copies?', b""I agree this is inconvenient, but unfortunately there's currently no solution currently to cache the casted value of AutoCastVariable. This is a tricky problem to solve, since a layer can modify the value of the weights in `call`, which would invalid the cached value if we did implement caching.\r\n\r\nAnother workaround for subclassed layers is to create the variable in `OuterLayer`, call `variable.read_value()` in `OuterLayer.call` (which causes the variable to be automatically casted), then pass the value to `InnerLayer.call`. This way the variable is read only once, so it is only casted once.\r\n\r\nFor some of the built-in RNN classes, the [variable is read outside the loop](https://github.com/keras-team/keras/blob/f5171d521acbf2ebbb6414352d5792163c41479f/keras/layers/recurrent_v2.py#L1194), so that the variable is casted only once per step instead of once per iteration of the RNN. For custom RNNs, currently you would need to implement that logic yourself to avoid creating many float16 copies of the variable.""]",open,2021-10-08 09:04:27,
https://api.github.com/repos/keras-team/keras/issues/15460,b'tf.keras.metrics.RootMeanSquaredError() not commutative for higher dimensional tensors',"[b""@MathiesW  I tried to run your code on colab using [2.5.0-gpu](https://colab.research.google.com/gist/sushreebarsa/064d13ca878c72688df36c9d872e1699/untitled439.ipynb) ,[2.6.0-gpu](https://colab.research.google.com/gist/sushreebarsa/88360ff0e1d87c157e8e807853ac02db/untitled438.ipynb#scrollTo=F0aPQV4yfg8q) and didn't face the issue reported here .Could you please have a look on the attached gists and let us know if it helps ? Thank you! "", b'@sushreebarsa thanks for the fast reply. Your attached gists work like a charm. This is interesting, I will have another look at my Python environment tomorrow.', b'Hi @sushreebarsa again! I managed to reproduce the error. Please take a look at my code on colab!\r\n[tf-2.6.0](https://colab.research.google.com/gist/MathiesW/5e7c7254806950dfb5b6abac71bbb47a/untitled438.ipynb)', b'@sachinprasadhs Was able to reproduce the issue on colab using TF-[2.6.0](https://colab.research.google.com/gist/sushreebarsa/71731375c309c9da08e5c726a773a15d/untitled441.ipynb) and [tf-nightly ](https://colab.research.google.com/gist/sushreebarsa/2e6a15e61346237d09e7dbc1c1b8306b/untitled442.ipynb),Please find the attached gists for reference.Thank you!']",open,2021-10-06 13:38:38,
https://api.github.com/repos/keras-team/keras/issues/15452,b'Support SPMD xla partitioning in TPUStrategyV2.',[],open,2021-10-04 18:43:05,
https://api.github.com/repos/keras-team/keras/issues/15451,b'Normalization layer masking',"[b'Hey there!  We are interested in this feature, please go ahead and prepare a pull request if you are willing!', b'I would like to work on this \r\n', b'Go for it! This fell on my back burner', b""Related if you're here looking for a different workaround: created this version that plays nicely with `tf.data.Dataset`: https://gist.github.com/kydonian/ddda453c8707cabb1eb9fbcbbd335ffa""]",open,2021-10-04 18:25:45,
https://api.github.com/repos/keras-team/keras/issues/15429,b'Updated image_dataset.py with `label_mode` is None and interpolation is `bilinear` it yields `uint8`.',"[b""@Saduf2019 Can you please check @fchollet's comments and keep us posted ? Thanks!""]",open,2021-09-29 18:22:59,
https://api.github.com/repos/keras-team/keras/issues/15425,b'Add layer trainable information in model summary and plot model',"[b""Thanks for the suggestion. We think this could be a useful feature, so please feel free to open a PR to add an optional `trainable` column to the summary (let's make it as narrow as possible since it's just a boolean flag)."", b'This could be closed as a patch implementing it was already merged.']",open,2021-09-28 11:33:14,
https://api.github.com/repos/keras-team/keras/issues/15420,b'[BUG] Omitting loss in model.compile will result in unhelpful error message',"[b'You are using one hot encoded vectors as the target variable, but in the loss function you are using ""binary crossentropy"", which is not correct rather you should use ""categorical cross entropy"", the loss obtained using ""binary crossentropy"" for this problem is wrong.\r\nKindly refer to this for more details :- https://stackoverflow.com/questions/42081257/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances', b""@nilay121 The `loss` used here is tangential to the issue I reported, as I'm not reporting an issue with the convergence of some model. \r\n\r\nHowever, to answer your question, I am not using a one hot encoding, which you can verify with `y.sum(axis=1) == 1`. Thanks for bringing up as I noticed that I am using `softmax` instead of `sigmoid`, which I have corrected in the original post for correctness."", b'@xhlulu [#15441](https://github.com/keras-team/keras/pull/15441)']",open,2021-09-25 18:35:47,
https://api.github.com/repos/keras-team/keras/issues/15407,b'Collect slot variable restorations in a queue and restore them as a batch.',[],open,2021-09-22 18:53:27,
https://api.github.com/repos/keras-team/keras/issues/15392,"b'Validation is not run when using ""fit"" method if the validation set changes its size'","[b'@jvishnuvardhan  I could reproduce the issue with TF 2.6 and tf nightly .Please, find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/2a002d5a04f0877a56074474ab36d4df/51241.ipynb#scrollTo=RSMnc5XAg9Ba).Thanks!']",open,2021-09-20 07:23:18,
https://api.github.com/repos/keras-team/keras/issues/15376,b'Copy variable for every step leads to slow training speed?',"[b'I think the copy-on-read is not suitable for larger variables, For larger variables, the performance of using shared locks will be better\xef\xbc\x9f', b'I was able to reproduce the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/63f9ed182854ad7732ca265cca5c0d2a/test.ipynb).\r\n\r\noutput from the code is as shown below\r\n\r\n### Training with Estimator\r\n\r\n```\r\nINFO:tensorflow:loss = 0.027672563, step = 50 (0.357 sec)\r\nINFO:tensorflow:global_step/sec: 25.7492\r\nINFO:tensorflow:loss = 0.025328398, step = 60 (0.389 sec)\r\nINFO:tensorflow:global_step/sec: 24.8684\r\nINFO:tensorflow:loss = 0.023781756, step = 70 (0.402 sec)\r\nINFO:tensorflow:global_step/sec: 24.6785\r\nINFO:tensorflow:loss = 0.022232072, step = 80 (0.408 sec)\r\nINFO:tensorflow:global_step/sec: 26.0616\r\nINFO:tensorflow:loss = 0.021159796, step = 90 (0.382 sec)\r\n```\r\n\r\n### Trainning with Keras\r\n`100/100 [==============================] - 81s 697ms/step - loss: 0.8490`\r\n\r\nAnalysis (As mentioned by @PWZER )\r\nWe can see the Keras trainning speed is 697ms/step, but the Estimator trainning speed is 38ms/step, About 20 times.']",open,2021-09-16 10:26:53,
https://api.github.com/repos/keras-team/keras/issues/15367,b'Rework the keras optimizer: define the structure of the new optimizer base class.',[],open,2021-09-14 23:22:21,
https://api.github.com/repos/keras-team/keras/issues/15365,b'Inverse kinematic approximation with neural network',[],open,2021-09-14 15:25:28,
https://api.github.com/repos/keras-team/keras/issues/15357,b'Cannot access y_true shape using a custom loss',"[b'Obviously, using tf.reshape with the y_true tensor inside my loss function is impossible : \r\n\r\n```\r\nValueError: in user code:\r\n\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\r\n        return step_function(self, iterator)\r\n    <ipython-input-13-b8bb8e39775a>:23 yoloLoss  *\r\n        inde_noobj = tf.cast(tf.ones(shape=inde_obj.shape, dtype=tf.float32) - inde_obj, dtype=tf.float32)\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:3221 ones\r\n        shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\r\n        return func(*args, **kwargs)\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    /home/valentin/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:355 _tensor_shape_tensor_conversion_function\r\n        raise ValueError(\r\n\r\n    ValueError: Cannot convert a partially known TensorShape to a Tensor: <unknown>\r\n\r\n```\r\n\r\n\r\n\r\n ', b'I tried to add a wrapper to my loss function as suggested in : https://stackoverflow.com/a/46465774/10819754\r\n\r\nI did this : \r\n```\r\n\r\nyolov1 = get_darknet_19(224,224, output_shape=OUTPUT_SHAPE[-1])\r\n\r\ninput_tensor = tf.keras.Input(shape=(224,224,3))\r\noutputs = yolov1(input_tensor)\r\nyolov1.compile(loss=custom_loss_wrapper(input_tensor), optimizer=""adam"")\r\n\r\nx = np.random.randn(64, 224, 224, 3) \r\ny = np.random.randn(64, 7, 7, 26)\r\n\r\nyolov1.test_on_batch(x,y)\r\n```\r\n\r\nAnd got the following desirable output : \r\n\r\n```\r\ny_true Tensor(""IteratorGetNext:1"", shape=(64, 7, 7, 26), dtype=float32)\r\ny_pred Tensor(""Yolo/conv2d_21/BiasAdd:0"", shape=(64, 7, 7, 31), dtype=float32)\r\n```\r\n\r\nTho, I suspect my problem to be related to my data pipeline, if anyone can confirm it, this would be awesome :]']",open,2021-09-13 08:11:31,
https://api.github.com/repos/keras-team/keras/issues/15351,"b""Keras Doesn't Use `KERAS_HOME` for Downloading Data on Windows""",[],open,2021-09-11 11:18:56,
https://api.github.com/repos/keras-team/keras/issues/15348,b'Bugs found in Normalization() and load_model()',"[b""@rafak360 I tried to run your code of **bug_1.py** on Colab using **TF v2.6-GPU** and didn't face the **`Traceback error`**  please find the [gist](https://colab.research.google.com/gist/sushreebarsa/4141e0dbf98e48b49e4c2615a5049110/untitled195.ipynb) for reference. I was able to reproduce   **bug_2.py** on Colab using **TF v2.6-GPU** , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a2ff51702af464ca86812337ded3c1d5/untitled194.ipynb) and confirm the same. Could you please refer to the [link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) and let us know if it helps?Thank you!"", b'> @rafak360 I tried to run your code of **bug_1.py** on Colab using **TF v2.6-GPU** and didn\'t face the **`Traceback error`** please find the [gist](https://colab.research.google.com/gist/sushreebarsa/4141e0dbf98e48b49e4c2615a5049110/untitled195.ipynb) for reference. I was able to reproduce **bug_2.py** on Colab using **TF v2.6-GPU** , please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/a2ff51702af464ca86812337ded3c1d5/untitled194.ipynb) and confirm the same. Could you please refer to the [link](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) and let us know if it helps?Thank you!\r\n\r\nWhat you mean by ""didn\'t face the Traceback error""? I took a look at the gist link you sent me and clearly saw the ""**ValueError: All `axis` values to be kept must have known shape. Got axis: (-1,), input shape: [None, None], with unknown axis at index: 1**""\r\n\r\n**bug_2.py is confirmed too**, as you can see model_loaded_prediction is predicting 168 which is far away from the actual 123 expected value, that\'s a loss of accuracy.\r\n\r\n', b""This issue mainly occurs When **Sequential.add(Normalization layer)**. After **normalizer.adapt(train_data)**, Normalization's **built** would be set to **True**, which will not lead to [self.build(input_shapes)](https://github.com/keras-team/keras/blob/6859a01cf1df31b8f31e2e8b13aa5db793ec2b4e/keras/engine/base_layer.py#L2724) occurs, but if the model is reloaded, Normalization's built defaults to **False**, then it will call **self.build(input_shapes)**, the error happens.\r\n \r\nA workaround is to remove the suffix 'h5', Do not save model as h5 format."", b""I can reproduce the issue with `tf-nightly`.  These two issues are only with `h5` format.\r\n\r\n However, everything works as expected when `save_format`='tf'. [Here](https://colab.research.google.com/gist/jvishnuvardhan/adf7ac032c793b17b73a0f7d58634422/untitled1075.ipynb) is a gist for bug1. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/8f0248f8fdf8c7ada20dbead221ef00b/untitled1076.ipynb) is a gist for bug2. Thanks!"", b""Would it be sufficient to use the workaround by using `save_format='tf'`?  \r\n\r\nAssigned to Matt"", b""I'm following https://www.tensorflow.org/io/tutorials/elasticsearch with Tensorflow 2.6.0 and Tensorflow-io 0.21.0 and the issue is immediately present. Any advice for a workaround would be appreciated."", b'Any update to this? Using 2.6 model that was running fine under 2.5 and now presents the above traceback. Additional info, using the Apple Metal plugin. ', b""Saving with `save_format='tf'` is probably the correct workaround right now. We should really be today erroring out when trying to save the stateful preprocessing layers (TextVectorization, StringLookup, IntegerLookup, Normalization, Discretization) with h5 format.\r\n\r\nIt probably is quite possible to support h5 for this layer and Discretization though, I can take a look at that.\r\n\r\nIs there a reason the workaround of using save format 'tf' is not working?"", b""> I'm following https://www.tensorflow.org/io/tutorials/elasticsearch with Tensorflow 2.6.0 and Tensorflow-io 0.21.0 and the issue is immediately present. Any advice for a workaround would be appreciated.\r\n\r\nWe are also seeing the same issue in Colab with tensorflow-io version: 0.22.0\r\ntensorflow version: 2.7.0 and ES 7.15.2. \r\n"", b'`h5` format limitations listed in the Keras page mentioned that **Does not support preprocessing layers.**. Please check the page https://github.com/keras-team/keras-io/blob/master/guides/ipynb/serialization_and_saving.ipynb\r\n\r\n> H5 limitations:\r\n> - External losses & metrics added via `model.add_loss()`\r\n> & `model.add_metric()` are not saved (unlike SavedModel).\r\n> If you have such losses & metrics on your model and you want to resume training,\r\n> you need to add these losses back yourself after loading the model.\r\n> Note that this does not apply to losses/metrics created *inside* layers via\r\n> `self.add_loss()` & `self.add_metric()`. As long as the layer gets loaded,\r\n> these losses & metrics are kept, since they are part of the `call` method of the layer.\r\n> - The *computation graph of custom objects* such as custom layers\r\n> is not included in the saved file. At loading time, Keras will need access\r\n> to the Python classes/functions of these objects in order to reconstruct the model.\r\n> See [Custom objects](#custom-objects).\r\n> - Does not support preprocessing layers.', b""> `h5` format limitations listed in the Keras page mentioned that **Does not support preprocessing layers.**. Please check the page https://github.com/keras-team/keras-io/blob/master/guides/ipynb/serialization_and_saving.ipynb\r\n> \r\n> > H5 limitations:\r\n> > \r\n> > * External losses & metrics added via `model.add_loss()`\r\n> >   & `model.add_metric()` are not saved (unlike SavedModel).\r\n> >   If you have such losses & metrics on your model and you want to resume training,\r\n> >   you need to add these losses back yourself after loading the model.\r\n> >   Note that this does not apply to losses/metrics created _inside_ layers via\r\n> >   `self.add_loss()` & `self.add_metric()`. As long as the layer gets loaded,\r\n> >   these losses & metrics are kept, since they are part of the `call` method of the layer.\r\n> > * The _computation graph of custom objects_ such as custom layers\r\n> >   is not included in the saved file. At loading time, Keras will need access\r\n> >   to the Python classes/functions of these objects in order to reconstruct the model.\r\n> >   See [Custom objects](#custom-objects).\r\n> > * Does not support preprocessing layers.\r\n\r\nWell that documentation does not make sense for `tensorflow-gpu 2.5.0`, when specifying the preprocessing `Normalization` layer like: `normalizer = tf.keras.layers.experimental.preprocessing.Normalization(input_dim=5, axis=None)` it does seem to work fine with .h5 save formats. That's why I switch back to version 2.5, unfortunately it's been my workaround."", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Bug 2 should now be fixed on tf-nightly. Discretization and normalization should now load back with proper state with h5. Note that the lookup preprocessing layers (StringLookup, IntegerLookup and TextVectorization) will still not function properly with h5. Fix should go out in 2.8, workaround until it releases will be to use the tf save format or use nightly.\r\n\r\nFor bug 1, the fix most people will want is probably to pass `axis=None` to the Normalization layer. That will have the layer compute a single mean and variance over all the input. The default to the layer is `axis=-1` which computes a separate mean and variance for each position along the last axis. In that case, I think bug 1 is still valid, for some reason we are building the restored model with different input shape that when originally building it, triggering the error in the colab.', b'I am facing the same error, any solution for it?\r\n\r\nHere is my code: https://colab.research.google.com/drive/1RHgQ6KsOF7FDmbRAHPyQtXGCtsrAzD0h?usp=sharing', b'My particular issue was not related to loading models, rather it was creating Normalization layers in Keras. My solution was to drop back to 2.5 and all was good. I have not tried it in 2.7 as yet.  \n\nBest,\n\nIain\n\n\n\n> On Dec 12, 2021, at 1:05 PM, Pain ***@***.***> wrote:\n> \n> \xef\xbb\xbf\n> I am facing the same error, any solution for it?\n> \n> Here is my code: https://colab.research.google.com/drive/1RHgQ6KsOF7FDmbRAHPyQtXGCtsrAzD0h?usp=sharing\n> \n> \xe2\x80\x94\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n', b""> I am facing the same error, any solution for it?\r\n> \r\n> Here is my code: https://colab.research.google.com/drive/1RHgQ6KsOF7FDmbRAHPyQtXGCtsrAzD0h?usp=sharing\r\n\r\nYou should specify the `input_dim` according to your input dimensions and set the `axis` to None (see @mattdangerw's answer for more details) in its `preprocessing.Normalization()` parameters."", b""> > I am facing the same error, any solution for it?\r\n> > Here is my code: https://colab.research.google.com/drive/1RHgQ6KsOF7FDmbRAHPyQtXGCtsrAzD0h?usp=sharing\r\n> \r\n> You should specify the `input_dim` according to your input dimensions and set the `axis` to None (see @mattdangerw's answer for more details) in its `preprocessing.Normalization()` parameters.\r\n\r\nI did what you said and my problem has been solved. \r\n\r\nBut they mention in the tutorial that the layer can inspect the input shape ( assum that I assign axis to None).\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/regression#linear_regression"", b""> > > I am facing the same error, any solution for it?\r\n> > > Here is my code: https://colab.research.google.com/drive/1RHgQ6KsOF7FDmbRAHPyQtXGCtsrAzD0h?usp=sharing\r\n> > \r\n> > \r\n> > You should specify the `input_dim` according to your input dimensions and set the `axis` to None (see @mattdangerw's answer for more details) in its `preprocessing.Normalization()` parameters.\r\n> \r\n> I did what you said and my problem has been solved.\r\n> \r\n> But they mention in the tutorial that the layer can inspect the input shape ( assum that I assign axis to None).\r\n> \r\n> https://www.tensorflow.org/tutorials/keras/regression#linear_regression\r\n\r\nI'm glad I helped you. Unfortunately the tutorial is out of date as many other tensorflow and keras documentations."", b""This is very relevant. We have an ImageNet-pretrained model (an [EfficientNet](https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet), concretely) that was trained with tensorflow==2.5.0 and saved as an `hdf5` file. \r\n\r\nAs you know, TensorFlow's EfficientNet uses a [Normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) layer, so this bug affects us. We cannot load this model with tensorflow==2.7.0 because the inferences are totally different. The `mean` and `variance` attributes of the `Normalization` layer that are loaded do not correspond to the ones of the ImageNet database, being this the source of the problem.\r\n\r\nWill this be fixed on tensorflow==2.8.0? Or is there any other workaround? We have a lot of EfficientNets trained with tensorflow==2.5.0 so transforming them to 'tf'  is very inconvenient."", b'@alegaros would it be possible to try test on nightly? We do have a fix for Normalization with h5 coming in 2.8, but it would be good to check with your use case.', b'Thanks for your answer @mattdangerw. I have tried on TensorFlow nightly and everything seems fixed now. Minimal code for reproducibility:\r\n\r\n```python\r\n>>> import numpy as np\r\n>>> import tensorflow\r\n\r\n>>> print(tensorflow.__version__)\r\n\'2.9.0-dev20220104\' \r\n>>> model = tensorflow.keras.applications.EfficientNetB0(weights=""imagenet"")\r\n>>> model.save(\'wop.hdf5\')\r\n>>> model = tensorflow.keras.models.load_model(\'wop.hdf5\')\r\n>>> model.layers[2].mean\r\n<tf.Tensor: shape=(1, 1, 1, 3), dtype=float32, numpy=array([[[[0.485, 0.456, 0.406]]]], dtype=float32)>\r\n```\r\n\r\nAs you can see, the ImageNet\'s mean is preserved as a layer attribute. I have tried on `2.8.0-rc0` and it is fixed too. I have also tried my use case (model trained on tf 2.5.1), and could confirm the fix on `2.8.0-rc0` and `2.9.0-dev20220104`.\r\n', b'@rafak360 Is this issue got resolved for you? I see Bug 1  and Bug2 are not throwing any error when `tf-nightly` was used. Please check the gist for [bug 1](https://colab.research.google.com/gist/jvishnuvardhan/370814a7973523bdccb89e398801ef08/untitled1075.ipynb) and [bug2](https://colab.research.google.com/gist/jvishnuvardhan/c41ffe26350d7bf27073428fcac8b7fc/untitled1076.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!']",open,2021-09-11 03:39:58,
https://api.github.com/repos/keras-team/keras/issues/15345,"b""Sub-classed constraint doesn't appear to be called when added via add_weight""","[b'@EnderWiggin14 Could you please refer to the [link1](https://keras.io/guides/making_new_layers_and_models_via_subclassing/),[link2](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) and let us know if it helps? Thank you!', b""@sushreebarsa \r\n\r\nI had previously looked at [link2](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight), and after looking at [link1](https://keras.io/guides/making_new_layers_and_models_via_subclassing/), I can tell you neither of those solves my the issue above.\r\n\r\nNeither of those offer actual examples of producing a Constraint subclass or discuss the explicit requirements to make a constraint class.\r\n\r\nBut like I said, and the code should demonstrate, the in-class function titled `_constraint`, which uses the same function arguments as the constraint class `call` function, works where as `Constraint.call` is never called. Either that occurs, or there is something in the backend of Tensorflow or Keras that prevents the weights from being changed by the constraints and/or prevent `tf.print` calls from actually printing. That's for the sub-classed constraint. `_constraint` works as is expected."", b'@jvishnuvardhan Was able to reproduce the issue on colab using TF v2.6 and tf-nightly , please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/d7d961ccb88acbe89e07567e8ebccd3e/untitled200.ipynb) for reference. Thanks!']",open,2021-09-10 17:11:39,
https://api.github.com/repos/keras-team/keras/issues/15339,"b'Saving a composite model that includes a custom layer results in error - None has NoneType, but expected one of: bytes, unicode'","[b'@ymodak Was able to reproduce the issue on Colab using TF v2.5, 2.6 and tf-nightly ,Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/81ad940461fb13cdf0823b0e286918c2/15339.ipynb) for reference. Thanks!']",open,2021-09-10 03:30:38,
https://api.github.com/repos/keras-team/keras/issues/15332,b'Different results for add_loss() within TimeDistributed for eager vs. compiled execution of keras Layer',"[b'I can reproduce this with recent TF2.6 and `tf-nightly`. May be this is a potential bug.', b'Thanks for reporting the issue. The implementation used in each case is different, hence why we see different results.\r\n\r\nThe double call on the first time step occurs at the level of `keras.backend.rnn`, which is the implementation used in eager mode. Specifically here: https://github.com/keras-team/keras/blob/master/keras/backend.py#L4507-L4510\r\n\r\nWe call the step function a first time to infer the shape/dtype of the output. This assumes that the step_function does not have any side effect, but in practice that isn\'t the case when `add_loss` is used.\r\n\r\nThere isn\'t really a clean way to fix it. We\'d need for `backend.rnn` to set some form of global state (a backend flag) that says ""ignore add_loss and add_metric"", then the base `Layer` would check for this flag in `add_loss` / `add_metric` and do nothing if it is set. Would you like to open a PR to fix the bug?\r\n\r\nIf not we\'ll look into fixing this. In the meantime, prefer not using `add_loss` with `TimeDistributed` (you can implement your own version of distribution based on reshaping instead).']",open,2021-09-08 19:49:39,
https://api.github.com/repos/keras-team/keras/issues/15329,b'Convert Functional API to Model Subclassing with Normalization Layers',[],open,2021-09-08 06:56:26,
https://api.github.com/repos/keras-team/keras/issues/15323,b'tf.keras computes incorrect loss values with Masking',[b'The issue exists even in tensorflow version 2.6. More detailed comment https://github.com/tensorflow/tensorflow/issues/34491#issuecomment-915134312'],open,2021-09-07 17:15:35,
https://api.github.com/repos/keras-team/keras/issues/15314,b'Problem for passing integer seed value for tensorflow.keras.layers.experimental.preprocessing functions in .map function',"[b'@Kaveh8 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!', b'@sushreebarsa  Thanks for your response. [Here](https://colab.research.google.com/drive/12h9BO2ymiF0OSTSR60eEzIHdewch3IgP?usp=sharing) I have provided a notebook in colab, to reproduce the issue. I get another error message in colab, anyway the error still exist. ', b'@Kaveh8 cc: @sushreebarsa \r\nThese are the layers: RandomFlip, RandomRotation, etc. So, I think you need to call them in such ways. Something like \r\n\r\n```python\r\nclass auglayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.seed = int(tf.random.uniform([], dtype=tf.int32, minval=0, maxval=100))\r\n        self.random_flip = pre.RandomFlip(""horizontal"", seed=self.seed)\r\n        self.random_rotate = pre.RandomRotation(factor=0.01, seed=self.seed)\r\n\r\n    def call(self, inputs):\r\n        x = self.random_flip(inputs)\r\n        x = self.random_rotate(x)\r\n        return x \r\n\r\naugs = tf.keras.Sequential([auglayer()])\r\n```\r\n```python\r\ntrain_ds_rand = (\r\n    tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\n    .batch(100)\r\n    .map(lambda x, y: (augs(x), y),num_parallel_calls=AUTO)\r\n    .prefetch(AUTO)\r\n    )\r\n```\r\n\r\n\r\n\r\n', b'@innat Thanks for your response. I finally did it like what you have done In Kaggle. But I think it should be fine to use them in a function (not a class), like other layers in Keras. I wonder why I get errors in a map function without subclassing (I know it will be run in a graph mode, and the error may be related for graph mode) but no error when implemented as layer subclass.', b""@Kaveh8 \r\nI think a functional approach can also be done. However, in your gist above, there is some issue, for example, initializing the layers and calling them stuff. Instead of what you've done on gist, you should call them as follows \r\n\r\n```python\r\ndef apply_augment(x):\r\n    x = keras_aug.RandomFlip(..., seed=set_seed)(x)\r\n    x = keras_aug.RandomRotation(..., seed=set_seed)(x)\r\n    x = keras_aug.RandomTranslation(..., seed=set_seed)(x)\r\n    x = keras_aug.RandomContrast(..., seed=set_seed)(x)\r\n    x = keras_aug.RandomRotation(..., seed=set_seed)(x)\r\n    return x \r\n```\r\n\r\n\r\n\r\nI've posted a possible solution [here](https://www.kaggle.com/ipythonx/tf-3d-2d-model-for-brain-tumor-classification/comments#1503883), hope it helps. \r\n"", b'@innat Thanks for your solution. In this way, the key to resolve the error is wrapping function with `tf.numpy_function`. Then I can set seed like what you said. But using `tf.numpy_function` or `py_function` is not allowed with TPUs. As you may see, I have used TPUs in my code. I am still curious to know is it possible to run this function (preprocessing layers) on `.map` and with TPU? Are they compatible with TPUs? ', b""Not sure exactly, it's known [issue](https://github.com/tensorflow/tensorflow/issues/38762#issuecomment-688247286) currently. "", b""@innat Yes. I know there is an issue for `py_function` with TPU. So, I considered to not use them in my code. I haven't used any raw python or numpy functions in my code, so I don't need to wrap my function with `py_function` or `numpy_function`. Now, I think `preprocessing` layers are using this underneath (like `if seed:`), and so they are not available on TPUs. But, how is it possible when using subclassing approach, and not possible when using Functional approach? I'm really get confused. "", b'@Kaveh8 cc @sushreebarsa \r\nIt should be available on all devices either CPU/GPU/TPU. TPU has some issue with `numpy_function`. It also has some issues with the `keras` generator or `tf. data.from_generator`. If we try to use a custom layer, it should work on TPU. The problem is with `tf. Data` and its strict graph execution. Even with `.run_functions_eagerly(True)` doesn\'t work here. \r\n\r\nThe following approach may work on all devices even with TPU and Not within the `tf. data` API. \r\n\r\n```python\r\nclass Keras3DAugmentation(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(KerasAug, self).__init__()\r\n        self.rng = tf.random.Generator.from_seed(global_seed)\r\n        print(\'init seed \', self.rng)\r\n\r\n    def apply_augment(self, sample):\r\n        set_seed = self.rng.make_seeds()[0]\r\n        print(\'dynamic seed \', set_seed)\r\n        x = keras_aug.RandomFlip(, seed=set_seed)(sample)\r\n        x = keras_aug.RandomRotation(, seed=set_seed)(x)\r\n        x = keras_aug.RandomTranslation(, seed=set_seed)(x)\r\n        x = keras_aug.RandomContrast(, seed=set_seed)(x)\r\n        return x \r\n        \r\n    def call(self, inputs):\r\n        splitted_modalities = tf.split(tf.cast(inputs, tf.float32), \r\n                  input_channel, axis=-1)\r\n        splitted_modalities = [tf.squeeze(i, axis=-1) \\\r\n                  for i in splitted_modalities] \r\n   \r\n        # Will contain the augmented outputs\r\n        flair = []\r\n        t1w = []\r\n        t1wce = []\r\n        t2w = []\r\n\r\n        flair = tf.expand_dims(self.apply_augment(splitted_modalities[0]), axis=-1)\r\n        t1w   = tf.expand_dims(self.apply_augment(splitted_modalities[1]), axis=-1)\r\n        t1wce = tf.expand_dims(self.apply_augment(splitted_modalities[2]), axis=-1)\r\n        t2w   = tf.expand_dims(self.apply_augment(splitted_modalities[3]), axis=-1)\r\n\r\n        image = tf.stack([flair, t1w, t1wce, t2w], axis=-1)\r\n        image = tf.reshape(image, [-1, input_height, input_width, \r\n                                   input_depth, input_channel])\r\n        return image\r\n```\r\n\r\n```python\r\n keras_image_augmentation = tf.keras.Sequential(\r\n        [\r\n           Keras3DAugmentation()\r\n        ],\r\n        name=\'keras_augment_layers\'\r\n    )\r\n\r\nsamples = tf.ones((2, input_height, input_width,  input_depth, input_channel))\r\nkeras_image_augmentation(samples).shape\r\n\r\n(2, 256, 256, 24)\r\ndynamic seed  Tensor(""keras_aug_4/strided_slice:0"", shape=(1,), dtype=int64)\r\n(2, 256, 256, 24)\r\ndynamic seed  tf.Tensor([-5927004847686741621], shape=(1,), dtype=int64)\r\ndynamic seed  tf.Tensor([4629789014631912635], shape=(1,), dtype=int64)\r\ndynamic seed  tf.Tensor([-344729292210807444], shape=(1,), dtype=int64)\r\ndynamic seed  tf.Tensor([-3758752224390587847], shape=(1,), dtype=int64)\r\nTensorShape([2, 256, 256, 24, 4])\r\n```\r\n\r\nThough I didn\'t test end-to-end. But I think it\'s possible to use this layer inside the model definition class and not in the data pipeline and also may need to run the code eagerly.\r\n\r\n```python\r\n\r\nmodel = tf.keras.Sequential([\r\n       Keras3DAugmentation(),\r\n       OTHER_3D_LAYERS,\r\n       Last_Layer\r\n   ]\r\n)\r\n```\r\n\r\nBut it\'s not good or best practice. \r\n', b'@innat  It is about `seed` argument of the `preprocessing` layers. When implementing layer subclassing, setting `seed` is done outside of the `tf.data` API (`seed` is set within `__init__` when creating object from class), and in `tf.data` only the `call` function is executed, without any problem. But when we implement it as a function, passing `seed` also, will be executed within `tf.data`, and it leads to an error. I want to know, is it possible to pass `seed` argument within `tf.data` API or not? And is it a bug or a reasonable behavior? Thanks.', b""@Kaveh8 not sure if it's a bug or else. I've raised a related [issue](https://github.com/keras-team/keras/issues/15358). "", b'@Kaveh8, the seed value for preprocessing layer is expected to be a integer, and not a tensor. If you would like to have some deterministic behavior, you can refer to https://github.com/keras-team/keras/issues/15358 as well.\r\n\r\nAssign to @mattdangerw who works on KPL.']",open,2021-09-04 13:16:05,
https://api.github.com/repos/keras-team/keras/issues/15308,b'Add the source code Link of the corresponding API!',"[b""@innat \r\n\r\nI think this is intentional. Keras doesn't want you to worry about the implementational details because it has such a nice abstraction of things. Thus, it makes sense to keep Keras docs for quick reference and TF docs to be more contributor oriented.\r\n\r\nAlso, TF docs contain nightly APIs whereas Keras docs contain only stable release APIs. Thus, it is assumed that there will be no need to tweak the internal code."", b""@AdityaKane2001 \r\n\r\n> Thus, it makes sense to keep Keras docs for quick reference and TF docs to be more contributor oriented.\r\n\r\nThanks for your comment. I understand your point. And it might be true for `tf < 2`, right before when `keras` didn't tightly integrate into `tf`. But now, `keras` has the nature of **progressively disclosure of complexity**, for example, now we can do the custom training loop in `keras` etc. Also, now `keras` moved back to its original repo (as said) to keep the `keras` development more concrete and focus; [keras.io](https://keras.io/) traffic is too high these days. So, just considering it for quick reference can be a wrong said. \r\n\r\n> Also, TF docs contain nightly APIs whereas Keras docs contain only stable release APIs. Thus, it is assumed that there will be no need to tweak the internal code.\r\n\r\nIf we assume that, `keras` gives stable release and for that, we don't need to tweak the internal code - **IMHO**, that's a bad argument. \r\n\r\n---\r\n\r\n\r\nIn end, I would say that having a source link would not break anything but surely add some value. \r\n""]",open,2021-09-03 12:19:54,
https://api.github.com/repos/keras-team/keras/issues/15298,b'Bug with optimizer_v2.OptimizerV2.set_weights method ',[],open,2021-09-02 13:04:19,
https://api.github.com/repos/keras-team/keras/issues/15290,b'Inconsistency In CategoricalCrossentropy Calculation',"[b'@jvishnuvardhan Was able to reproduce the issue in TF v2.6 , TF v2.5 , TF-nightly please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/ea5f9ab14a440ce6da92d6e7b664bff6/51720.ipynb#scrollTo=jg0RUpoEJwHX).Thanks!']",open,2021-09-01 11:43:57,
https://api.github.com/repos/keras-team/keras/issues/15282,"b""MobileNetV3 models can't infer the static shape""",[],open,2021-08-31 07:57:43,
https://api.github.com/repos/keras-team/keras/issues/15279,b'Implement compiled MWMS with XLA on GPU.',[],open,2021-08-30 22:22:14,
https://api.github.com/repos/keras-team/keras/issues/15273,b'layers.TimeDistributed does not change number of weights or network architecture',"[b'@hdavis472 Could you please refer to [**`link`** ](https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters) and let us know if it helps. Thanks!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""I have read the stack overflow post you linked and tried running my own experiments based off the answer, but I still don't understand what using TimeDistributed dense does. I believe the original poster of the stack overflow thread was also still confused even after having read the answer. Would it be possible for someone to explain it clearly here please?""]",open,2021-08-29 20:37:08,
https://api.github.com/repos/keras-team/keras/issues/15270,"b'Add `ResNeXt [50, 101]` to keras.applications'","[b'Thanks for opening the issue. Can you provide a paragraph as a proposal for justifying adding this model? If you can provide some context of how widely this is used or some citation, that would be helpful.']",open,2021-08-28 17:14:56,
https://api.github.com/repos/keras-team/keras/issues/15269,"b'Add `ResNet [18, 34] ` to keras.applications'","[b'Can I work on this?', b'@quantumalaviya of course. Just send a PR. It should be welcomed. ', b'So, I just wrote the code but I notice some differences in the number of parameters when compared to https://github.com/qubvel/classification_models. Any suggestions on how to add weights?', b'Thanks for opening the issue. Can you provide a paragraph as a proposal for justifying adding this model? If you can provide some context of how widely this is used or some citation, that would be helpful.', b'@rchao \r\nFYI, These have been asked before. [#151](https://github.com/keras-team/keras-applications/issues/151)\r\n\r\n@quantumalaviya could you please check out [this implementation](https://github.com/keras-team/keras-contrib/tree/master/keras_contrib/applications), it already exists in `keras-contrib` but is not included in `keras`. ', b'https://github.com/breadbread1984/resnet18-34 . you may try this implement.']",open,2021-08-28 17:14:54,
https://api.github.com/repos/keras-team/keras/issues/15268,b'Add `SE-ResNet` to keras.applications',"[b'Thanks for opening the issue. Can you provide a paragraph as a proposal for justifying adding this model? If you can provide some context of how widely this is used or some citation, that would be helpful.']",open,2021-08-28 17:14:52,
https://api.github.com/repos/keras-team/keras/issues/15267,b'Add `SE-Net` and `SE-ResNeXt ` to keras.applications',"[b'Thanks for opening the issue. Can you provide a paragraph as a proposal for justifying adding this model? If you can provide some context of how widely this is used or some citation, that would be helpful.', b""@rchao Thanks for asking. \r\nReplying to all your requests, [#15267](https://github.com/keras-team/keras/issues/15267), [#15268](https://github.com/keras-team/keras/issues/15268), [#15269](https://github.com/keras-team/keras/issues/15269), [#15270](https://github.com/keras-team/keras/issues/15270)\r\n\r\nThere is nothing new additional argument for justification I could use compared to the arguments that apply for available models like `inceptionv3`, `resent` `densenet` etc. I'm not fully sure why you're seeking a proposal as you mentioned, so please let me know why these models are not worthy to add and need proposals (I'm seriously don't know if there is any known issue. Is it for lack of man-power or something else! If `keras` follows some policy here to add new models). \r\n\r\nHowever, I can give some overview of how these models are widely used. For example, let's say in kaggle competition, these **SENet** models are used frequently used as part of the experiments, [link](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/210016), amazing [notebooks](https://www.kaggle.com/debarshichanda/seresnext50-but-with-attention/notebook), [link](https://www.kaggle.com/ammarali32/seresnet152d-inference-single-model-lb-96-2), [link](https://www.kaggle.com/yasufuminakama/ranzcr-resnext50-32x4d-starter-training). I can provide more where these models are used actively. Along with these models, [RegNets](https://github.com/keras-team/keras/issues/15240), ResNeSt, [NFNet](https://github.com/keras-team/keras/issues/15229) is also worthy to add. All of these models are widely used in the Computer Vision competitions in Kaggle.\r\n\r\nThe point is, those models which are not available in `keras.applications`, a user needs to adopt other sources, and sometimes it's not always feasible to use those implementations and needs to support along with the new `tf. keras` version. \r\n\r\n[ It looks some of the requested models were added but dunno what happened next, [ResNeXt #85](https://github.com/keras-team/keras-applications/pull/85), [ResNeXt #26](https://github.com/keras-team/keras-applications/pull/26) ]"", b'@rchao \r\nCould you please give some feedback on this? ', b""@innat thanks for the information, and sorry for the delayed response here. The general criteria for adding a new application is that the model has 50 or more citations on the paper, and the requester can provide a PR for us to review. In this case, the PR should include an ImageNet checkpoint. \r\n\r\nCan you show us the citation count or other reference demonstrating the popularity, and we'll assess to let you know whether we should proceed?"", b""@rchao Thanks for the response.\r\n \r\nFor citation, I think it's not required to state the citation count for Issue [#15269](https://github.com/keras-team/keras/issues/15269) as we have already `ResNet 50/101`. However,\r\n\r\n- For [`SENet`](https://github.com/hujie-frank/SENet?utm_source=catalyzex.com#trained-models) based models such as `SENet`, `SE-ResNet`, `SE-ResNeXt`, could you please check the citation count here, [source](https://ieeexplore.ieee.org/document/8578843/citations?tabFilter=papers), I think it's about ~1900 citation.\r\n\r\n- For the `ResNeXt` model, could you please check here, [source](https://ieeexplore.ieee.org/document/8100117), I think it's about ~1000 citations?\r\n\r\nIs this something you asked? Please let me know. ( And just to inform, **I've created different issue regarding these models so that if PRs are welcomed, the contributor can work on specifically.** )"", b'Thanks @innat. Does this model have an imagenet checkpoint as well?', b'@rchao yes, (afaik) they do. [Here](https://github.com/qubvel/classification_models) is the third-party implementation, and here is the list of [imagenet weights](https://github.com/qubvel/classification_models/releases/tag/0.0.1).']",open,2021-08-28 17:14:49,
https://api.github.com/repos/keras-team/keras/issues/15266,b'TF RNN subclass model called twice although sequence length is one',"[b'@ymodak  Was able to reproduce the issue in TF v2.6 , TF v2.5 and TF-nightly please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/59cde232d61c55f90c8c7da00399ebce/51376.ipynb#scrollTo=8vhj22mqcbFU).Thanks!']",open,2021-08-28 17:07:39,
https://api.github.com/repos/keras-team/keras/issues/15261,b'[tf2.6] Loaded subclassed-Model fails to be retrained',"[b'Ok, I tried it out myself and here is what I found and hence the possible workaround.\r\n**BOTH** `get_config` and `from_config` **MUST** be explicitly defined. \r\n\r\nIf only `get_config` is defined will end up getting:\r\n```\r\nTraceback (most recent call last):\r\n  File ""test2.py"", line 159, in <module>\r\n    loaded_model = keras.models.load_model(""model_saved"")\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/save.py"", line 205, in load_model\r\n    return saved_model_load.load(filepath, compile, options)\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/load.py"", line 134, in load\r\n    keras_loader.load_layers(compile=compile)\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/load.py"", line 394, in load_layers\r\n    self.loaded_nodes[node_metadata.node_id] = self._load_layer(\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/load.py"", line 436, in _load_layer\r\n    obj, setter = self._revive_from_config(identifier, metadata, node_id)\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/load.py"", line 454, in _revive_from_config\r\n    self._revive_layer_or_model_from_config(metadata, node_id))\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/saving/saved_model/load.py"", line 518, in _revive_layer_or_model_from_config\r\n    obj = layers_module.deserialize(\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/layers/serialization.py"", line 208, in deserialize\r\n    return generic_utils.deserialize_keras_object(\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/utils/generic_utils.py"", line 674, in deserialize_keras_object\r\n    deserialized_obj = cls.from_config(\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/engine/training.py"", line 2397, in from_config\r\n    functional.reconstruct_from_config(config, custom_objects))\r\n  File ""/usr/local/lib/python3.8/site-packages/keras/engine/functional.py"", line 1272, in reconstruct_from_config\r\n    for layer_data in config[\'layers\']:\r\nKeyError: \'layers\'\r\n```\r\n\r\nIf only `from_config` is defined will end up getting the same error of:\r\n```\r\n    ValueError: No gradients provided for any variable: [\'abs_output:0\', \'non_distributional_model/dense_1/kernel:0\', \'non_distributional_model/dense_1/bias:0\', \'non_distributional_model/dense_2/kernel:0\', \'non_distributional_model/dense_2/bias:0\', \'non_distributional_model/output_layer/kernel:0\', \'non_distributional_model/output_layer/bias:0\'].\r\n```\r\n\r\nIt works with both explicitly defined.\r\n\r\nBut here comes my confusion. In what case `from_config` must be explicitly defined, which differs from what the document told us that it could be done optionally?', b'@jvishnuvardhan Was able to reproduce the issue in TF v2.6 and  TF v2.5 . please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/477e8bc996a3e9e77dd33b0342138466/51241.ipynb#scrollTo=efebmgClichs).Thanks!', b""> Ok, I tried it out myself and here is what I found and hence the possible workaround.\r\nBOTH get_config and from_config MUST be explicitly defined.\r\n\r\nThank you! I was seeing `KeyError: 'layers'` and couldn't figure out where it was coming from until I found this. My problem was indeed from not defining my own from_config function in a custom Model.\r\n\r\n> But here comes my confusion. In what case from_config must be explicitly defined, which differs from what the document told us that it could be done optionally?\r\n\r\nI believe the issue is with the default implementation of `from_config`:\r\nhttps://github.com/keras-team/keras/blob/v2.6.0/keras/engine/training.py#L2389\r\n\r\n```\r\ndef from_config(cls, config, custom_objects=None):\r\n  # `from_config` assumes `cls` is either `Functional` or a child class of\r\n  # `Functional`. In the case that `cls` is meant to behave like a child class\r\n  # of `Functional` but only inherits from the `Model` class, we have to call\r\n  # `cls(...)` instead of `Functional.from_config`.\r\n  ...\r\n```\r\n\r\nIf I'm reading this correctly the default from_config for a Model subclass assumes the model is a Functional model and makes no attempt to handle other Model subclasses. This is why you need to define a from_config function which explicitly handles your config.\r\n\r\nIMO this is one of several unintuitive, subtle differences between custom Layers and Models that the documentation does a poor job explaining.""]",open,2021-08-27 12:42:11,
https://api.github.com/repos/keras-team/keras/issues/15256,b'Fix alpha-related bug in mobilenet.py',[],open,2021-08-26 18:49:37,
https://api.github.com/repos/keras-team/keras/issues/15255,b'Tokenizer converts padding integers to OOV when oov_token is not None',"[b'@ymodak  Was able to reproduce the issue in TF v2.6 , TF v2.5 , TF-nightly please find the gist [**`here`**](https://colab.research.google.com/gist/kumariko/4dde948009a449a7b0ace9011ca14874/51376.ipynb#scrollTo=_MJn1e6qlg1B).Thanks!']",open,2021-08-26 16:45:51,
https://api.github.com/repos/keras-team/keras/issues/15253,b'Implementing cauchy-schwarz divergence and negative log likelihood as custom loss functions in Keras',"[b'Could you share a colab notebook with us with demo inputs? That would help us use the functions and also debug. TIA!', b'> Could you share a colab notebook with us with demo inputs? That would help us use the functions and also debug. TIA!\r\n\r\n[colab_custom_loss.zip](https://github.com/keras-team/keras/files/7061536/colab_custom_loss.zip)\r\n\r\n@ariG23498 PFA the colab notebook per your suggestions. Looking forward to your response. Many thanks!', b'Hey @sivaramakrishnan-rajaraman \r\nThanks for the colab, unfortunately I could not load the data and was not able to run the code. I saw that you were using scipy and numpy code in the loss functions.\r\nCould [tf.function](https://www.tensorflow.org/api_docs/python/tf/numpy_function) be a solution to the issue?', b'@ariG23498 Not sure how to use it, however. It would be great if you can help with the complete code. For a sample dataset, you can download from https://www.tensorflow.org/datasets/catalog/malaria and split it across train and test sets. ']",open,2021-08-25 21:40:13,
https://api.github.com/repos/keras-team/keras/issues/15252,b'Remove all the tests under tensorflow/python/keras/distribute.',[],open,2021-08-25 21:03:48,
https://api.github.com/repos/keras-team/keras/issues/15249,b'A Suspected Bug in binary_crossentropy',"[b'We are working on a fix to this, thanks for reporting the issue!', b'Hi there, is it there any update on this one?\r\n']",open,2021-08-25 15:18:43,
https://api.github.com/repos/keras-team/keras/issues/15248,"b'Test cases failed for training_test, culprit is TestFunctionTracing class'","[b""Thanks for reporting. We'll take a further look into this.""]",open,2021-08-25 13:52:36,
https://api.github.com/repos/keras-team/keras/issues/15247,b'Inconsistent behavior of tf.keras.losses.binary_crossentropy',"[b'Not sure if this is a Keras or TF issue.', b'Hey, I would like to help you with this issue? Could you make me an assignee? ', b'@eli-osherovich does tf.nn.sigmoid_cross_entropy_without_logits supports broadcasting?\r\n', b'@Vidit-Ostwal  no.', b'Any ideas from keras-team?', b'I think it should be fixed in `tf.nn.sigmoid_cross_entropy_with_logits`, hence opened an issue in tensorflow. ']",open,2021-08-25 11:29:10,
https://api.github.com/repos/keras-team/keras/issues/15246,b'BaseLogger callback seems broke in v2 - KeyError and wrong seen sample compute',"[b""I am able to reproduce the error. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/606043f7066fa56ca7a7f4b62af361db/untitled.ipynb).\r\n\r\nThe following is the error trace \r\n\r\n```\r\nEpoch 1/10\r\n1/1 [==============================] - 0s 377ms/step - loss: 7.7125\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-4-a9e266b0dcc1> in <module>()\r\n      8 x = np.random.rand(4,5)\r\n      9 y = np.random.randint(0, 2, (4,))\r\n---> 10 model.fit(x, y, epochs=10, callbacks=[tf.keras.callbacks.BaseLogger()])\r\n\r\n1 frames\r\n/usr/local/lib/python3.7/dist-packages/keras/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    920   def on_epoch_end(self, epoch, logs=None):\r\n    921     if logs is not None:\r\n--> 922       for k in self.params['metrics']:\r\n    923         if k in self.totals:\r\n    924           # Make value available to next callbacks.\r\n\r\nKeyError: 'metrics') \r\n```""]",open,2021-08-25 10:54:50,
https://api.github.com/repos/keras-team/keras/issues/15245,b'[tf2.6] Model saving error for a customized model and loss function',"[b""Hey could you make me an assignee? I'd like to help with your issue."", b""Hey OniReimu, currently the compiled loss isn't being tracked by the model, leading to this error. (Untracked means that we do not save these objects to SavedModel or the Checkpoint -- when you call `self.criterion = ...`, this essentially forces the model to track the loss object)\r\n\r\nGenerally, compiled parameters are not saved with the model (other than the optimizer), so you have two options here as we take a closer look into whether we *should* save these parameters to the model:\r\n\r\n1. Go with the `self.criterion` workaround \r\n2. Remove the tf.function decorator from `train_step` so it does not get saved to the SavedModel."", b'Hi k-w-w, thanks for your response. I will then probably go for the first workaround as a graph execution is normally the preference.']",open,2021-08-25 10:18:58,
https://api.github.com/repos/keras-team/keras/issues/15239,b'3 Possible Essential Features for `model.summary()`',"[b'I am implementing the expand_nested in the model.summary(). I will make a PR when I will be done!', b'I can do feature requests #1 and #2. I will make a PR when I finish.']",open,2021-08-24 15:09:44,
https://api.github.com/repos/keras-team/keras/issues/15236,b'Check for invalid attribute names to the Model class.',[],open,2021-08-24 09:16:11,
https://api.github.com/repos/keras-team/keras/issues/15229,b'Add NFNets to keras.applications',"[b'Any ETAs here? ', b""Hi there,\r\n\r\nYes, we'd welcome such a contribution. However, we have no availability to work on it, so you'd have to open a PR yourself (we will review the PR and guide you). Do you have an ImageNet checkpoint available for the model?"", b""Got it, i'll see if i can make a PR on this. i have some components built in the past but just need to clean it up a bit\r\n"", b'@fchollet would you mind tagging **Contributions welcome** label on this? ']",open,2021-08-22 23:08:43,
https://api.github.com/repos/keras-team/keras/issues/15225,b'InputSpec argument ignored',"[b'@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf v2.5,v2.6 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/2fed28117cf080e7e3ba4037c3cf7104/15225.ipynb).']",open,2021-08-22 17:49:10,
https://api.github.com/repos/keras-team/keras/issues/15221,b'Is there a way in Keras to apply class_weight functionality if I modify model.fit() method ?',"[b'Hi,\r\nAny thought about this problem?', b'Hello @n33lkanth , as shown here every class that subclasses from keras.Model class will get the .fit() method in which you can pass the class_weight parameter like you do in the original .fit() method. The only difference will be I think that you have to utilize these class weights in the custom .fit() method regarding your requirements. Let me know if this helps.', b'Hi @MrinalTyagi, Yes you are right that I can pass the class_weight in model.compile() method but when I am trying to pass class weights for all four outputs I am getting error that class_weight is not supported for the multi-output model.', b'@n33lkanth Try having a look at the following. [Applying class weights to a multi-output model](https://datascience.stackexchange.com/questions/41698/how-to-apply-class-weight-to-a-multi-output-model). I think u are currently encountering the same issue described here. Let me know if this helps.', b'Hi @MrinalTyagi,\r\nYes, you are right. However, I am getting the following error:\r\n\r\n`ValueError: `class_weight` is only supported for Models with a single output`', b'@n33lkanth could you please share a colab file of the code for me to have a look?\r\n']",open,2021-08-21 22:27:42,
https://api.github.com/repos/keras-team/keras/issues/15212,b'Need to convert Lasagne to Keras code (CNNLSTM)',[b'@tilakrayal  would you please help'],open,2021-08-20 14:22:02,
https://api.github.com/repos/keras-team/keras/issues/15210,b'Tutorial model consistently contains NaN',"[b'possibly related to https://github.com/keras-team/keras/issues/11927', b'Thanks for reporting this issue! We will talk to the author for further check.']",open,2021-08-20 05:28:41,
https://api.github.com/repos/keras-team/keras/issues/15206,"b'""classes"" not working on flow_from_dataframe '","[b'I am a first time contributer . Can I work on this issue? ', b'Also it will be helpful if you could tell wht is the expected output .', b'@jvishnuvardhan ,\r\nI was able to reproduce the issue tf v2.5, v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/89c05883c44906cdf514cdefea6ce074/untitled59.ipynb).', b""This looks valid, and like it might be a small fix to https://github.com/keras-team/keras-preprocessing. If someone would like to contribute a small and tested fix there, we can merge. We'd like to avoid any large refactors here, as the `ImageDataGenerator` flows are mainly for compatibility at this point, with the focus on image augmentation through preprocessing layers instead."", b'This is fixed in master [branch code base](https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/dataframe_iterator.py#L255-L265) but the fix is not part of Keras-Preprocessing 1.1.2']",open,2021-08-19 20:08:44,
https://api.github.com/repos/keras-team/keras/issues/15196,b'Dynamically add Trackable methods to the LossScaleOptimizer.',[],open,2021-08-19 02:13:05,
https://api.github.com/repos/keras-team/keras/issues/15194,b'Reshape layer drops mask from previous layers',"[b'issue reproducible at [colab](https://colab.research.google.com/drive/1uN0QrjiMXA7N6N6jdFQVVO1He8NTrZZ-?usp=sharing)\r\n\r\n', b'@brian36 ,\r\nCan you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/42403) with similar error.It helps.Thanks', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b""> @brian36 ,\r\n> Can you please take a look at this [issue](https://github.com/tensorflow/tensorflow/issues/42403) with similar error.It helps.Thanks\r\n\r\nThis other issue is unrelated.\r\n\r\nThe issue here is, there is no keras mask when there should be one, after passing a tensor with a mask through reshape.\r\n\r\nThis is not a support issue or question about implementation, but in my opinion a bug with the Reshape layer implementation.  At the very least it should give the option of keeping the underlying mask when passing through the Reshape layer, and it definitely should warn the user it's dropping the underlying mask / mention this in the documentation if nothing else.  Instead it does neither.\r\n\r\n"", b'@jvishnuvardhan I Was able to reproduce the issue in TF **[v2.6](https://colab.research.google.com/gist/kumariko/c5dd4b87a86cec75578028f1c202e2d0/15194.ipynb#scrollTo=8TpXmySqd-YQ)** and **[TF-nightly](https://colab.research.google.com/gist/kumariko/1ea7e3a7a39fb5e1334a75b5fdf41221/15194.ipynb#scrollTo=aJJtjFqWI9cf)** please find the gist .Thanks!', b""We don't believe the reshape layer supports masking at this time, though it could be added. We are happy to accept contributions that add it as a feature\r\n\r\n(Separately, we should consider raising an error when passing a tensor with a mask to a layer that does not support masking, rather than silently dropping the mask)""]",open,2021-08-18 22:28:37,
https://api.github.com/repos/keras-team/keras/issues/15192,b'Namespace for custom objects not included in serialised config',[],open,2021-08-18 17:29:33,
https://api.github.com/repos/keras-team/keras/issues/15191,b'Performance issues in the program',"[b'@DLPerf  Please check the following paragraph that explains `num_parallel_calls` depend on few parameters and depending on your application and data, you can choose `num_parallel_calls`. You could also use `tf.data.AUTOTUNE` which automatically decide how many parallel threads can be run at a given time. \r\n\r\n> Choosing the best value for the num_parallel_calls argument depends on your hardware, characteristics of your training data (such as its size and shape), the cost of your map function, and what other processing is happening on the CPU at the same time. A simple heuristic is to use the number of available CPU cores. However, as for the prefetch and interleave transformation, the map transformation supports `tf.data.AUTOTUNE` which will delegate the decision about what level of parallelism to use to the tf.data runtime.\r\n\r\nThe above paragraph is from TF webpage https://www.tensorflow.org/guide/data_performance#parallelizing_data_transformation\r\n\r\nPlease let us know what you think. If you want to update some docs, please feel free to raise a PR. Thanks!', b'Thanks for your reply @jvishnuvardhan.\r\n According to the doc you cited,  we should call `map` with num_parallel_calls=tf.data.AUTOTUNE [here](https://github.com/keras-team/keras/blob/05048b7c0d56c4c3c90460c31b5fa2406a0ea1e2/keras/layers/preprocessing/benchmarks/image_preproc_benchmark.py#L89)?\r\nPlease correct me if I miss something.', b'@DLPerf Correct. If you want TF to control `num_parallel_calls `, then you need to call with `num_parallel_calls=tf.data.AUTOTUNE`. \r\n\r\nPlease close the issue if this was resolved for you. Thanks!', b'@jvishnuvardhan Sorrry, maybe I explained not clearly. I mean that the code in keras/keras/layers/preprocessing/benchmarks/image_preproc_benchmark.py [line 89] (https://github.com/keras-team/keras/blob/05048b7c0d56c4c3c90460c31b5fa2406a0ea1e2/keras/layers/preprocessing/benchmarks/image_preproc_benchmark.py#L89) should be fixed with `num_parallel_calls=tf.data.AUTOTUNE` ? Otherwise it will run `map` sequentially even in multi-cores machine, which will take more time.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Hello,I mean if you need a free PR?\r\nI can help you.', b'Thanks for reporting the issue, and yes, your contribution is  welcome and appreciated! ']",open,2021-08-18 14:56:05,
https://api.github.com/repos/keras-team/keras/issues/15183,b'ValueError when accessing submodules of compiled subclassed tf.keras model',"[b'@jvishnuvardhan ,\r\nI was able to reproduce the issue in tf v2.4,v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/21e79f78bfed98c0cdf813590435cd19/untitled57.ipynb).', b'I think this may be intended behavior.\r\n\r\ntriage notes: \r\nSubmodules of a subclassed model can be accessed before compilation but not after `model.compile`', b'@jvishnuvardhan Thank you, but how could this be the intended behavior?  Is the statement that `model.compile` should affect access to submodules documented anywhere?  If `model.compile` is supposed to revoke access to submodules, could we raise a more informative exception?  Thank you.']",open,2021-08-16 20:25:25,
https://api.github.com/repos/keras-team/keras/issues/15181,b'temporal 2D sample_weight in fit() causing dimension mismatch',[],open,2021-08-16 12:23:27,
https://api.github.com/repos/keras-team/keras/issues/15179,b'Concatenate CNN (trained using Keras flow_from_directory and generators) and LSTM (time-series data)',"[b'Hello,\r\n\r\nI am trying to develop a model using multimodal data. My one set of data is images that is trained using Keras flow_from_directory and generators) and another set of data is time-series that is trained using LSTM and CONV1D. How to concatenate the two models to train them as a single model (model with two inputs and single output)? Any example or guidance will assist in its practical implementation.\r\n\r\nThanks and Regards', b'Hey I tried to merge two models I have merge LSTM and CNN(in which VGG16 has been used) and it works\r\n  join = Concatenate([lstm_model, cnn_model])\r\n  hidden = Dense(1, activation = \'sigmoid\')\r\n  new_model = Sequential()\r\n  new_model.add(join)\r\n  new_model.add(hidden)\r\n  new_model.compile(optimizer="""",loss="""",metrices=[""""])\r\n\r\n  history = new_model .fit(X_train, y_train, epochs=50, batch_size=64) \r\n  import matplotlib.pyplot as plt\r\n  plt.plot(history.history[\'accuracy\'],label=""train_accuracy"")\r\n  plt.plot(history.history[\'val_accuracy\'],label=""test_accuracy"")\r\n  plt.legend()\r\n\r\nOr their is another method you can use any one of the model as base model  just like VGG16 is used for CNN\r\nbase_model =  VGG16(input_shape = (224, 224, 3), include_top = False, weights = \'imagenet\')\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\nx = Flatten()(base_model.output)\r\nx = Dense(1024, activation=\'relu\')(x)\r\nx = Dropout(0.5)(x)\r\nx = Dense(3, activation=\'softmax\')(x)\r\nmodel = Model(base_model.input, x)\r\nmodel.compile(optimizer = ""adam"", loss = \'categorical_crossentropy\',metrics = [""accuracy""])', b'We try to concatenate things up when they are of same property or shape. \r\nSo, if we try to concatenate `flow_from_directry()` to `LSTM` and `Conv1D` it would not make sense as one is for data augmentation and other is not .\r\nSo, basically we kind of try to do this thing of concatenation for monitoring some gestures or movements over video which basically involve temporal and spatial domains here temporal feature extraction is done by ` LSTM` , `Conv1D` and spatial features extraction is done by `Conv2D` .\r\nSo, the solution is do the data augmentation before training and store images somewhere , now for every image you would be having time series data , but you have to generate for augmented images .\r\nAfter that make a parallel model as it is after all the calculation just concatenate the features together which could be done easily .\r\nand then pass it to dense to predict .\r\nI can do a pull request which can make it more simpler for you to understand!', b'> Hey I tried to merge two models I have merge LSTM and CNN(in which VGG16 has been used) and it works\r\n> join = Concatenate([lstm_model, cnn_model])\r\n> hidden = Dense(1, activation = \'sigmoid\')\r\n> new_model = Sequential()\r\n> new_model.add(join)\r\n> new_model.add(hidden)\r\n> new_model.compile(optimizer="""",loss="""",metrices=[""""])\r\n> \r\n> history = new_model .fit(X_train, y_train, epochs=50, batch_size=64)\r\n> import matplotlib.pyplot as plt\r\n> plt.plot(history.history[\'accuracy\'],label=""train_accuracy"")\r\n> plt.plot(history.history[\'val_accuracy\'],label=""test_accuracy"")\r\n> plt.legend()\r\n> \r\n> Or their is another method you can use any one of the model as base model just like VGG16 is used for CNN\r\n> base_model = VGG16(input_shape = (224, 224, 3), include_top = False, weights = \'imagenet\')\r\n> for layer in base_model.layers:\r\n> layer.trainable = False\r\n> x = Flatten()(base_model.output)\r\n> x = Dense(1024, activation=\'relu\')(x)\r\n> x = Dropout(0.5)(x)\r\n> x = Dense(3, activation=\'softmax\')(x)\r\n> model = Model(base_model.input, x)\r\n> model.compile(optimizer = ""adam"", loss = \'categorical_crossentropy\',metrics = [""accuracy""])\r\n\r\nThanks for the reference code.', b'> We try to concatenate things up when they are of same property or shape.\r\n> So, if we try to concatenate `flow_from_directry()` to `LSTM` and `Conv1D` it would not make sense as one is for data augmentation and other is not .\r\n> So, basically we kind of try to do this thing of concatenation for monitoring some gestures or movements over video which basically involve temporal and spatial domains here temporal feature extraction is done by ` LSTM` , `Conv1D` and spatial features extraction is done by `Conv2D` .\r\n> So, the solution is do the data augmentation before training and store images somewhere , now for every image you would be having time series data , but you have to generate for augmented images .\r\n> After that make a parallel model as it is after all the calculation just concatenate the features together which could be done easily .\r\n> and then pass it to dense to predict .\r\n> I can do a pull request which can make it more simpler for you to understand!\r\n\r\nThanks for the valuable suggestions.\r\nI am searching if there is any library or solution available to concatenate CNN and LSTM. Also, I want to get the benefit of the Keras image data generator that provides data augmentation and improves performance considerably. By doing manual data augmentation, I will not be able to get diversity over each iteration. Basically, I want to improve images (CNN) performance by adding LSTM in parallel. However, without the Keras image data generator, substantial improvement may not be possible. ']",open,2021-08-15 17:37:34,
https://api.github.com/repos/keras-team/keras/issues/15167,b'ResNetV2 implementation problem',"[b'We are marking this as contributions welcome. It would also be very helpful if others from the community could also help us verify that this is a change we would want.\r\n\r\nThanks for filing!', b'@seermer Just to let you know, I checked the official implementation by the authors of the paper and it seems to match what Keras has (see linked pull request).', b'> @seermer Just to let you know, I checked the official implementation by the authors of the paper and it seems to match what Keras has (see linked pull request).\r\n\r\n@MaanasArora I am not exactly sure which code/repo are you referring to, I read https://github.com/KaimingHe/resnet-1k-layers but the code README actually says it is ""re-implemented by Xiang Ming"", who is not the author of the paper, it is just referenced by the original author, so I would not call it an official code.\r\nMore importantly, this repo is a re-implementation based on facebook\'s Torch implementation, NOT based on the code used in the original paper. If you look at the repo by Facebook (where Xiang Ming reimplemented based on), they only used preact **before** split after conv1, all other stages, including when dimension change, they used preact **after** split, you can see the code for this described behavior [here](https://github.com/facebookarchive/fb.resnet.torch/blob/985e569d468d23baeef5b952954dcdf3c61c5e73/models/preresnet.lua#L147-L150) which is exactly what the original paper says. I remain skeptical on why Xiang Ming changed something during re-implementation.\r\n\r\nConclusion:\r\nAlthough the resnet-1k-layers repo is released by author Kaiming He, it is a re-implementation based on Facebook\'s repo by non-author. And Facebook has its implementation exactly the same as described in the paper, whereas resnet-1k-layers differs from the paper. **I would say it is better to follow the description on the paper rather than following a repo written by a non-author that differs from the original paper.**\r\n\r\nedit: this problem is also questioned in [this ](https://github.com/KaimingHe/resnet-1k-layers/issues/2#issuecomment-545396056)issue, but Kaiming He did not reply, I am definitely not an expert on this, so probably I will leave you and Keras Team to decide on this, I just feel like following paper\'s description is probably more desired.', b""@seermer That makes sense; I'll add corrections to my comments in the PR for further discussion. Thanks for the clarification!""]",open,2021-08-13 15:53:10,
https://api.github.com/repos/keras-team/keras/issues/15166,b'Keras `predict_step` is not preserved across save and restore',[],open,2021-08-13 12:44:12,
https://api.github.com/repos/keras-team/keras/issues/15165,b'Intercept creation of StaticHashTable (and KPLs that use it) to be a DistributedTable with resource on each worker for ParameterServerStrategy.',[],open,2021-08-13 11:43:37,
https://api.github.com/repos/keras-team/keras/issues/15164,"b""dtype of RNN cell's state is changed to tf.float32 during reset_states""",[],open,2021-08-13 08:56:39,
https://api.github.com/repos/keras-team/keras/issues/15163,"b""AttributeError: 'VocabWeightHandler' object has no attribute 'name'""","[b'The same issue happened to me.\r\n\r\nI ""fixed"" it by changing the Tensorboard(histogram_freq=0), but it seems you mentioned it\'s a bad idea as we can\'t get `tensorflow profiler` in tensorboard.\r\n\r\n', b'This is a known bug with some special classes we introduced for handling StaticHashTable and MutableHashTable in keras layers (TrackableWeightHandler and VocabWeightHandler). Both the classes have caused a number of issues, so I believe the best approach here would be to remove them entirely. We will need to think about the design of how best to do that.']",open,2021-08-13 07:23:40,
https://api.github.com/repos/keras-team/keras/issues/15161,"b'public no-op, only modifies a BUILD visibility rule.'",[],open,2021-08-13 00:19:17,
https://api.github.com/repos/keras-team/keras/issues/15160,"b'public no-op, only modifies a BUILD visibility rule.'",[],open,2021-08-13 00:18:53,
https://api.github.com/repos/keras-team/keras/issues/15159,"b'public no-op, only modifies a BUILD visibility rule.'",[],open,2021-08-13 00:15:54,
https://api.github.com/repos/keras-team/keras/issues/15156,b'This changes the default for `tpu_py_test` from `disable_mlir_bridge = True` to `disable_mlir_bridge = False`.',[],open,2021-08-12 15:02:44,
https://api.github.com/repos/keras-team/keras/issues/15148,b'Support the DepthwiseConv3D and SeparableConv3D layer',[],open,2021-08-11 05:33:45,
https://api.github.com/repos/keras-team/keras/issues/15143,"b""Add track_times=False to Model.save_weights('.h5')""","[b'Original issue: [https://github.com/tensorflow/tensorflow/issues/51405](https://github.com/tensorflow/tensorflow/issues/51405)\r\n', b""@fchollet and @k-w-w, I think we should actually not putting the timestep into the h5 and its checksum at all. Only the weight itself should be saved, and the metadata shouldn't affect the artifacts.""]",open,2021-08-10 10:46:53,
https://api.github.com/repos/keras-team/keras/issues/15135,"b'[TF 2.5.0] model.build(input_shape) results in output shape of ""multiple""'","[b'This issue persists in TF 2.6.0 while using a model sub-classing API, followed by model.build() and model.summary()', b'@HunderlineK There are few workarounds suggested in [this resource](https://stackoverflow.com/questions/55235212/model-summary-cant-print-output-shape-while-using-subclass-model) bu SO community. May be it will help you. Thanks!']",open,2021-08-07 23:34:10,
https://api.github.com/repos/keras-team/keras/issues/15121,b'TypeError: Cannot convert a symbolic Keras input/output to a numpy array by using a custom layer',"[b'Is there news about this?', b'@sunw70 ,\r\nCould you please try as per the [comment](https://stackoverflow.com/questions/65366442/cannot-convert-a-symbolic-keras-input-output-to-a-numpy-array-typeerror-when-usi) above and let us know if this is still an issue ? Thanks!\r\n', b'By following the link I tried to use `disable_eager_execution()`, but now my custom layer isn\'t used and I get this error:\r\n\r\n      Model: ""model""\r\n\r\n      Layer (type)                 Output Shape              Param #   \r\n      =================================================================\r\n      input_1 (InputLayer)         [(1, 7, 1)]               0         \r\n      _________________________________________________________________\r\n      flatten (Flatten)            (1, 7)                    0         \r\n      _________________________________________________________________\r\n      dense (Dense)                (1, 10)                   80        \r\n      _________________________________________________________________\r\n      dense_1 (Dense)              (1, 7)                    77        \r\n      =================================================================\r\n      Total params: 160\r\n      Trainable params: 160\r\n      Non-trainable params: 0\r\n      _________________________________________________________________\r\n      Traceback (most recent call last):\r\n       File ""model.py"", line 408, in <module>\r\n       model = build_model(x_train, y_train)\r\n       File ""model.py"", line 383, in build_model\r\n       model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\r\n       File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 777, in fit\r\n       return func.fit(\r\n       File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 615, in fit\r\n       x, y, sample_weights = model._standardize_user_data(\r\n       File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 2294, in _standardize_user_data\r\n       training_utils_v1.check_steps_argument(x, steps, steps_name)\r\n       File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_utils_v1.py"", line 1315, in check_steps_argument\r\n       raise ValueError(\'When using {input_type} as input to a model, you should\'\r\n       ValueError: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.\r\n', b""By using Tensorflow version 2.4.1 instead, I get this:\r\n  \r\n    Epoch 1/10\r\n    WARNING:tensorflow:Gradients do not exist for variables ['MyConv/kernel:0'] when minimizing the loss.\r\n    WARNING:tensorflow:Gradients do not exist for variables ['MyConv/kernel:0'] when minimizing the loss.\r\n    2/2 [==============================] - 1s 1ms/step - loss: 113.0949 - accuracy: 1.0000\r\n    Epoch 2/10\r\n    2/2 [==============================] - 0s 1ms/step - loss: 117.4004 - accuracy: 1.0000\r\n    Epoch 3/10\r\n    2/2 [==============================] - 0s 1ms/step - loss: 124.0515 - accuracy: 1.0000\r\n    Epoch 4/10\r\n    2/2 [==============================] - 0s 994us/step - loss: 132.9754 - accuracy: 1.0000\r\n    Epoch 5/10\r\n    2/2 [==============================] - 0s 1ms/step - loss: 154.4105 - accuracy: 1.0000\r\n    Epoch 6/10\r\n    2/2 [==============================] - 0s 4ms/step - loss: 157.2047 - accuracy: 1.0000\r\n    Epoch 7/10\r\n    2/2 [==============================] - 0s 981us/step - loss: 184.0751 - accuracy: 1.0000\r\n    Epoch 8/10\r\n    2/2 [==============================] - 0s 902us/step - loss: 201.2888 - accuracy: 1.0000\r\n    Epoch 9/10\r\n    2/2 [==============================] - 0s 888us/step - loss: 219.6019 - accuracy: 1.0000\r\n    Epoch 10/10\r\n    2/2 [==============================] - 0s 5ms/step - loss: 238.6077 - accuracy: 1.0000\r\n\r\n    ValueError: Input 0 of node model/MyConv/model/MyConv/strided_slice/_assign was passed float from \r\n    model/MyConv/ReadVariableOp/resource:0 incompatible with expected resource.\r\n   \r\nHow to get rid of these warnings and the last error?"", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Updates about this?', b'@sunw70 ,\r\nCan you please take a look at this [comment](https://forums.developer.nvidia.com/t/incompatible-with-expected-resource/72935/12) and [link](https://stackoverflow.com/questions/56580538/tf-keras-api-with-tf-dataset-problem-steps-per-epoch-argument-problem) for similar error.It helps.Thanks!', b'@tilakrayal thanks for your reply: I tried to use `steps_per_epoch` in `model.fit`, but now I get this error:\r\n\r\n    Model: ""model""\r\n    _________________________________________________________________\r\n    Layer (type)                 Output Shape              Param #   \r\n    =================================================================\r\n    input_1 (InputLayer)         [(1, 7, 1)]               0         \r\n    _________________________________________________________________\r\n    flatten (Flatten)            (1, 7)                    0         \r\n    _________________________________________________________________\r\n    dense (Dense)                (1, 10)                   80        \r\n    _________________________________________________________________\r\n    dense_1 (Dense)              (1, 7)                    77        \r\n    =================================================================\r\n    Total params: 160\r\n    Trainable params: 160\r\n    Non-trainable params: 0\r\n    _________________________________________________________________\r\n    \r\n    Traceback (most recent call last):\r\n     File ""model.py"", line 409, in <module>\r\n     model = build_model(x_train, y_train)\r\n     File ""model.py"", line 384, in build_model\r\n     model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose, steps_per_epoch=10)\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 777, in fit\r\n     return func.fit(\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 640, in fit\r\n     return fit_loop(\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 177, in model_iteration\r\n     f = _make_execution_function(model, mode)\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py"", line 547, in _make_execution_function\r\n     return model._make_execution_function(mode)\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 2079, in _make_execution_function\r\n     self._make_train_function()\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/engine/training_v1.py"", line 2010, in _make_train_function\r\n     updates = self.optimizer.get_updates(\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 759, in get_updates\r\n     grads = self.get_gradients(loss, params)\r\n     File ""/home/es/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 751, in get_gradients\r\n     raise ValueError(""Variable {} has `None` for gradient. ""\r\n     ValueError: Variable <tf.Variable \'MyConv/kernel:0\' shape=(3, 1, 1) dtype=float32> has `None` for gradient. Please make sure      \r\n     that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\r\n\r\nI did some research: maybe the problem could be that the kernel weights have been defined, but some of them are not used. How could I make sure that all weights are used?\r\n\r\nThanks again for your support.', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n""]",open,2021-08-06 16:00:21,
https://api.github.com/repos/keras-team/keras/issues/15119,b'No information in official doc regarding implementation of class_weight in customized model.fit() method',[],open,2021-08-06 14:16:26,
https://api.github.com/repos/keras-team/keras/issues/15101,"b'Enter cross-replica context when reading SyncOnReadVariable in operator override, if the read happens in the default replica-context.'",[],open,2021-08-05 21:59:32,
https://api.github.com/repos/keras-team/keras/issues/15066,b'tf.saved_model.save fails when there is tf.reshape in tf.keras.Model call method',"[b'@kacper-kleczewski,\r\nCould reproduce the error with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/e9b75f4dfe8b144a44db41fbc04154b0/keras_15066.ipynb) of the working code. ', b""Can you try with `tf.shape` instead of `get_shape`? Note that it doesn't unpack automatically as a tuple and it doesn't have `as_list` method. "", b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""> Can you try with `tf.shape` instead of `get_shape`? Note that it doesn't unpack automatically as a tuple and it doesn't have `as_list` method.\r\n\r\nProposed solution doesn't help. Do you have plans to fix issue with get_shape?\r\n\r\n`tf.shape` doesn't help because according to TF spec:\r\n`Note: When using symbolic tensors, such as when using the Keras API, tf.shape() will return the shape of the symbolic tensor.`\r\n\r\nSo the results are:\r\n\r\n- first call:\r\n\r\n![image](https://user-images.githubusercontent.com/84719017/128513740-a1298b76-866c-45c6-a170-47583dc60858.png)\r\n\r\n- second call:\r\n\r\n![image](https://user-images.githubusercontent.com/84719017/128513809-6723cf15-f2ca-4b9e-be39-76db14bb4e08.png)\r\n\r\nresulta from `tf.shape` doesn't contain shape information of `inputs` tensor shape. When I try to print it with tf.print, output is empty line.""]",open,2021-08-04 10:58:15,
https://api.github.com/repos/keras-team/keras/issues/15065,b'model.summary() has inconsistent output for identical model (sequential vs functional api)',"[b'@JulianCologne,\r\nThis behavior seems to expected from the documentation of [Functional API](https://keras.io/guides/functional_api/) and [Sequential API](https://keras.io/guides/sequential_model/). What do you think?', b'I do not see an explicit mention that the apis produce slightly different outputs with the summary function.\r\n\r\nFor me as a user this behaviour is unexpected and feels inconsistent.\r\nI see the sequential and functional api as two different styles to create a model.\r\nI then expect the models to be 100% identical and not 99% identical with some small differences here and there.\r\n\r\nThey even produce the exact same graph/plot using `keras.utils.plot_model()`.\r\n\r\n![image](https://user-images.githubusercontent.com/25177421/128190776-5d255c52-7ac1-46b0-9314-ec477bba30ad.png)\r\n\r\n', b""I agree with @JulianCologne. This confused me at first before I learned that `Sequential` models don't show the initial `InputLayer`. We can see that the [source code](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/engine/sequential.py#L144) just hides the initial `InputLayer` for historical reasons:\r\n```py\r\n  def layers(self):\r\n    # Historically, `sequential.layers` only returns layers that were added\r\n    # via `add`, and omits the auto-generated `InputLayer` that comes at the\r\n    # bottom of the stack.\r\n    # `Trackable` manages the `_layers` attributes and does filtering\r\n    # over it.\r\n    layers = super(Sequential, self).layers\r\n    if layers and isinstance(layers[0], input_layer.InputLayer):\r\n      return layers[1:]\r\n    return layers[:]\r\n```\r\n\r\nI think things would be clearer and more consistent if the `InputLayer` was shown in `Sequential` models.""]",open,2021-08-04 10:00:05,
https://api.github.com/repos/keras-team/keras/issues/15048,b'Failure to load model files',"[b'Keras does not know about your subclasses, so it cannot load the model. This may not be the problem you are seeing, but you will need to use CustomObjects anyway.\r\n\r\nThis describes how to use the CustomObject registry:\r\n\r\nhttps://keras.io/guides/serialization_and_saving/', b'Yes exactly.\r\n', b'> Keras does not know about your subclasses, so it cannot load the model. This may not be the problem you are seeing, but you will need to use CustomObjects anyway.\r\n> \r\n> This describes how to use the CustomObject registry:\r\n> \r\n> https://keras.io/guides/serialization_and_saving/\r\n\r\n@LanceNorskog  Thanks a lot for your response. I followed your advice and made the following change to my load_model func().\r\n```\r\ndef load_model(model):\r\n    model = KM.load_model(\r\n        ""save_model/save_model.h5"", custom_objects={""Trainer"": Trainer}\r\n    )\r\n```\r\n\r\nHowever, the problem persists..\r\n>     raise ValueError(\'No model found in config file.\')\r\n> ValueError: No model found in config file.', b""There are two kinds of model files: .json and .h5. .h5 files are for storing lots of numbers. It may be that Keras will only load weights from a .h5 file. If you save the model as 'save_model.json' it should reload. It will also be in text form, so it will be a much larger file than a .h5 file. \r\n\r\nThe manual page mentions .hdf5 files, which is the full name of this format.\r\nhttps://keras.io/api/callbacks/model_checkpoint/\r\n\r\nThis blog has some good examples of saving&loading models:\r\nhttps://machinelearningmastery.com/check-point-deep-learning-models-keras/\r\n\r\nMachine Learning Master has a great collection of Keras recipes. I've fixed several problems by looking up what he did.\r\n""]",open,2021-08-02 00:42:25,
https://api.github.com/repos/keras-team/keras/issues/15038,b'Add migration docstring to `initializers/initializers_v2:ones`.  Ones takes no arguments and has no changed behavior.',[],open,2021-07-30 19:42:25,
https://api.github.com/repos/keras-team/keras/issues/15024,b'tf.keras.models.save_model not saving the probabilistic_models',"[b'@rrklearn2020,\r\nCan you try using **`Custom Objects`** as suggested in this [Stack Overflow Answer](https://stackoverflow.com/a/62690263/14290681) and let us know how it goes? Thanks! ', b""@rmothukuru\r\nI had tried the custom object solution for simple models, and it works for some TFP layers. But when it comes to functional or sub-classing model creation, it's difficult to handle custom objects with various types of TFP layers. ""]",open,2021-07-29 16:46:32,
https://api.github.com/repos/keras-team/keras/issues/15021,b'Update example of aggregating gradients by self',"[b'Ping @rchao for review.', b'Let me run some tests with the change and will update back here. Thanks', b""@Tilps  Can you please check @rchao's comments and keep us posted ? Thanks!"", b'This example is currently specifically for the experimental_aggregate_gradients=False scenario, which is a case not handled in either of those tutorials.\r\n', b'@rchao Can you please assist on above comments from @Tilps. Thanks!', b'> This example is currently specifically for the experimental_aggregate_gradients=False scenario, which is a case not handled in either of those tutorials.\r\n\r\nThanks! This makes sense. Can you add a unit test to cover this example code to make sure it achieves the gradient aggregation goal?', b'(Removing the label is @rchao is the active reviewer for this).']",open,2021-07-29 03:22:46,
https://api.github.com/repos/keras-team/keras/issues/15013,b'Support DistributeDatasetFromFunction for model.fit + ParameterServerStrategy.',[],open,2021-07-28 07:50:30,
https://api.github.com/repos/keras-team/keras/issues/14999,b'Switch initializers and regularization layers (except dropout) to stateless RNG.',[],open,2021-07-23 23:43:56,
https://api.github.com/repos/keras-team/keras/issues/14984,b'Support input of temporal sample_weights for model training on ragged tensors',[],open,2021-07-22 17:47:10,
https://api.github.com/repos/keras-team/keras/issues/14979,b'Inconsistency reporting model(x) vs model.predict(x)',"[b'I could reproduce the issue. The warning is correct as the input data size is different when compared to the model that was built. \r\n\r\n[Here](https://colab.research.google.com/gist/jvishnuvardhan/b73559f67d738bb699d2908f5df5dbf9/untitled1005.ipynb) is a gist for our reference. Thanks!', b'@jvishnuvardhan The issue is not that the warning is not correct. The question is why is the warning only shown for model.predict(x) but not for model(x)?\r\n\r\n']",open,2021-07-20 23:32:16,
https://api.github.com/repos/keras-team/keras/issues/14969,"b'Add ""Copy Code Sample"" Option in Keras Documentation, similar to that in TF.Org'",[],open,2021-07-20 11:32:30,
https://api.github.com/repos/keras-team/keras/issues/14960,b'Model trains successfully but generating predictions gives ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor',"[b""I changed my attention layer to the one from Keras. But I still get the same error.\r\n\r\n```\r\nattention = Attention(causal = True)([decoder_outputs,encoder_outputs ])\r\n\r\n# Dense layer\r\ndecoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\r\ndecoder_outputs = decoder_dense(attention)\r\n\r\n# Define the model\r\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n```\r\n\r\nAnd this is now  in the inference model:\r\n\r\n```\r\nattention = model.layers[-2]([decoder_outputs,encoder_outputs ])\r\n\r\ndecoder_dense = model.layers[-1]\r\ndecoder_outputs = decoder_dense(attention)\r\ndecoder_model = Model(\r\n    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\r\n)\r\n```\r\n\r\nPlease help. I have no idea what is going on. From what I see the graph is connected. Model fits successfully! "", b'I also created [a new question](https://stackoverflow.com/questions/68590104/graph-disconnected-cannot-obtain-value-for-tensor-kerastensor-in-inference-mode) in SO']",open,2021-07-19 21:45:53,
https://api.github.com/repos/keras-team/keras/issues/14954,b'Allow keras constraints that affect both the kernel and the bias at the same time.',[b'Is there any idea on how this change in the constraints behaviour could be approached?'],open,2021-07-19 15:12:47,
https://api.github.com/repos/keras-team/keras/issues/14930,b'Update functional.py',[],open,2021-07-15 11:10:37,
https://api.github.com/repos/keras-team/keras/issues/14925,b'Fix tf.name_scope support for Keras nested layers.',[],open,2021-07-14 06:24:02,
https://api.github.com/repos/keras-team/keras/issues/14919,b'How to get shape[0] of the input tensor of a custom layer and also the tensor values',[],open,2021-07-13 13:38:30,
https://api.github.com/repos/keras-team/keras/issues/14915,b'Performance Bug: Quadratic runtime behavior in model.fit/predict with tf.keras.layers.experimental.preprocessing.TextVectorization',"[b'You can also find a gist [here](https://colab.research.google.com/gist/aucth/9a020572f3427ad3e0f0974be2ae4b2e/untitled50666.ipynb).', b'Any news on this issue?', b'After a little investigation in tensorflow 2.5.0 and 2.6.0 I have been able to conclude that the issue is completely unrelated to any specific Keras layer. It seems to have something to do with the iteration over numpy arrays of string dtype.\r\n\r\nThe O(n^2) runtime behavior only occurs when `x` is a numpy array of string dtype. If `x` is a numpy array of int or float, or a `tf.data.Dataset` of any dtype, then the runtime becomes O(n) as it should be.\r\n\r\nIn the following example a custom `DummyLayer` is used instead of any existing Keras layer, and `x` is casted to `str` which becomes of numpy dtype `<U21`.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass DummyLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(DummyLayer, self).__init__()\r\n\r\n    def call(self, inputs, *args, **kwargs):\r\n        return tf.random.uniform(shape=tf.shape(inputs))\r\n\r\n\r\n# generate some dummy data\r\nn = 1000000\r\nx = np.random.randint(1, 100, (n, 1)).astype(str)\r\ny = np.random.uniform(0, 1, (n, 1))\r\n\r\ninput = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\r\noutput = DummyLayer()(input)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\nmodel.compile(loss=""mse"", optimizer=""adam"")\r\n\r\nx_first_half, x_second_half = x[: int(n / 2)], x[int(n / 2 + 1) :]\r\ny_first_half, y_second_half = y[: int(n / 2)], y[int(n / 2 + 1) :]\r\n\r\nbatch_size = 32\r\nmodel.fit(x, y, epochs=1, batch_size=batch_size)\r\nmodel.fit(x_first_half, y_first_half, epochs=1, batch_size=batch_size)\r\nmodel.fit(x_second_half, y_second_half, epochs=1, batch_size=batch_size)\r\n\r\nmodel.predict(x, verbose=1, batch_size=batch_size)\r\nmodel.predict(x_first_half, verbose=1, batch_size=batch_size)\r\nmodel.predict(x_second_half, verbose=1, batch_size=batch_size)\r\n```\r\nRuntimes are observed to not scale linearly with input size:\r\n```\r\n31250/31250 [==============================] - 208s 7ms/step - loss: 0.1663\r\n15625/15625 [==============================] - 42s 3ms/step - loss: 0.1662\r\n15625/15625 [==============================] - 44s 3ms/step - loss: 0.1669\r\n31250/31250 [==============================] - 209s 7ms/step\r\n15625/15625 [==============================] - 54s 3ms/step\r\n15625/15625 [==============================] - 47s 3ms/step\r\n```\r\n\r\nIf `x` is instead casted to `int` and input layer dtype configured to `tf.int64`, the runtimes become:\r\n```\r\n31250/31250 [==============================] - 12s 389us/step - loss: 0.1668\r\n15625/15625 [==============================] - 7s 421us/step - loss: 0.1669\r\n15625/15625 [==============================] - 12s 760us/step - loss: 0.1667\r\n31250/31250 [==============================] - 9s 283us/step\r\n15625/15625 [==============================] - 4s 276us/step\r\n15625/15625 [==============================] - 7s 452us/step\r\n```\r\n\r\nIf `x` and `y` are passed to the model as a `tf.data.Dataset` the runtime remains O(n) even when `x` has string dtype.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nclass DummyLayer(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(DummyLayer, self).__init__()\r\n\r\n    def call(self, inputs, *args, **kwargs):\r\n        return tf.random.uniform(shape=tf.shape(inputs))\r\n\r\n\r\n# generate some dummy data\r\nn = 1000000\r\nx = np.random.randint(1, 100, (n, 1)).astype(str)\r\ny = np.random.uniform(0, 1, (n, 1))\r\n\r\ninput = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\r\noutput = DummyLayer()(input)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\nmodel.compile(loss=""mse"", optimizer=""adam"")\r\n\r\nx_first_half, x_second_half = x[: int(n / 2)], x[int(n / 2 + 1) :]\r\ny_first_half, y_second_half = y[: int(n / 2)], y[int(n / 2 + 1) :]\r\n\r\n\r\ndef to_ds(x, y, batch_size):\r\n    return tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\r\n\r\n\r\nds = to_ds(x, y, batch_size)\r\nds_first_half = to_ds(x_first_half, y_first_half, batch_size)\r\nds_second_half = to_ds(x_second_half, y_second_half, batch_size)\r\n\r\nmodel.fit(ds, epochs=1)\r\nmodel.fit(ds_first_half, epochs=1)\r\nmodel.fit(ds_second_half, epochs=1)\r\n\r\nmodel.predict(ds, verbose=1)\r\nmodel.predict(ds_first_half, verbose=1)\r\nmodel.predict(ds_second_half, verbose=1)\r\n```\r\n\r\ngives:\r\n```\r\n31250/31250 [==============================] - 19s 591us/step - loss: 0.1668\r\n15625/15625 [==============================] - 9s 593us/step - loss: 0.1662\r\n15625/15625 [==============================] - 9s 584us/step - loss: 0.1667\r\n31250/31250 [==============================] - 10s 333us/step\r\n15625/15625 [==============================] - 5s 333us/step\r\n15625/15625 [==============================] - 5s 334us/step\r\n```', b'Thanks for investigating. This helps a lot.\r\n']",open,2021-07-13 09:37:35,
https://api.github.com/repos/keras-team/keras/issues/14914,b'Save the Embeddings from a Complete Triplet Loss Network',[],open,2021-07-13 08:30:51,
https://api.github.com/repos/keras-team/keras/issues/14913,"b""Saved model loading : KeyError: '__inference_depthwise_conv2d_layer_call_fn_126'""","[b'@sushreebarsa Can you please assign someone or provide some help on this ? Thanks ', b'@rmothukuru Can you please provide any suggestions on this ? Thanks !!', b""Hi everyone, has a solution been found to this issue? I'm also facing the same problem:\r\n```KeyError: '__inference_efficientnet_layer_call_fn_53207'```."", b'https://drive.google.com/file/u/0/d/1o00sUhX93xtY4eLo7U8Nrz1xpaevHrz8/edit for potential repro for this issue.', b'@k-w-w Can you please let me know any suggestions you have on this issue ?  Thanks ', b'Is there any update on this issue?', b'Any updates?', b'Any updates on this issue?', b'Any updates on this issue?']",open,2021-07-13 00:08:25,
https://api.github.com/repos/keras-team/keras/issues/14909,b'How to get all information of an input tensor inside a custom layer?',[],open,2021-07-12 18:32:20,
https://api.github.com/repos/keras-team/keras/issues/14907,"b'For Keras training, can we relax the steps checking when distributed dataset is passed in'","[b'Hello @annyan09023, thanks for opening the issue. Can you give the [DatasetCreator](https://www.tensorflow.org/api_docs/python/tf/keras/utils/experimental/DatasetCreator) API a try and see if it mitigates the OOM issue?', b'Okay, looking']",open,2021-07-12 17:36:20,
https://api.github.com/repos/keras-team/keras/issues/14904,b'Model with nested input spec fails',[],open,2021-07-12 10:45:17,
https://api.github.com/repos/keras-team/keras/issues/14888,b'Saved model performance discrepancies for models with equivalent behavior',"[b'@haifeng-jin Do you have any idea on why we see that behavior: our models have a lot of inputs and so a lot of layers and currently the loading memory consumption is very large for what it is. We tried reducing the dtype quality but it does not reduce the ram enough.  \r\nThanks !', b'@tanguycdls Do you have a notebook or something for me to reproduce this? I am not aware of this issue.', b'@haifeng-jin thanks the colab is here: https://colab.research.google.com/gist/tanguycdls/fee7c221484cf4ee0e3273faa8e270b0/memory-overhead-tests.ipynb\r\n\r\nIn the test we can see that memory usage at loading time is not correlated to the dtype and the weights (all are similar) but to the actual structure of the code.\r\n```\r\nTo test it, we decided to write a simple but heavy model with 250 Dense layers in order to showcase the overhead for each layer. The variations tested were as follows, with the top RAM consumption as well as time of execution:\r\n\r\n1. 250 Dense layers w/o Activation (BASELINE) => 600 MB, 9s\r\n2. 250 Dense layers w/ activation set as a parameter inside of the Dense call => 600MB, 9s\r\n3. 250 Dense layers using Functional API, each layer followed by another activation layer => 710MB, 12s\r\n4. One custom layer with one sequential layer containing all 250 Denses => 790MB, 12.2s\r\n5. One custom layer w/ list of Dense layers as attributes, called using a for loop over the list (equivalent to Baseline) => 550MB, 7.5s\r\n\r\n```\r\n\r\nHere all the patterns are the same only the code writing is different, we clearly see that having layers in layers increase memory usage (1 vs 4 are strictly equivalent). Our hypothesis was that since each layer is wrapped into a tf function we have a bigger graph since each just calls the graph underneath while in option 4 we only have 1 Tf function. \r\n\r\n\r\nWe worked more on the subject and for now the fix was to convert our model to constants using low level TF api convert_to_constants_v2 and then save again the model as a savedmodel: then the graph is inlined and smaller and the memory usage is improved (in our real model that i cannot share we divided the memory usage by two 1.4Gb --> 0.6Gb). \r\n\r\nHowever its unfortunate that we need this and we would prefer to stay in the high level api of Tensorflow/ keras.\r\n\r\nfeel free to tell us if you need more repro !']",open,2021-07-08 09:23:19,
https://api.github.com/repos/keras-team/keras/issues/14877,b'sample_weight taking long to initialize',"[b""```py\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\n\r\nx_train = np.random.rand(10000, 10)\r\ny_train = np.random.rand(10000, 1)\r\nw_train = np.random.rand(10000, 1)\r\n\r\ndef run(with_sample_weights=True):\r\n  model = keras.Sequential()\r\n  model.add(keras.layers.Dense(64, input_dim=x_train.shape[1], activation='relu')) \r\n  model.add(keras.layers.Dropout(0.1))\r\n  model.add(keras.layers.Dense(64, activation='relu'))\r\n  model.add(keras.layers.Dropout(0.1))\r\n  model.add(keras.layers.Dense(64, activation='relu'))\r\n  model.add(keras.layers.Dropout(0.1))\r\n  model.add(keras.layers.Dense(64, activation='relu'))\r\n  model.add(keras.layers.Dropout(0.1))\r\n  model.add(keras.layers.Dense(1, activation='sigmoid'))\r\n  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n\r\n  if with_sample_weights:\r\n    model.fit(x_train, y_train, sample_weight=w_train)\r\n  else:\r\n    model.fit(x_train, y_train)\r\n\r\nimport time\r\n\r\nstart = time.time()\r\nfor i in range(10):\r\n  run(True)\r\nend = time.time()\r\nprint(end - start)\r\n\r\nstart = time.time()\r\nfor i in range(10):\r\n  run(False)\r\nend = time.time()\r\nprint(end - start)\r\n```\r\nThe time elapsed is 31s vs 14s.\r\nIt seems the cause is strided_slice() function."", b'Yes, exactly. The problem gets worse when the sample size is higher.\r\n\r\nFor example, using the above code (plus a batch size of 10,000), for a sample size of 1,000,000 the time taken was 1253.8s vs 17.1s \r\ni.e 73 times more time to train the model!\r\nFor comparison, for the 10,000 case as above, the time was 18.2s vs 5.1s', b'This bug is preventing usage of sample weights for high instance count data as initialization time explode. Can this please be looked at. Please try an example with `w_train.shape[0] == 100,000,000`.  Left model fit running for hours and did not initialize']",open,2021-07-07 18:16:17,
https://api.github.com/repos/keras-team/keras/issues/14872,b'Convert model with  `tensorflow.keras.layers.experimental.preprocessing` to TFLite',[],open,2021-07-06 20:12:05,
https://api.github.com/repos/keras-team/keras/issues/14867,b'Keras bisect and editable install',"[b'@qlzh727 Would you please take a look at this issue?\r\nI am not sure about the process for publishing to PyPI.', b'Took a quick look for development mode in https://setuptools.readthedocs.io/en/latest/userguide/quickstart.html?highlight=development-mode#development-mode. \r\n\r\nIt seems that the requirement is to have the setup.py in the keras package root directory, and I had following try, which failed.\r\n\r\nIt seems that you will need to build the protobufs that are needed by save model to work, and you probably grab them either from the existing keras pip package, or build them from scratch. \r\n```\r\n(keras-nightly) scottzhu-macbookpro2:keras scottzhu$ mv keras/tools/pip_package/setup.py .\r\n(keras-nightly) scottzhu-macbookpro2:keras scottzhu$ pip install -e .\r\nObtaining file:///Users/scottzhu/keras/keras\r\nInstalling collected packages: keras\r\n  Running setup.py develop for keras\r\nSuccessfully installed keras\r\n(keras-nightly) scottzhu-macbookpro2:keras scottzhu$ pip list\r\nPackage                 Version             Location                   \r\n----------------------- ------------------- ---------------------------\r\nabsl-py                 0.12.0              \r\nastunparse              1.6.3               \r\ncached-property         1.5.2               \r\ncachetools              4.2.1               \r\ncertifi                 2020.12.5           \r\nchardet                 4.0.0               \r\nclang                   5.0                 \r\nflatbuffers             1.12                \r\ngast                    0.4.0               \r\ngoogle-auth             1.28.0              \r\ngoogle-auth-oauthlib    0.4.4               \r\ngoogle-pasta            0.2.0               \r\ngrpcio                  1.37.0              \r\nh5py                    3.1.0               \r\nidna                    2.10                \r\nimportlib-metadata      3.10.0              \r\nkeras                   2.7.0               /Users/scottzhu/keras/keras\r\nKeras-Preprocessing     1.1.2               \r\nMarkdown                3.3.4               \r\nnumpy                   1.19.5              \r\noauthlib                3.1.0               \r\nopt-einsum              3.3.0               \r\npip                     18.1                \r\npip-licenses            3.4.0               \r\nprotobuf                3.15.7              \r\nPTable                  0.9.2               \r\npyasn1                  0.4.8               \r\npyasn1-modules          0.2.8               \r\nrequests                2.25.1              \r\nrequests-oauthlib       1.3.0               \r\nrsa                     4.7.2               \r\nsetuptools              54.2.0              \r\nsix                     1.15.0              \r\ntb-nightly              2.6.0a20210427      \r\ntensorboard-data-server 0.6.0               \r\ntensorboard-plugin-wit  1.8.0               \r\ntermcolor               1.1.0               \r\ntf-estimator-nightly    2.5.0.dev2021032601 \r\ntf-nightly              2.6.0.dev20210619   \r\ntyping-extensions       3.7.4.3             \r\nurllib3                 1.26.4              \r\nWerkzeug                1.0.1               \r\nwheel                   0.36.2              \r\nwrapt                   1.12.1              \r\nzipp                    3.4.1               \r\nYou are using pip version 18.1, however version 21.1.3 is available.\r\nYou should consider upgrading via the \'pip install --upgrade pip\' command.\r\n(keras-nightly) scottzhu-macbookpro2:keras scottzhu$ python\r\nPython 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import keras\r\n2021-07-07 10:45:04.479581: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\r\n2021-07-07 10:45:04.479642: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\r\n2021-07-07 10:45:04.479655: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\r\n2021-07-07 10:45:04.479665: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/scottzhu/keras/keras/keras/__init__.py"", line 25, in <module>\r\n    from keras import models\r\n  File ""/Users/scottzhu/keras/keras/keras/models.py"", line 20, in <module>\r\n    from keras import metrics as metrics_module\r\n  File ""/Users/scottzhu/keras/keras/keras/metrics.py"", line 26, in <module>\r\n    from keras import activations\r\n  File ""/Users/scottzhu/keras/keras/keras/activations.py"", line 20, in <module>\r\n    from keras.layers import advanced_activations\r\n  File ""/Users/scottzhu/keras/keras/keras/layers/__init__.py"", line 24, in <module>\r\n    from keras.engine.input_layer import Input\r\n  File ""/Users/scottzhu/keras/keras/keras/engine/input_layer.py"", line 21, in <module>\r\n    from keras.engine import base_layer\r\n  File ""/Users/scottzhu/keras/keras/keras/engine/base_layer.py"", line 42, in <module>\r\n    from keras.saving.saved_model import layer_serialization\r\n  File ""/Users/scottzhu/keras/keras/keras/saving/saved_model/layer_serialization.py"", line 22, in <module>\r\n    from keras.saving.saved_model import save_impl\r\n  File ""/Users/scottzhu/keras/keras/keras/saving/saved_model/save_impl.py"", line 32, in <module>\r\n    from keras.saving.saved_model import load as keras_load\r\n  File ""/Users/scottzhu/keras/keras/keras/saving/saved_model/load.py"", line 28, in <module>\r\n    from keras.protobuf import saved_metadata_pb2\r\nImportError: cannot import name \'saved_metadata_pb2\'\r\n>>> \r\n```', b'After copying the protobuf generated py files from existing distribution, the development mode works.\r\n\r\n```\r\n(tf-nightly) scottzhu-macbookpro2:~ scottzhu$ ls tf-nightly/lib/python3.6/site-packages/keras/protobuf/*.py \r\nBUILD                   projector_config.proto  saved_metadata.proto    versions.proto          \r\n(tf-nightly) scottzhu-macbookpro2:~ scottzhu$ cp tf-nightly/lib/python3.6/site-packages/keras/protobuf/*.py keras/keras/keras/protobuf/\r\n(tf-nightly) scottzhu-macbookpro2:~ scottzhu$ ls keras/keras/keras/protobuf/\r\nBUILD\t\t\tprojector_config.proto\tsaved_metadata.proto\tversions.proto\r\n__init__.py\t\tprojector_config_pb2.py\tsaved_metadata_pb2.py\tversions_pb2.py\r\n\r\nscottzhu-macbookpro2:~ scottzhu$ source keras-nightly/bin/activate\r\n(keras-nightly) scottzhu-macbookpro2:~ scottzhu$ python\r\nPython 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import keras\r\n2021-07-07 10:56:17.480611: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count\r\n2021-07-07 10:56:17.480667: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count\r\n2021-07-07 10:56:17.480706: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api\r\n2021-07-07 10:56:17.480722: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api\r\n>>> \r\n\r\n```', b'In summary, I think the walk around is to mv the setup.py to a package root, and copy the protobuf py from the existing distribution, which will then make the keras Git source tree work with PIP development mode.', b""@qlzh727 Not so quickly close an issue :wink: .. as I've already tried something like this before opening the ticket and it is still quite not practical to do all these manual steps if you need to move commit by commit as with `git bisect`. \r\n\r\nAlso I think that we need need other manual steps for golden or not?\r\n\r\nHave you tried to:\r\n`import keras.api._v2` ?\r\n\r\n"", b'Hum, good point for keras.api.*. They are the \\_\\_init__ files that used to populate the tf.keras namespace (if you use `from tensorflow import keras`). Those files are included in the distribution, but not in the source code (since they are generated files via script, same as the protobuf py files).\r\n\r\nIt seems that we will need some small script that move and generate all the needed files from the source code, which I think is doable (since they should be generate via bazel). You can rerun the scripts after you checkout certain git commits to do bisect. ', b""We can run two scripts with `git bisect` eg:\r\n\r\n`git bisect run sh -c 'bootstrap_keras.sh  && bazel test -c opt keras:backend_test'`.\r\n\r\nBut I think that simply coping stuffs in `keras/keras/keras/protobuf/` as you suggest it will go to produce a git mess in the source code checkout:\r\n\r\nE.g. the protobuf dir at checkout has already:\r\n `BUILD  projector_config.proto  saved_metadata.proto  versions.proto`\r\n\r\nSo I suppose that we need also something else to reach a working solution.\r\n\r\nAnd now the 1 Billion dollar question:\r\nWhen we move between Keras commits what we can do with the Tensorflow dependency? Can we move over nightly? Are we at least 1 day tf-nightly API compatible?"", b'For the mess it creates, I guess the script need some post cleanup for any generated files, or at least make git to untrack them (eg update the gitignore just for bisect). \r\n\r\nFor TF dependency, I think we are at least 1 tf-nightly API compatible. I think seldomly make backward incomatible changes in tf. ', b""> For the mess it creates, I guess the script need some post cleanup for any generated files, or at least make git to untrack them (eg update the gitignore just for bisect).\r\n\r\nI don't know if in the end it could be clean enough or we need to change something else. I think it is not only bisect it is simply also the case if a dev wants to manually move between Keras commits and test a third_party library that depends on Keras.\r\n\r\n> For TF dependency, I think we are at least 1 tf-nightly API compatible. I think seldomly make backward incomatible changes in tf.\r\n\r\nCan we handle this more formally in:\r\nhttps://github.com/keras-team/keras/blob/41bd035a19904118944ef271342562f65b90fd3c/requirements.txt#L6\r\n\r\nE.g. `dev20210707` or a `>= <=` range so that the compatibility control is on the Keras team side. If I move between commits on a long range or simply on a longer then 1 day range I will know exactly on what specific TF version it is tested in the Keras CI the specific commit and so what TF version to install on my local setup/CI.\r\n\r\n\r\n\r\n"", b'we could force a more strict tf-nightly version, but this will leave more maintainance issue since we have to update this often. So far we only change this file when least tf-nightly is broken, and we want to ping to an older nightly version. \r\n\r\nCould you also just make the update on your local workspace to achieve smaller range you want?', b""> Could you also just make the update on your local workspace to achieve smaller range you want?\n\nI really don't know when the API is broken or not. \nI only know that you have runned tests on master with the tf-nightly version available at the datetime of the commit or at the CI job datetime that I  suppose are quite similar.\n\nSo what we are asking to the developer? To install tf-nightly with the same datetime of the commit every time he move between commits? \nDoes he need also to manually check  that the commit time crossed the new tf-nightly scheduled release in the same day? \nDoes he need to calc timezones?\n\nBy the external developer point of view he really don't know when a specific Keras commit is not compatibile anymore with a new or old tf-nightly version as the dependency is generic.\n\n\n"", b'I think that that the CI itself or copybara will need to update the tf-nightly version on the commit push if tested on a new nightly release.\n\nCan we claim any other compatibility on the specific commit? I suppose not as we are not testing a tf-nightly matrix', b'If we are quite shure that we have stable TF API we could just soften the TF nightly version dependency with a `>=`.', b'@qlzh727 Just to make some step ahead with `pip install -e ` \r\n\r\n> Hum, good point for keras.api.*. They are the __init__ files that used to populate the tf.keras namespace (if you use from tensorflow import keras). Those files are included in the distribution, but not in the source code (since they are generated files via script, same as the protobuf py files).\r\n\r\nDo we have any other candidate solution instead of moving manually all these file every time?', b'\r\n\r\n> @qlzh727 Just to make some step ahead with `pip install -e `\r\n> \r\n> > Hum, good point for keras.api.*. They are the **init** files that used to populate the tf.keras namespace (if you use from tensorflow import keras). Those files are included in the distribution, but not in the source code (since they are generated files via script, same as the protobuf py files).\r\n> \r\n> Do we have any other candidate solution instead of moving manually all these file every time?\r\n\r\nSo far I think either copying or regenerate those files will be the solution I recommend. Including the generated content in the Git repo will create more maintainance overhead since we need to keep them in sync with the original source.', b'> So far I think either copying or regenerate those files will be the solution I recommend. Including the generated content in the Git \r\nIf will copy these file from `bazel-bin/` they will be monitored by  git\r\n\r\nAlso I cannot play with `PYTHONPATH=bazel-bin/` as the proto source  `.proto` and the expected gen python are expected to have has the same module/dir hierarchy.\r\n\r\nCan we change in the repository `keras/protobuf/` with something like `keras/protobuf_defs/` or `protobuf_src` or as you like?', b'@qlzh727  As we claim now in the Readme:\r\n\r\n`the nightly Keras releases are usually compatible with the corresponding version of the tf-nightly releases (e.g. keras-nightly==2.7.0.dev2021100607 should be used with tf-nightly==2.7.0.dev2021100607).` \r\n\r\nCan we just pin on the `tf-nightly` version on what we are running the test with the CI on master every day?', b'> @qlzh727 As we claim now in the Readme:\r\n> \r\n> `the nightly Keras releases are usually compatible with the corresponding version of the tf-nightly releases (e.g. keras-nightly==2.7.0.dev2021100607 should be used with tf-nightly==2.7.0.dev2021100607).`\r\n> \r\n> Can we just pin on the `tf-nightly` version on what we are running the test with the CI on master every day?\r\n\r\nSorry for the late reply. \r\n\r\nWe could do that, (we sometimes pin to a old version on CI to avoid some breakage caused on TF side). This will involve some script to automatically update the requirements.txt at a certain time everyday, and it need to make sure the corresponding tf-nightly does exist.\r\n\r\nLet me see if this is easer to do it internally or on github. ', b'> This will involve some script to automatically update the requirements.txt at a certain time everyday, and it need to make sure the corresponding tf-nightly does exist.\n\nYes this is exactly what I meant. Probably we could handle this with a Github action directly (or your copybara).']",open,2021-07-03 12:56:52,
https://api.github.com/repos/keras-team/keras/issues/14849,b'Add activation parameter to BatchNormalization',"[b'@eli-osherovich,\r\nCan you please provide more details and a use case where this Feature is applicable? Thanks!', b'@rmothukuru \r\nMost modern architectures use BN followed by an activation layer (typically ReLU). It would be nice (and more aligned with other layers, e.g., Dense, Conv) to have activation parameter directly within the BN layers. The code will also be more concise. ', b'@fchollet: leave this to Francois as API owner for further comment.']",open,2021-07-01 10:57:24,
https://api.github.com/repos/keras-team/keras/issues/14808,b'SaveModel fails after multiple roundtrips',"[b'@adriangb This way of `roundtrips` of saving and loading not supported as it could lead to some tracing issues as mentioned in the error. Just a question. Why do you need that kind of `roundtrips` and what is your use-case (if you can tell) that requires this kind of functionality? thanks!', b"">  what is your use-case\r\n\r\nWell I originally found this bug in #14748, where one of the tests essentially consists of `roundtrips` since we want to test that a model can be pickled, deepcopied, etc. So it's not a real world use case, but IMO it is quite arbitrary to not support it.\r\n\r\n> This way of roundtrips of saving and loading not supported\r\n\r\nI am confused as to why this is the case. It is not mentioned _anywhere_ in the docs, and it seems like a bug or design problem to me.\r\n"", b'@adriangb I may be wrong here `This way of roundtrips of saving and loading not supported`. \r\n\r\n@k-w-w Can you please take a look? Thanks', b""For what it's worth, I pushed a workaround to #14748, which also confirms the issue""]",open,2021-06-24 17:37:47,
https://api.github.com/repos/keras-team/keras/issues/14776,b'variables are not trackable in optimizer',"[b""@fsx950223,\r\nThe API, **`model.optimizer._create_hypers()`** couldn't be found in the [Keras Documentation](https://keras.io/search.html?query=model.optimizer._create_hypers). Can you please let us know the reference of that API in [Keras Documentation](https://keras.io/getting_started/)? Thanks!"", b""> @fsx950223,\r\n> The API, **`model.optimizer._create_hypers()`** couldn't be found in the [Keras Documentation](https://keras.io/search.html?query=model.optimizer._create_hypers). Can you please let us know the reference of that API in [Keras Documentation](https://keras.io/getting_started/)? Thanks!\r\n\r\nThe API doesn't have any documents. My question is how to track optimizer variables that have been created.\r\nThe feature is useful when I want to load variables from the checkpoint."", b'@haifeng-jin. Please update our documentation about optimizer v2 and why the slot variable are not tracked if possible.']",open,2021-06-22 09:24:53,
https://api.github.com/repos/keras-team/keras/issues/14670,b'DepthwiseConv3D',[],open,2021-05-26 09:35:24,
https://api.github.com/repos/keras-team/keras/issues/14656,b'Added the explanation that distinguishes `categorical_crossentropy` and `sparse_categorical_crossentropy`',[],open,2021-05-20 15:19:28,
https://api.github.com/repos/keras-team/keras/issues/14649,b'Wrong calculation of output value in GRU',[b'I agree with you :)'],open,2021-05-16 19:32:33,
https://api.github.com/repos/keras-team/keras/issues/14643,b'Tree LSTM in Keras',"[b""There's no one working on Tree LSTM in keras right now, but this would be a great spot for a contribution.\r\n\r\nThe place to start would probably be contributing this as an example on keras.io. Thanks!""]",open,2021-05-12 23:32:53,
https://api.github.com/repos/keras-team/keras/issues/14627,b'Why the loss function (mse) calculated by keras not the same as mine',"[b'I think the issue here is that the model output shape is [404, 1] and the label shape is [404]. The MSE produce the incorrect value since the label and prediction doesn\'t have the same shape. It didn\'t error out since the label can be broadcast to prediction\'s shape ([404, 404] in this case which is the probably the cause of the error).\r\n\r\nIf you add a line to expand the dim of the y_train like ""y_train = numpy.expand_dim(y_train, axis=1)"", the model.fit/eval and raw numpy calculation should all produce the same number.', b'I think this is tricky error and an easy pitfall for end user. We should either error out when there is a shape mismatch, or broadcast to same shape.']",open,2021-05-01 16:26:29,
https://api.github.com/repos/keras-team/keras/issues/14607,b'dropout rate in dense layer',"[b'https://jmlr.org/papers/v15/srivastava14a.html\r\n\r\nBased on the original paper proposing dropout. Keep dropout between 0.2 < Dropout < 0.5.  As you can see when you used 0.9 as your dropout rate, there was over fitting occurring which is not good and is the opposite intent of the use of dropout. When you have 0.5 you have your accuracy oscillating and that might be due two reasons. The first reason being that your learning rate needs is too high and needs to lowered and the other being that your batch size is too small, thus needs to be increased.', b'  @saichatla I tried different dropout rates and here are the results. I used 64 as batch size and 0.01 as learning rate\r\n\r\nusing dropout 0.4 \r\n\r\n<img width=""839"" alt=""0 4"" src=""https://user-images.githubusercontent.com/34564454/116552053-0991fd00-a912-11eb-801a-cd66c7420513.PNG"">\r\n\r\nusing dropout 0.5\r\n<img width=""948"" alt=""0 5"" src=""https://user-images.githubusercontent.com/34564454/116552093-16aeec00-a912-11eb-881d-24ad957c93ab.PNG"">\r\n\r\nusing dropout 0.6\r\n<img width=""895"" alt=""0 6"" src=""https://user-images.githubusercontent.com/34564454/116552131-229aae00-a912-11eb-9a0e-d43f228b18e7.PNG"">\r\n\r\nusing dropout 0.7\r\n<img width=""896"" alt=""0 7"" src=""https://user-images.githubusercontent.com/34564454/116552171-2cbcac80-a912-11eb-860a-dcebfb9f4032.PNG"">\r\n\r\nin all these cases the val_accuracy stagates at 50%. I read somewhere that if validation set does not have enough data than this might occur. but we are talking about 3k+ different classes folders with 200+ samples in each folder (UCF-101 Dataset)\r\n\r\nAnother reason I read for this is that there might not be enough randomness in the validation set, which is also not the case.\r\n\r\nWhat can be the potential reason for that? \r\nThank you.', b'Could you rerun these tests by lowering the learning rate to something small like .00001 and increasing the batch size. Thank you!', b""Yes, I have run my model on 0.01 now. Sorry it was 0.1 previously.\r\nMore over I'm using batch size 128 for 50 classes only, because i cant fix batch size more than 64 for 101 classes due to memory limitation. \r\nI will get back to you when I get the results.\r\nThank You!"", b'@saichatla I trained my model this time on 25 classes only to get an idea of how it will work for 101 classes. I used learning rate of 0.0001 while keeping batch size of 128 and a dropout of 0.5 after both of the dense layers . The validation accuracy again stagnates at 60%.\r\n![Capture](https://user-images.githubusercontent.com/34564454/116660308-47dbfa80-a9ac-11eb-803f-f3fc11895deb.PNG)\r\n']",open,2021-04-20 20:34:20,
https://api.github.com/repos/keras-team/keras/issues/14580,b'WGAN-GP Keras tutorial is slow to start the first epoch #48356',"[b'@AakashKumarNain who is the original author the notebook. https://github.com/keras-team/keras-io/pull/15. ', b'This is weird. Although there is nothing special about the model here, so model is definitely not the problem. The only thing that can be a ""bit"" slow is the `gradient_penalty()` but I doubt that would add so much of training time.\r\n\r\nUpdate: I just tried running the model in Colab and the first epoch took around 35 sec to start which IMO isn\'t unusual.', b""No worries, I'd be glad to have this fixed as I've been tinkering a lot more with custom architectures with explicite gradient tapes and this is a bit of a paper-cut. I'm happy to further debug if you need me check anything."", b'Can you try using the TF debugger and see which op is taking time?', b'@romanovzky did you try using the TF debugger?', b""Hi, I'm sorry but I have a lot on my hands right now and my current \non-going projects do not involve these type of architectures. Might be \nable to pick it up again, but not in the next few weeks.\n\nOn 07/09/2021 07:39, Aakash Kumar Nain wrote:\n>\n> @romanovzky <https://github.com/romanovzky> did you try using the TF \n> debugger?\n>\n> \xe2\x80\x94\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub \n> <https://github.com/keras-team/keras/issues/14580#issuecomment-914033250>, \n> or unsubscribe \n> <https://github.com/notifications/unsubscribe-auth/AB3O6K6BXPYMSTB3CJX446TUAWXRHANCNFSM42QKSUOA>.\n> Triage notifications on the go with GitHub Mobile for iOS \n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> \n> or Android \n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>. \n>\n>\n\n"", b'No worries.', b'> @romanovzky did you try using the TF debugger?\r\n\r\nHi, I have a similar problem with my model. How would I use the TF debugger to see what is taking all the time in the first epoch?']",open,2021-04-07 08:41:20,
https://api.github.com/repos/keras-team/keras/issues/14573,b'Worse  performance  with multiple GPU  training with Keras',"[b""This can be due to: the reduction strategy that you specified (https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce). It's quite tricky to determine how to fuse the results of the GPUs sometimes.\r\nSharing your code can help us figuring out."", b""> This can be due to: the reduction strategy that you specified (https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce). It's quite tricky to determine how to fuse the results of the GPUs sometimes.\r\n> Sharing your code can help us figuring out.\r\n\r\n`from` keras.utils import multi_gpu_model\r\n\r\nself.model=multi_gpu_model(self.model,gpus=self.gpu_counts)\r\nself.model.compile(loss=categorical_crossentropy,\r\n                             optimizer=Adam(lr=self.lr),\r\n                             metrics=['acc', `categorical_crossentropy])`\r\n\r\n"", b'Which version of tensorflow/keras are you using ?\r\nI cannot find `keras.utils.multi_gpu_model` in the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/utils).', b'Keras  version  is  2.3.1 \xef\xbc\x8cwith python =3.6.6\xef\xbc\x8c tensorflow-gpu=1.14.0   on Ubuntu  18.04', b""Can you re-run the training with `self.model=multi_gpu_model(self.model,gpus=self.gpu_counts, cpu_relocation=True)` please. Maybe upgrading the model on CPU can overcome the problem.\r\n\r\nhttps://github.com/tensorflow/docs/blob/r1.14/site/en/api_docs/python/tf/keras/utils/multi_gpu_model.md\r\n`cpu_relocation: A boolean value to identify whether to create the model's weights under the scope of the CPU. If the model is not defined under any preceding device scope, you can still rescue it by activating this option.`"", b'I have tried the resolution ,but there is  no improvement. The metrics such as  loss and acc  are still  same as the description  firstly']",open,2021-04-02 06:40:45,
https://api.github.com/repos/keras-team/keras/issues/14496,"b'Model does not converge (Loss does not decrease, accuracy does not change)'",[],open,2021-03-03 17:10:57,
https://api.github.com/repos/keras-team/keras/issues/14388,b'How does backprop work on a shared layer used twice in the same model?',"[b""![image](https://user-images.githubusercontent.com/36856589/109801713-dea66800-7c44-11eb-9fc1-0d070b628ca8.png)\r\n\r\nAs far as I understand the architecture the backpropagation should go in this way:\r\n- Dense\r\n- Dropout\r\n- Flatten\r\n- MaxPooling\r\n- Diffract: This is the first time this is going to get updated. Let's say it is updated to Diffract1.\r\n- Conv2D\r\n- MaxPooling\r\n- Diffract: This is the second time this is going to get updated. The changes will be based on top of Diffract1.\r\n- Conv2d\r\n\r\nA simple experiment would be to manually calculate the gradients with `tf.gradients` and see what changes when. \xf0\x9f\x98\x84 ""]",open,2021-01-21 10:41:40,
https://api.github.com/repos/keras-team/keras/issues/14386,b'Model compression problem',"[b'Hey @ewwll could you share a colab instead of the what you have given here! Right now it is not readable or executable. After you share the code, I can reproduce and check for any issues.', b'#!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\nimport sys\r\nimport os\r\nimport cv2 \r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom keras.callbacks import ModelCheckpoint\r\nimport matplotlib.pyplot as plt\r\nfrom keras.models import *\r\nfrom keras.layers import *\r\nfrom keras.optimizers import *\r\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\r\nfrom keras import backend as keras\r\nfrom keras import applications\r\ntrainname=os.listdir(""./camera/image/"")\r\ntestname=os.listdir(""./camera/label/"")\r\nprint(trainname)\r\nprint(len(trainname))\r\nprint(testname)\r\nprint(len(testname))\r\ntrain=[]\r\ntest=[]\r\ntrain_label=[]\r\ntest_label=[]\r\nn=0\r\nfor i in trainname:\r\n    if(n<200):\r\n        img=cv2.imread(""./camera/image/""+i)\r\n        img=cv2.resize(img,(252,189))\r\n        img=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\r\n        img=img/255\r\n        train.append(img)\r\n        j=0\r\n        num=[]\r\n        while(j<6):\r\n            img1=cv2.imread(""./camera/label/""+str(n)+\'_\'+str(j)+"".jpg"")\r\n            img1=cv2.resize(img1,(252,180))\r\n            img1=cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY)\r\n            img1=img1/255\r\n            img1=img1.transpose()\r\n            num.append(img1)\r\n            j=j+1\r\n        num=np.array(num)\r\n        num=num.transpose()\r\n        train_label.append(num)\r\n    else:\r\n        img=cv2.imread(""./camera/image/""+i)\r\n        img=cv2.resize(img,(252,189))\r\n        img=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\r\n        img=img/255\r\n        test.append(img)\r\n        j=0\r\n        num=[]\r\n        while(j<6):\r\n            img1=cv2.imread(""./camera/label/""+str(n)+\'_\'+str(j)+"".jpg"")\r\n            img1=cv2.resize(img1,(252,180))\r\n            img1=cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY)\r\n            img1=img1/255\r\n            img1=img1.transpose()\r\n            num.append(img1)\r\n            j=j+1\r\n        print(len(num))\r\n        num=np.array(num)\r\n        num=num.transpose()\r\n        print(num.shape)\r\n        test_label.append(num)\r\n    n=n+1\r\nplt.imshow(train[0])\r\nplt.show()\r\nplt.imshow(test[0])\r\nplt.show()\r\nprint(train_label[0].shape)\r\nproject_name = ""fcn_segment""\r\nchannels = 1\r\nstd_shape = (189, 252, channels)\r\nmodel = Sequential(name = project_name)\r\n# model.add(Conv2D(32, kernel_size = (3, 3), activation = ""relu"",padding = ""same"", input_shape = std_shape,name = ""conv_1""))\r\n# model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2), name = ""max_pool_1""))\r\n# model.add(Dropout(0.25))\r\n# model.add(Conv2D(64, kernel_size = (3, 3), activation = ""relu"",padding = ""same"", name = ""conv_2""))\r\n# model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2), name = ""max_pool_2""))\r\n# model.add(Dropout(0.25))\r\n# model.add(Conv2D(128, kernel_size = (3, 3), activation = ""relu"",padding = ""same"", name = ""conv_3""))\r\n# model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2), name = ""max_pool_3""))\r\n# model.add(Dropout(0.25))\r\n# model.add(Conv2D(256, kernel_size = (3, 3), activation = ""relu"",padding = ""same"", name = ""conv_4""))\r\n# model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2), name = ""max_pool_4""))\r\n# model.add(Dropout(0.25))\r\n# model.add(Conv2D(512, kernel_size = (3, 3), activation = ""relu"",padding = ""same"", name = ""conv_5""))\r\n# model.add(MaxPool2D(pool_size = (2, 2), strides = (2, 2), name = ""max_pool_5""))\r\n# model.add(Dropout(0.25))\r\nmodel.add(applications.VGG19(input_shape=std_shape,include_top=None,weights=None))\r\nmodel.add(BatchNormalization())\r\nmodel.add(UpSampling2D(size = (6, 6), interpolation = ""nearest"", name = ""upsamping_6""))\r\nmodel.add(Conv2D(3, kernel_size = (5, 5), activation = ""sigmoid"",padding = ""same"", name = ""conv_6""))\r\nmodel.add(UpSampling2D(size = (6, 6), interpolation = ""nearest"", name = ""upsamping_7""))\r\nmodel.add(Conv2D(6, kernel_size = (3, 3), activation = ""sigmoid"",padding = ""same"", name = ""conv_7""))\r\nmodel.compile(optimizer = ""adam"",loss = ""mean_squared_error"",metrics = [""accuracy""])\r\nmodel.summary()\r\ntrain=np.array(train)\r\ntest=np.array(test)\r\ntrain_label=np.array(train_label)\r\ntest_label=np.array(test_label)\r\nprint(train.shape)\r\nprint(test.shape)\r\nprint(train_label.shape)\r\nprint(test_label.shape)\r\ntrain=np.expand_dims(train,axis=3)\r\ntest=np.expand_dims(test,axis=3)\r\nprint(train.shape)\r\nprint(test.shape)\r\nfilepath = ""best_weights.h5""\r\ncheckpoint = ModelCheckpoint(filepath, monitor=\'accuracy\', verbose=0, save_best_only=True, mode=\'max\')\r\ncallbacks_list = [checkpoint]\r\nhistory = model.fit(train, train_label,batch_size =6,epochs = 40, shuffle=True,verbose = 1,validation_data=(test,test_label),callbacks = callbacks_list)\r\nacc = history.history[\'accuracy\']\r\nloss = history.history[\'loss\']\r\nepochs = range(1, len(acc) + 1)\r\nplt.title(\'Accuracy and Loss\')\r\nplt.plot(epochs, acc, \'red\', label=\'Training acc\')\r\nplt.plot(epochs, loss, \'blue\', label=\'Validation loss\')\r\nplt.legend()\r\nplt.show()\r\n']",open,2021-01-21 08:10:53,
https://api.github.com/repos/keras-team/keras/issues/14373,b'EfficientNet transfer learning and greyscale images',[],open,2021-01-16 16:57:34,
https://api.github.com/repos/keras-team/keras/issues/14345,"b""load_model - the two structures don't have the same nested structure""","[b""I'm having the same issue with TF 2.4.1 and Transformers 4.2.2.\r\n\r\nFirst structure also have shape=(None, 5) while second structure is shape=(None, maximum length of a sequence).\r\n\r\nEdit: \r\n\r\nWorkaround for BERT: Save your model as .h5 instead of .tf. I didn't test this for other but it might worth a try."", b""Same issue with TF 2.5.0-rc3 and Transformer 4.5.1. \r\n\r\nUsing out of the box Bert classifier:\r\n`model = TFBertForSequenceClassification.from_pretrained(bert-base-multilingual-uncased',num_labels=2)`\r\nthen traning the model and then trying to save it give me the same error\r\n```\r\nTypeError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name='input_ids/input_ids')}\r\n\r\nSecond structure: type=list str=[TensorSpec(shape=(None, 128), dtype=tf.int32, name='input_ids/0')]\r\n```\r\nsaving the mode .hd5 results in the same error message we reloading the model:\r\n`model.save('bert_input.hd5')`\r\n\r\n"", b""Same issue with `tensorflow 2.5.0-rc3`\r\nUsing Huggingface save/load is working fine\r\n```\r\nmodel.save_pretrained('bert_input')\r\nreconstructed_model = TFBertForSequenceClassification.from_pretrained('bert_input')\r\n```\r\n\r\nBut this is not we need for tf.serving.\r\n\r\nIt seems some class/function definition are not saved with the model so reloading the model will not work. Maybe there are some paramters to set to have this working ? ([Stackoverflow](https://stackoverflow.com/questions/67391794/saved-model-after-reloading-with-tf-keras-models-load-model-request-different-in))"", b'Same issue ', b'Same issue with transformers 4.6.1 and tensorflow 2.4.1 \r\n\r\nI built the model as the following. \r\n```\r\nModel: ""model_1""\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_4 (InputLayer)            [(None, 512)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_3 (InputLayer)            [(None, 512)]        0                                            \r\n__________________________________________________________________________________________________\r\ntf_distil_bert_model_1 (TFDisti TFBaseModelOutput(la 66362880    input_4[0][0]                    \r\n                                                                 input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\nbidirectional_1 (Bidirectional) (None, 128)          320256      tf_distil_bert_model_1[1][7]     \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 16)           2064        bidirectional_1[0][0]            \r\n__________________________________________________________________________________________________\r\ndropout_58 (Dropout)            (None, 16)           0           dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 10)           170         dropout_58[0][0]                 \r\n==================================================================================================\r\nTotal params: 66,685,370\r\nTrainable params: 322,490\r\nNon-trainable params: 66,362,880\r\n__________________________________________________________________________________________________\r\n```\r\nThen, it\'s trained and save as SavedModel. But when load it back, it\'s an error.\r\n```\r\nThe two structures don\'t have the same nested structure.\r\n\r\nFirst structure: type=dict str={\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'input_ids/input_ids\')}\r\n\r\nSecond structure: type=dict str={\'input_ids\': TensorSpec(shape=(None, 512), dtype=tf.int32, name=\'input_ids/input_ids\'), \'attention_mask\': TensorSpec(shape=(None, 512), dtype=tf.int32, name=\'input_ids/attention_mask\')}\r\n\r\nMore specifically: The two dictionaries don\'t have the same set of keys. First structure has keys type=list str=[\'input_ids\'], while second structure has keys type=list str=[\'input_ids\', \'attention_mask\']\r\nEntire first structure:\r\n{\'input_ids\': .}\r\nEntire second structure:\r\n{\'input_ids\': ., \'attention_mask\': .}\r\n```', b'I finally solved this issue by using `tf.saved_model.save(model, path_to_dir)` instead of `model.save(path_to_dir)` or `tf.keras.models.save_model(model, path_to_dir)`.\r\n\r\nfollowed by this guide: https://www.tensorflow.org/guide/saved_model', b'@eyalshafran Can you share a simple standalone code to reproduce the issue? Did you try recent TF/Keras versions? Thanks!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'Same issue', b""I am also facing the same issue\r\n![image](https://user-images.githubusercontent.com/82885255/135749607-87bf9987-d4c9-49e6-84d1-897d72fd9eb8.png)\r\n\r\nand after applying saved_model instead of save.... again this error appeared \r\ntf.saved_model.save(intent_model,'/content/model')\r\n\r\n![image](https://user-images.githubusercontent.com/82885255/135749630-9e377181-fcaf-48a3-81fb-f3e72a139c80.png)\r\n\r\nDo tell me about this if someone has solved this issue.\r\n"", b'Is it possible for anyone of you to share a simple standalone code to reproduce the issue? Thanks!', b'Just use the post\'s codes and I can reproduce it (`tf.__version__`: `2.2.0`):\r\n``` Python\r\nimport tensorflow as tf\r\nfrom transformers import TFRobertaModel\r\n\r\n\r\nroberta_layer = TFRobertaModel.from_pretrained(\'roberta-base\')\r\n\r\nids = tf.keras.layers.Input((64,), dtype=tf.int32)\r\natt = tf.keras.layers.Input((64,), dtype=tf.int32)\r\n\r\nroberta_inputs = [ids, att]\r\n\r\nsequence_output,pooled_output = roberta_layer(roberta_inputs)\r\n\r\n# unigram\r\nx1 = tf.keras.layers.Conv1D(32,1,activation=\'relu\')(sequence_output)\r\nx1 = tf.keras.layers.GlobalMaxPool1D()(x1)\r\n\r\n# bigram\r\nx2 = tf.keras.layers.Conv1D(32,2,activation=\'relu\')(sequence_output)\r\nx2 = tf.keras.layers.GlobalMaxPool1D()(x2)\r\n\r\n# trigram\r\nx3 = tf.keras.layers.Conv1D(32,3,activation=\'relu\')(sequence_output)\r\nx3 = tf.keras.layers.GlobalMaxPool1D()(x3)\r\n\r\nconcat = tf.keras.layers.Concatenate()([x1,x2,x3])\r\nconcat = tf.keras.layers.Dropout(0.5)(concat)\r\n\r\noutputs = tf.keras.layers.Dense(11, activation=\'sigmoid\')(concat)\r\n\r\nmodel = tf.keras.Model(inputs=roberta_inputs, outputs=outputs)\r\n```\r\n\r\nThen save the model\r\n``` Python\r\nmodel.save(\'_tmp_model\')\r\n```\r\n\r\nThen the following load will produce the error\r\n``` Python\r\nrestored_model = tf.keras.models.load_model(""_tmp_model"")\r\n```\r\n\r\nBut if I load by `tf.saved_model.load` instead of `tf.keras.models.load_model`, it will succeed\r\n``` Python\r\nrestored_model = tf.saved_model.load(""_tmp_model"")\r\n```\r\n\r\nAlso tried saving with `tf.keras.models.save_model` instead of `model.save`, the same error still exists\r\n``` Python\r\ntf.keras.models.save_model(model, \'_tmp_model_keras\')\r\nrestored_model = tf.keras.models.load_model(\'_tmp_model_keras\')\r\n``` \r\n\r\n---\r\n\r\nI also tried tf 2.6.0, still the same.\r\nHere\'s the error it will produce:\r\n``` Python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n    527     _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\r\n--> 528                                       expand_composites)\r\n    529   except (ValueError, TypeError) as e:\r\n\r\nTypeError: The two structures don\'t have the same nested structure.\r\n\r\nFirst structure: type=tuple str=(({\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'inputs/input_ids\')},), {\'training\': False})\r\n\r\nSecond structure: type=tuple str=(([TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/0\'), TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/1\')],), {\'training\': False})\r\n\r\nMore specifically: The two namedtuples don\'t have the same sequence type. First structure type=dict str={\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'inputs/input_ids\')} has type dict, while second structure type=list str=[TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/0\'), TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/1\')] has type list\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/tmp/ipykernel_25463/2006687310.py in <module>\r\n----> 1 restored_model = tf.keras.models.load_model(""_tmp_model"")\r\n\r\n/anaconda/lib/python3.7/site-packages/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)\r\n    203         filepath = path_to_string(filepath)\r\n    204         if isinstance(filepath, str):\r\n--> 205           return saved_model_load.load(filepath, compile, options)\r\n    206 \r\n    207   raise IOError(\r\n\r\n/anaconda/lib/python3.7/site-packages/keras/saving/saved_model/load.py in load(path, compile, options)\r\n    141 \r\n    142   # Finalize the loaded layers and remove the extra tracked dependencies.\r\n--> 143   keras_loader.finalize_objects()\r\n    144   keras_loader.del_tracking()\r\n    145 \r\n\r\n/anaconda/lib/python3.7/site-packages/keras/saving/saved_model/load.py in finalize_objects(self)\r\n    638         layers_revived_from_config.append(node)\r\n    639 \r\n--> 640     _finalize_saved_model_layers(layers_revived_from_saved_model)\r\n    641     _finalize_config_layers(layers_revived_from_config)\r\n    642 \r\n\r\n/anaconda/lib/python3.7/site-packages/keras/saving/saved_model/load.py in _finalize_saved_model_layers(layers)\r\n    835           continue\r\n    836         if call_fn.input_signature is None:\r\n--> 837           args, kwargs = infer_inputs_from_restored_call_function(call_fn)\r\n    838           args = list(args)\r\n    839           inputs = args.pop(0)\r\n\r\n/anaconda/lib/python3.7/site-packages/keras/saving/saved_model/load.py in infer_inputs_from_restored_call_function(fn)\r\n   1172   for concrete in fn.concrete_functions[1:]:\r\n   1173     spec2 = concrete.structured_input_signature\r\n-> 1174     spec = tf.nest.map_structure(common_spec, spec, spec2)\r\n   1175   return spec\r\n   1176 \r\n\r\n/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    861   for other in structure[1:]:\r\n    862     assert_same_structure(structure[0], other, check_types=check_types,\r\n--> 863                           expand_composites=expand_composites)\r\n    864 \r\n    865   flat_structure = (flatten(s, expand_composites) for s in structure)\r\n\r\n/anaconda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n    533                   ""Entire first structure:\\n%s\\n""\r\n    534                   ""Entire second structure:\\n%s""\r\n--> 535                   % (str(e), str1, str2))\r\n    536 \r\n    537 \r\n\r\nTypeError: The two structures don\'t have the same nested structure.\r\n\r\nFirst structure: type=tuple str=(({\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'inputs/input_ids\')},), {\'training\': False})\r\n\r\nSecond structure: type=tuple str=(([TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/0\'), TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/1\')],), {\'training\': False})\r\n\r\nMore specifically: The two namedtuples don\'t have the same sequence type. First structure type=dict str={\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'inputs/input_ids\')} has type dict, while second structure type=list str=[TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/0\'), TensorSpec(shape=(None, 64), dtype=tf.int32, name=\'inputs/1\')] has type list\r\nEntire first structure:\r\n(({\'input_ids\': .},), {\'training\': .})\r\nEntire second structure:\r\n(([., .],), {\'training\': .})\r\n```\r\n', b'@Nash2325138 I am facing different error. Can you please check this [gist](https://colab.research.google.com/gist/jvishnuvardhan/4073e69e08dd8ad2c4e603e27ff0fc71/untitled.ipynb) and let us know what version you are using. Thanks!', b'A solution is to call the roberta model slightly different:\r\n`sequence_output,pooled_output = roberta_layer.roberta(roberta_inputs)`\r\n\r\nThis fixes the problem', b'@Nash2325138 I am not familiar with this `roberta_layer`. As you suggested, I updated the above line. Now it is throwing another error. [Here](https://colab.research.google.com/gist/jvishnuvardhan/9cdcdb5520a5d57c921053160991d7ce/untitled.ipynb) is a gist for reference. Thanks!', b'Sorry that I forgot to give my `transformers` version. It\'s `2.8.0`, which is a bit old and might be an issue.\r\nThe full screenshots:\r\n![image](https://user-images.githubusercontent.com/17662095/136346176-36f11d57-a586-4a14-9b29-ec561f725610.png)\r\n![image](https://user-images.githubusercontent.com/17662095/136346227-1fa1873e-bda2-4383-b3f4-5f2c2ef4e36e.png)\r\n![image](https://user-images.githubusercontent.com/17662095/136346265-36566930-3b51-435e-9d19-b4121607856e.png)\r\n\r\nThe input codes are:\r\n``` Python \r\nimport tensorflow as tf\r\nimport transformers\r\nfrom transformers import TFRobertaModel\r\n\r\n\r\nprint(tf.__version__)\r\nprint(transformers.__version__)\r\n\r\n\r\nroberta_layer = TFRobertaModel.from_pretrained(\'roberta-base\')\r\n\r\nids = tf.keras.layers.Input((64,), dtype=tf.int32)\r\natt = tf.keras.layers.Input((64,), dtype=tf.int32)\r\n\r\nroberta_inputs = [ids, att]\r\n\r\nsequence_output,pooled_output = roberta_layer(roberta_inputs)\r\n\r\n# unigram\r\nx1 = tf.keras.layers.Conv1D(32,1,activation=\'relu\')(sequence_output)\r\nx1 = tf.keras.layers.GlobalMaxPool1D()(x1)\r\n\r\n# bigram\r\nx2 = tf.keras.layers.Conv1D(32,2,activation=\'relu\')(sequence_output)\r\nx2 = tf.keras.layers.GlobalMaxPool1D()(x2)\r\n\r\n# trigram\r\nx3 = tf.keras.layers.Conv1D(32,3,activation=\'relu\')(sequence_output)\r\nx3 = tf.keras.layers.GlobalMaxPool1D()(x3)\r\n\r\nconcat = tf.keras.layers.Concatenate()([x1,x2,x3])\r\nconcat = tf.keras.layers.Dropout(0.5)(concat)\r\n\r\noutputs = tf.keras.layers.Dense(11, activation=\'sigmoid\')(concat)\r\n\r\nmodel = tf.keras.Model(inputs=roberta_inputs, outputs=outputs)\r\n\r\nmodel.save(\'_tmp_model\')\r\nrestored_model = tf.keras.models.load_model(""_tmp_model"")\r\n```\r\n', b""Update: if I change\r\n```\r\nsequence_output,pooled_output = roberta_layer(roberta_inputs)\r\n```\r\nto \r\n```\r\nsequence_output,pooled_output = roberta_layer.roberta(roberta_inputs)\r\n```\r\nas @eyalshafran suggested, the problem is magically solved.\r\nI wonder what's the cause"", b""Hmm... Any follow up about this issue? Though  @eyalshafran's suggestion solved this case. I still don't understand why the original one will fail"", b'Even I am facing the same issue.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-10-3a1d333d606b> in <module>()\r\n----> 1 ner_model_2 = pickle.load(open(\'ner_model_2.pkl\', \'rb\'))\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n    533                   ""Entire first structure:\\n%s\\n""\r\n    534                   ""Entire second structure:\\n%s""\r\n--> 535                   % (str(e), str1, str2))\r\n    536 \r\n    537 \r\n\r\nValueError: The two structures don\'t have the same nested structure.\r\n\r\nFirst structure: type=tuple str=(({\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'input_ids/input_ids\')}, None, None, None, None, None, None, None, None, None, None, None, None, False), {})\r\n\r\nSecond structure: type=tuple str=((TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'input_ids\'), TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'attention_mask\'), TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'token_type_ids\'), None, None, None, None, None, None, None, None, None, None, False), {})\r\n\r\nMore specifically: Substructure ""type=dict str={\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'input_ids/input_ids\')}"" is a sequence, while substructure ""type=TensorSpec str=TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'input_ids\')"" is not\r\nEntire first structure:\r\n(({\'input_ids\': .}, ., ., ., ., ., ., ., ., ., ., ., ., .), {})\r\nEntire second structure:\r\n((., ., ., ., ., ., ., ., ., ., ., ., ., .), {})\r\n```', b'> Even I am facing the same issue.\r\n> \r\n> ```\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-10-3a1d333d606b> in <module>()\r\n> ----> 1 ner_model_2 = pickle.load(open(\'ner_model_2.pkl\', \'rb\'))\r\n> \r\n> 2 frames\r\n> /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n>     533                   ""Entire first structure:\\n%s\\n""\r\n>     534                   ""Entire second structure:\\n%s""\r\n> --> 535                   % (str(e), str1, str2))\r\n>     536 \r\n>     537 \r\n> \r\n> ValueError: The two structures don\'t have the same nested structure.\r\n> \r\n> First structure: type=tuple str=(({\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'input_ids/input_ids\')}, None, None, None, None, None, None, None, None, None, None, None, None, False), {})\r\n> \r\n> Second structure: type=tuple str=((TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'input_ids\'), TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'attention_mask\'), TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'token_type_ids\'), None, None, None, None, None, None, None, None, None, None, False), {})\r\n> \r\n> More specifically: Substructure ""type=dict str={\'input_ids\': TensorSpec(shape=(None, 5), dtype=tf.int32, name=\'input_ids/input_ids\')}"" is a sequence, while substructure ""type=TensorSpec str=TensorSpec(shape=(None, 384), dtype=tf.int32, name=\'input_ids\')"" is not\r\n> Entire first structure:\r\n> (({\'input_ids\': .}, ., ., ., ., ., ., ., ., ., ., ., ., .), {})\r\n> Entire second structure:\r\n> ((., ., ., ., ., ., ., ., ., ., ., ., ., .), {})\r\n> ```\r\n\r\nI have figured out a way to overcome this problem. Instead of saving the model using `model.save` or `tf.keras.models.save_model()`, try saving the model weights using `model.save_weights(\'model_weight.h5\')` during the training process.\r\n\r\nAt the prediction or testing phase, you need to create the model (architecture) manually, the same way you did in the training phase, and then load the weights to it.', b""Yes, as introduced by shreyas-jk,  model.save_weights('model_weight.h5') can deal with this problem.\r\nMy model contains a huggingface bert. It cannot be saved.\r\nHowever, I can save the weights. Subsequently, I build the model again in other files and load the weights.\r\n"", b'Same issue.\r\ntf 2.7.0\r\ntransformers 4.14.1\r\npretrained model: bert-base-cased', b'tensorflow 2.7.0\r\nkeras 2.7.0\r\ntransformers 4.15.0\r\npretrained model: ""bert-base-multilingual-cased""\r\n\r\nI also had the same issue when using model.save() & keras.model.load_model().\r\nbut, I change my python code to model.save_weights() & model.load_weights(), this issue resolved.\r\nthanks to @Allen-Qiu  & @shreyas-jk ']",open,2021-01-04 20:47:49,
https://api.github.com/repos/keras-team/keras/issues/14338,b'masking with batch size > 1 - how does this work?',[b'Any update on that?'],open,2020-12-31 06:23:33,
https://api.github.com/repos/keras-team/keras/issues/14329,b'Changing channel orientation of trained model',[],open,2020-12-25 12:10:38,
https://api.github.com/repos/keras-team/keras/issues/14321,b'[Feature request] - Load weights locally on applications',"[b'model=Sequential()\r\nfrom tensorflow.keras.applications.vgg16 import VGG16\r\nbase_model =  VGG16(input_shape = (224, 224, 3), include_top = False, weights = \'imagenet\')\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\nx = Flatten()(base_model.output)\r\nx = Dense(1024, activation=\'relu\')(x)\r\nx = Dropout(0.5)(x)\r\nx = Dense(3, activation=\'softmax\')(x)\r\nmodel = Model(base_model.input, x)\r\nmodel.compile(optimizer = ""adam"", loss = \'categorical_crossentropy\',metrics = [""accuracy""])\r\nmodel.summary()\r\n\r\nModel: ""model_1""\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 224, 224, 3)]     0         \r\n_________________________________________________________________\r\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \r\n_________________________________________________________________\r\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \r\n_________________________________________________________________\r\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \r\n_________________________________________________________________\r\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \r\n_________________________________________________________________\r\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \r\n_________________________________________________________________\r\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \r\n_________________________________________________________________\r\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \r\n_________________________________________________________________\r\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \r\n_________________________________________________________________\r\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \r\n_________________________________________________________________\r\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0\r\n\r\n']",open,2020-12-21 10:59:18,
https://api.github.com/repos/keras-team/keras/issues/14317,b'Feature request - testing_split in ImageDataGenerator',"[b'Is there an indirect way of splitting train, test, and validation?']",open,2020-12-19 16:57:44,
https://api.github.com/repos/keras-team/keras/issues/14315,b'How to slice a tensor in keras with variable indices?',"[b'Is it a problem related to broadcasting during matrix multiplication', b""No. I think the problem is that the input defined in keras in this form `index1 = Input(shape=(1,), name='ind1', dtype = 'int32')` cannot be used as index because its shape is (None,1). But I need it in this format because the index will change for every sample in the batch. Hence I want to give the index as numpy array during training and testing."", b'@ShivamP1993 Sorry for the late response! \r\nThe normal way to go is to pad your inputs to the same maximum length before batching.Please refer to this [tensor slicing  ](https://www.tensorflow.org/guide/tensor_slicing) and  try with latest TF v2.7.0 .Please let us know if it helps?Thanks!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b'> @ShivamP1993 Sorry for the late response! The normal way to go is to pad your inputs to the same maximum length before batching.Please refer to this [tensor slicing ](https://www.tensorflow.org/guide/tensor_slicing) and try with latest TF v2.7.0 .Please let us know if it helps?Thanks!\r\n\r\nThis method is about general slicing of tensors. The problem comes when I want to use the indices for slicing as input in a Keras based model (using ""Input"" layer) and use that ""Input"" for slicing.']",open,2020-12-18 18:50:54,
https://api.github.com/repos/keras-team/keras/issues/14288,b'Keras configuration json and application model (cache) files folder needs to be parsed correctly on Windows in a worker process (such as IIS WSGI)',[],open,2020-12-06 15:24:17,
https://api.github.com/repos/keras-team/keras/issues/14273,"b'ValueError: Error when checking input: expected lstm_47_input to have shape (128, 1) but got array with shape (1, 11)'","[b'Hello Aurtus, \r\n\r\nBy features, do you intent to say timesteps? Otherwise, I am not sure why are you using LSTM for just one time step. (I may be completely wrong in understanding your problem)', b'Hello Thank You for Your reply. \r\nEach row is daily reading from 11 sensor. based on historical data I want forecast readings from next couple measurements.']",open,2020-11-13 11:34:45,
https://api.github.com/repos/keras-team/keras/issues/14247,b'Can I use the VAE code in the example if I want a higher dimenional latent vector?',[],open,2020-10-15 14:52:08,
https://api.github.com/repos/keras-team/keras/issues/14243,b'run K.ctc_decode() decode on GPU',[],open,2020-10-12 07:09:49,
https://api.github.com/repos/keras-team/keras/issues/14232,b'New Update functional-api-guide.md',[],open,2020-10-04 04:51:34,
https://api.github.com/repos/keras-team/keras/issues/14230,"b'ValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build'",[],open,2020-10-01 13:51:46,
https://api.github.com/repos/keras-team/keras/issues/14225,b'how to use TextVectorization to vectorize Chinese text ',"[b'\r\n> In Chinese text, there is no whitespace between words, so when I use TextVectorization.adapt(train_dataset), I can only get Sentence-level vocabulary. the code I used is https://keras.io/examples/nlp/text_classification_from_scratch/\r\n\r\nJieba library in python is built for chinese word segmentation.  You can install ` pip install jieba` .\r\n[refer](https://github.com/fxsjy/jieba) this for more information on how to use jieba and the methods .\r\n', b""> > In Chinese text, there is no whitespace between words, so when I use TextVectorization.adapt(train_dataset), I can only get Sentence-level vocabulary. the code I used is https://keras.io/examples/nlp/text_classification_from_scratch/\r\n> \r\n> Jieba library in python is built for chinese word segmentation. You can install ` pip install jieba` .\r\n> [refer](https://github.com/fxsjy/jieba) this for more information on how to use jieba and the methods .\r\n\r\nwhich function's parameter can pass the jieba.cut() directly\xef\xbc\x9fcustom_standardization\xef\xbc\x9fIn this function,  I can only insert space between words,  then return a new string. Is these a function's parameter can accept the jieba.cut() directly?"", b""> > In Chinese text, there is no whitespace between words, so when I use TextVectorization.adapt(train_dataset), I can only get Sentence-level vocabulary. the code I used is https://keras.io/examples/nlp/text_classification_from_scratch/\r\n> \r\n> Jieba library in python is built for chinese word segmentation. You can install ` pip install jieba` .\r\n> [refer](https://github.com/fxsjy/jieba) this for more information on how to use jieba and the methods .\r\n\r\nwhich function's parameter can pass the jieba.segment()\xef\xbc\x9fcustom_standardization\xef\xbc\x9f"", b'@cherry247 Until now, as far as I know\xef\xbc\x8cthere is no library support tensor data segment, may be HanLP2.x support.I want to use the parameter \'split\' to segment text. the demo code like this:\r\n```\r\n# coding:utf-8\r\n""""""\r\n\r\n""""""\r\nimport tensorflow as tf\r\nfrom pyhanlp.static import HANLP_DATA_PATH\r\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\nfrom pyhanlp import *\r\n\r\nchn_senti_corp = os.path.join(HANLP_DATA_PATH, r\'train/ChnSentiCorp\')\r\nbatch_size = 32\r\nraw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\r\n    chn_senti_corp,\r\n    batch_size=batch_size,\r\n    validation_split=0.2,\r\n    subset=""training"",\r\n    seed=1337,\r\n)\r\nraw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\r\n    chn_senti_corp,\r\n    batch_size=batch_size,\r\n    validation_split=0.2,\r\n    subset=""validation"",\r\n    seed=1337,\r\n)\r\nprint(""Number of batches in raw_train_ds: %d"" % tf.data.experimental.cardinality(raw_train_ds))\r\nprint(""Number of batches in raw_val_ds: %d"" % tf.data.experimental.cardinality(raw_val_ds))\r\n\r\ndef preprocess(text_li):\r\n    return tf.strings.unicode_split(text_li, input_encoding=\'UTF-8\', errors=""ignore"")\r\n\r\n\r\nmax_features = 20000\r\nvectorize_layer = TextVectorization(\r\n    max_tokens=max_features,\r\n    output_mode=""int"",\r\n    split=preprocess,\r\n)\r\n\r\ntext_ds = raw_train_ds.map(lambda x, y: x)\r\nvectorize_layer.adapt(text_ds)\r\nprint(""vocabulary top 50"", vectorize_layer.get_vocabulary()[:50])\r\nprint(""vocabulary size"", len(vectorize_layer.get_vocabulary()))\r\n\r\ninteger_data = vectorize_layer([[""\xe4\xb8\x80\xe5\x87\xba\xe5\xb7\xae\xe5\xb0\xb1\xe5\x8e\xbb\xe5\xae\xbe\xe9\xa6\x86happy\xe4\xba\x86""]])\r\nprint(integer_data)\r\n```\r\n\r\nthe run result is \r\n```\r\nvocabulary top 50 [\'\', \'[UNK]\', \'\xef\xbc\x8c\', \'\\r\', \'\xe7\x9a\x84\', \'\\n\', \'\xe3\x80\x82\', \'\xe4\xb8\x8d\', \'\xe6\x98\xaf\', \'\xe4\xba\x86\', \'\xe6\x88\xbf\', \'\xe4\xb8\x80\', \'\xe5\xba\x97\', \'\xe6\x9c\x89\', \'\xe6\x88\x91\', \'\xe9\x85\x92\', \'\xe9\x97\xb4\', \'\xe5\xbe\x88\', \'\xe4\xbd\x8f\', \'\xe8\xbf\x98\', \' \', \'\xe6\x9c\x8d\', \'\xe5\x8a\xa1\', \'\xe5\x9c\xa8\', \'\xe5\xa5\xbd\', \'\xe5\x88\xb0\', \'\xe4\xb8\xaa\', \'\xe6\xb2\xa1\', \'\xe4\xba\xba\', \'\xe8\xbf\x99\', \'\xe4\xb8\x8a\', \'\xe4\xb9\x9f\', \'\xe5\xb0\xb1\', \'0\', \'\xe5\xa4\xa7\', \'\xef\xbc\x81\', \'\xe8\xa6\x81\', \'\xe6\x9d\xa5\', \'\xe7\x82\xb9\', \'\xe4\xbb\xa5\', \'\xe4\xbb\xac\', \'2\', \'\xe8\xaf\xb4\', \'\xe5\x8f\xaf\', \'1\', \'\xe6\x97\xb6\', \'\xe9\x83\xbd\', \'\xe5\x8e\xbb\', \'\xe5\xb0\x8f\', \'\xe5\x90\x8e\']\r\nvocabulary size 3255\r\ntf.Tensor([[ 11  80  58  32  47 111 123 336 412 514 514 637   9]], shape=(1, 13), dtype=int64)\r\n```\r\nI don\'t know the how jieba apply to the parameter \'split\', or there is something else method to use jieba.\r\n', b""Hi, have you solved the problem yet? I'm facing the same issue, thanks!""]",open,2020-09-25 04:45:33,
https://api.github.com/repos/keras-team/keras/issues/14213,"b""Specify 'use_bias=True' explicitly, the model get convergence; but use default settings couldn't""",[b'The issue maybe caused by the local optimum~~~~'],open,2020-09-11 08:32:37,
https://api.github.com/repos/keras-team/keras/issues/14212,b'The estimator Sequential should be a classifier.',"[b'@chafikblm  Moving this issue to closed status as there has been no recent activity, in case you still face the error please create a new issue.Thanks!', b""@sushreebarsa \r\nThank you for your attention, yes I still have the same problem and I didn't find a solution to it""]",open,2020-09-10 15:54:25,
https://api.github.com/repos/keras-team/keras/issues/14193,b'Multiple ground truths (incorporate in image generator mask folder and loss) or slice ground truth in custom loss',[],open,2020-08-19 21:08:32,
https://api.github.com/repos/keras-team/keras/issues/14187,b'Batch index or batch label using data generator',[],open,2020-08-10 16:12:00,
https://api.github.com/repos/keras-team/keras/issues/14179,"b'Keras reports \xe2\x80\x9cValueError: initial_value must have a shape specified:Tensor(\xe2\x80\x9ddense_4/MatMul:0\xe2\x80\x9c,shape=(?, 1),dtype=float32)\xe2\x80\x9d when customizing random AF'","[b'Tensorflow version: 1.14.0\r\nThe dataset is attached\r\n[Dataset.zip](https://github.com/keras-team/keras/files/5013091/Dataset.zip)\r\n\r\n\r\nHere is the complete code:\r\n\r\n```\r\nfrom __future__ import print_function\r\nfrom hyperopt import Trials, STATUS_OK, tpe\r\nfrom hyperas import optim\r\nfrom hyperas.distributions import choice, uniform\r\nfrom keras.layers.core import Dense, Dropout, Activation\r\nfrom keras.utils import np_utils \r\nimport pandas as pd\r\nfrom imblearn.over_sampling import ADASYN\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nimport numpy as np\r\nfrom numpy import loadtxt\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, GRU\r\nfrom keras import regularizers\r\nfrom keras.layers import Activation\r\nfrom keras.utils.generic_utils import get_custom_objects\r\nfrom keras.utils.np_utils import to_categorical\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import precision_score , recall_score\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom keras.callbacks import Callback, LearningRateScheduler\r\nimport math\r\nfrom keras.models import load_model\r\nimport csv\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling1D\r\nfrom keras.layers.convolutional import Conv1D\r\nfrom keras.optimizers import adam\r\nfrom keras.callbacks import Callback, LearningRateScheduler\r\nfrom keras import backend as K\r\n#!pip install h5py\r\n\r\n#activation function\r\nclass Rand(Activation):             # Take care of Rand and rand in the following few lines\r\n    \r\n    def __init__(self, activation, **kwargs):\r\n        super(Rand, self).__init__(activation, **kwargs)\r\n        self.__name__ = \'rand\'\r\n\r\n  \r\ndef rand(x):\r\n    # r = np.random.uniform(low=0.0, high=1.0, size=None)\r\n    # bool = tf.Variable(x<r)\r\n    # result = init_bias = tf.Variable(init_bias,validate_shape=False)\r\n    result = tf.Variable(tf.cond(tf.random.uniform(shape=[1])[0] > tf.Variable(x), 1, 0))\r\n    print (""asdasdasdasD"", result)\r\n    return result\r\n    # if x is not None:\r\n        # print (""yes"")\r\n    # if x<r:\r\n        # return int(0)\r\n    # else:\r\n        # return int(1)\r\n\r\nget_custom_objects().update({\'rand\': Rand(rand)})\r\n\r\npath_data_original = ""/home/tntech.edu/miibrahem42/GAN_Paper/Defense/""\r\n\r\n# load the data\r\ndef data():\r\n    no_samples = 200000 #number of zero of one samples\r\n    last_col_indx = 100\r\n    data = pd.read_csv(\'Dataset.csv\', sep=\',\', index_col=False, header=None)\r\n    \r\n    # # take n balanced samples\r\n    # s0 = data[last_col_indx][data[last_col_indx].eq(0)].sample(no_samples).index\r\n    # s1 = data[last_col_indx][data[last_col_indx].eq(1)].sample(no_samples).index \r\n    # data = data.loc[s0.union(s1)]\r\n    # # data = data.reindex(np.random.permutation(data.index))\r\n    # data = data.sample(frac=1)\r\n    # # print (len(list(data.to_numpy())[0]))\r\n    # # print ((list(data.to_numpy())))\r\n    \r\n    X_res = data.iloc[:,:data.shape[1]-1].to_numpy() #400,000\r\n    print (""total data shape: "",X_res.shape)\r\n    Y_res = data.iloc[:,data.shape[1]-1]\r\n    print (""total label shape: "",Y_res.shape)\r\n    \r\n    ada = RandomUnderSampler(ratio=\'majority\',random_state=42)\r\n    x_train, y_train = ada.fit_sample(X_res,Y_res) # over-sampled data\r\n    print (""sum of resulted labels is: "", sum(y_train))\r\n\r\n    xtr, x_valid, ytr, y_valid = train_test_split(x_train, y_train, test_size=0.3)\r\n\r\n    xtr = xtr.reshape(-1,xtr.shape[1],1)\r\n    x_valid = x_valid.reshape(-1,x_valid.shape[1],1)\r\n    \r\n    nb_classes = 2\r\n\r\n    print (""Number of training samples is: "",xtr.shape[0])\r\n    print (""Number of test samples is: "",x_valid.shape[0])\r\n\r\n    # ytr = to_categorical(ytr, nb_classes)\r\n    # y_valid = to_categorical(y_valid, nb_classes)\r\n\r\n    print (""label has a shape of: "", y_valid.shape,ytr.shape)\r\n    return xtr,ytr,x_valid,y_valid\r\n\r\nlr = 0.0001 \r\n\r\ndef scheduler(epoch):\r\n  if epoch < 8:\r\n    print (lr)\r\n    return lr\r\n  else:\r\n    return lr * math.exp(0.1 * (4 - epoch))\r\nlearning_rte = LearningRateScheduler(scheduler)\r\n\r\nclass TestCallback(Callback):\r\n    def __init__(self, test_data):\r\n        self.test_data = test_data\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        x, y = self.test_data\r\n        # loss, acc = self.model.evaluate(x, y, verbose=1)\r\n        # print(\'\\nTesting loss: {}, acc: {}\\n\'.format(loss, acc*100))\r\n \r\n\r\nxtr,ytr,x_valid,y_valid=data()\r\n\r\n# input dimensions\r\n\r\n# Train a RNN model without optimization\r\n\r\nbatch_size = 400\r\nnum_classes = 2\r\nepochs = 1  #should be set\r\n\r\ninput_shape = (100,1)\r\n\r\nprint (xtr.shape)\r\nprint (x_valid.shape)\r\n\r\n# the monitoring parameter should be the same on both earlystopping and modelcheckpoint in case of classification problems\r\nstop_training = EarlyStopping( monitor=\'val_accuracy\', mode=\'max\', verbose=1, patience=15) #monitor=\'val_loss\', mode=\'min\'\r\nbest_model_save = ModelCheckpoint(path_data_original+\'best_model_RNN_rand.h5\',monitor=\'val_accuracy\', mode=\'max\', verbose=1\r\n, save_best_only=True)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv1D(150, kernel_size=50, activation=\'relu\', input_shape=(input_shape)))\r\nmodel.add(MaxPooling1D(pool_size=4))\r\nmodel.add(GRU(200))\r\nmodel.add(Dropout(0.25))\r\n# model.add(Flatten())\r\nmodel.add(Dense(128, activation=\'relu\'))\r\nmodel.add(Dense(35, activation=\'relu\'))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Dense(1, activation=\'relu\', use_bias=False))\r\nmodel.add(Dense(1, activation=\'rand\', use_bias=False))  \r\nprint (model.summary())\r\n# model.layers[8].trainable = False  \r\n\r\n# # weights_of_last_layer = list([np.array([[1 , 1 ],\r\n       # # [0, 0]], dtype=\'float32\'), np.array([0., 0.], dtype=\'float32\')])\r\n\r\n# weights_of_last_layer = list([np.array([[1] ,[ 0 ]], dtype=\'float32\'), np.array([0.], dtype=\'float32\')])\r\n       \r\n# model.layers[8].set_weights(weights_of_last_layer)  \r\n  \r\nmodel.compile(loss=\'mean_squared_error\', optimizer=adam(lr=lr), metrics=[\'accuracy\'])  #categorical_crossentropy if binary classification\r\nmodel.fit(xtr, ytr,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1, \r\n         validation_split=0.1, callbacks=[TestCallback((x_valid, y_valid)), best_model_save, \r\n         stop_training, learning_rte])\r\n```\r\n\r\nplease let me know if you need further info.\r\nAll I want to do is implementing an activation function that takes the output of the last hidden layer and then generating a random number, then compares this number with the output -> if it is greater, the output will be one, otherwise, it will be zero.', b'I guess it maybe caused by compute graph workstyle. In tensorflow 1.x, it only supports constant graph, you could upgrade tensorflow to 2.0 which supports dynamic graph.']",open,2020-07-31 00:24:52,
https://api.github.com/repos/keras-team/keras/issues/14140,b'Recieve list of all outputs as input to a custom loss function.',"[b'I am trying to create a model with multiple outputs (4) with different shapes. I created a custom loss function with (y_true, y_pred) parameters and I expected that I will recieve a list of all outputs as y_pred. But instead I get only one of the output as y_pred.   \r\n\r\nHow can I receive a list of all outputs as input to my loss fucntion ?', b'Could you give an example of your code?', b'I have this question too. Some example code:\r\n```\r\nmodel = Model(\r\n    inputs=[input_a, input_b],\r\n    outputs=[output_a, output_b, output_c, output_d],\r\n)\r\n\r\ndef loss(y_true, y_pred):\r\n    # i want to get the 4 outputs of the model as y_pred, as they are all needed to calc loss\r\n    assert len(y_pred) == 4  # fails\r\n```', b""Could you show how you are compiling the model? If you instantiate the Model object with a list of outputs, you will need a list of loss functions when compiling it. This is fine if you are calculating each of their loss individually and you do not to do any loss calculation involving multiple outputs (e.g. ouput_a and output_b).\r\n\r\nAn example is below:\r\n```\r\nmodel = Model(\r\n    inputs=[input_a, input_b],\r\n    outputs=[output_a, output_b, output_c, output_d],\r\n)\r\n\r\nmodel.compile(optimizer=..., loss = ['rmse']*4)\r\n```\r\n\r\nHowever, if your loss calculation requires multiple outputs to be used, what can do is add a concatenate layer at the end of your model to produce one single output. From there, you should be able to access the 4 outputs in your custom loss function.\r\n\r\n\r\nAn example is below:\r\n```\r\noutput_a = # some layer\r\noutput_b = # some layer\r\noutput_c = # some layer\r\noutput_d = # some layer\r\noutput = tf.keras.backend.concatenate([output_a, output_b, output_c, output_d])\r\nmodel = Model(\r\n    inputs=[input_a, input_b],\r\n    outputs=output,\r\n)\r\n\r\ndef custom_loss(y_true, y_pred):\r\n    # i want to get the 4 outputs of the model as y_pred, as they are all needed to calc loss\r\n    output_a, output_b, output_c, output_d = y_pred\r\n    # calculate loss\r\n\r\nmodel.compile(optimizer=..., loss = custom_loss) # use the custom loss that you have created\r\n```"", b'I have the same problem, and in my case, it is not possible to concatenate at the end since the different output tensors have different shapes. So, I would really need a loss function that gets the list of outputs as y_pred input. ', b'For this you have to write loss functions seperately. If you are using tensorflow.keras you can use gradient tape to make your life easy.', b'I have the same problem too.  my model outputs 3 tensors. \r\n `model= Model(inputs = inputs,outputs =[predicted_image,depth,x_train])`   \r\n\r\nmy loss function should be  \r\n\r\n`\r\n def depth_loss_function(y_true, y_pred):\r\n\r\n    print(y_pred)\r\n\r\n    predict_image, depth , input_image = y_pred\r\n\r\n    # Point-wise depth\r\n    depth_loss = K.mean(K.abs(depth - y_true), axis=-1)\r\n\r\n    # image reconstruction\r\n    mse = tf.keras.losses.MeanSquaredError()\r\n    image_loss = mse(input_image - predict_image)\r\n\r\n    # Weights\r\n    w1 = 1.0\r\n    w2 = 1.0\r\n\r\n    return  (w2 * depth_loss) + (w2 * image_loss )`\r\ncompile method \r\n\r\n`model.compile(optimizer= optimizer, loss =depth_loss_function)`\r\n\r\n\r\n\r\nerror 1 : y_pred is only first tensor. \r\nerror 2  :  by using    predict_image, depth , input_image = y_pred   Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.', b'@manaspalaparthi maybe you can also stress this out in the issue I opened a few days ago: https://github.com/tensorflow/tensorflow/issues/46428', b'Has anyone found a solution to this? I am encountering the same issue: I have a model with two outputs (different shapes so I cannot concatenate them) and I need to pass them together in the loss function. ', b'I have the same issue, I have 3 outputs but custom loss function gets only one of them', b'Anyone found the solution to this?\r\n', b'Yes, I created a custom keras layer (as last layer in my function), and inside it I used self.add_loss function (you can pass any function with any number of parameters in add_loss). ']",open,2020-06-26 08:03:23,
https://api.github.com/repos/keras-team/keras/issues/14126,b'No convenient way to apply class weights for a segmentation model',[],open,2020-06-17 22:15:41,
https://api.github.com/repos/keras-team/keras/issues/14098,"b'expected PREDICTIONS to have 4 dimensions, but got array with shape (20, 131)'","[b""Hi,\r\n\r\nCan you try the following:\r\n\r\n1. **base_model = MobileNetV2(weights='imagenet', include_top=False)**\r\ninstead of \r\nbase_model = MobileNetV2(weights='imagenet', include_top=True)\r\n\r\n2. **for layer in base_model.layers:**\r\n            **layer.trainable = False**\r\ninstead of \r\nfor layer in model.layers[:-5]:\r\n    layer.trainable = False\r\n\r\n3. After the above write, \r\n**last_layer = base_model.get_layer('out_relu')**\r\n**x = layers.GlobalAveragePooling2D()(last_layer)**\r\ninstead of \r\nx = base_model.layers[-6].output\r\n\r\nand the rest follows as written\r\npredictions = Dense(len(classes), activation='softmax', name='PREDICTIONS')(x)\r\nmodel = Model(input=base_model.input, outputs=predictions)\r\n\r\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\r\n                  loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nmodel.fit_generator(train_generator, epochs= epochs,\r\n                                      steps_per_epoch=train_steps,\r\n                                      validation_data = val_generator,\r\n                                      validation_steps= valid_steps)\r\n\r\nLet us know if this doesn't work.\r\n"", b""Thank you for your reply. The suggestion didn't work. The updated code is as follows:\r\n\r\nbase_model = MobileNetV2(weights='imagenet', include_top=False)\r\n\r\nlast_layer = base_model.get_layer('out_relu')\r\nx = GlobalAveragePooling2D()(last_layer)\r\n\r\npredictions = Dense(len(classes), activation='softmax', name='PREDICTIONS')(x)\r\nmodel = Model(input=base_model.input, outputs=predictions)\r\n\r\nfor layer in base_model.layers:\r\n    layer.trainable = False\r\n\r\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\r\n                  loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\nmodel.summary()\r\n\r\nmodel.fit_generator(train_generator, epochs= epochs,\r\n                                      steps_per_epoch=train_steps,\r\n                                      validation_data = val_generator,\r\n                                      validation_steps= valid_steps,\r\n                                      callbacks=callbacks_list)\r\n\r\n----------------------------------------\r\n\r\nHowever, when I execute the above code it throws the following error:\r\n\r\nValueError: Layer global_average_pooling2d_1 was called with an input that isn't a symbolic tensor. Rec\r\neived type: <class 'keras.layers.advanced_activations.ReLU'>. Full input: [<keras.layers.advanced_activ\r\nations.ReLU object at 0x13661fcd0>]. All inputs to the layer should be tensors."", b""Hi,\r\n\r\nDid you put these lines of code\r\nfor layer in base_model.layers:\r\nlayer.trainable = False\r\nbefore\r\nlast_layer = base_model.get_layer('out_relu')\r\nx = GlobalAveragePooling2D()(last_layer)\r\n?\r\nIf not, please put that there (as I mentioned) and let us know the results. \r\nIf already, let us know.\r\n\r\n""]",open,2020-06-06 04:17:20,
https://api.github.com/repos/keras-team/keras/issues/14093,b'Model parallelism OOM error on unused GPU',[],open,2020-06-04 21:22:53,
https://api.github.com/repos/keras-team/keras/issues/14066,b'Regression in model.optimizer.get_gradients() with TF 2.2',"[b'So seems like this only works when the eager mode is deactivated. Otherwise should use the obscure `GradientsTape` class.', b'@philipperemy, Can you please try in latest Tensorflow and Keras version and let us know if the issue still persists?', b'@chunduriv will do thanks!', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""Closing as stale. Please reopen if you'd like to work on this further.\n"", b'@chunduriv the issue still persists with the latest tensorflow 2.7.0.\r\n\r\n@googlebot hey re-open this issue please.', b'```\r\npip list | grep tensorflow\r\ntensorflow                   2.7.0\r\ntensorflow-estimator         2.7.0\r\ntensorflow-io-gcs-filesystem 0.21.0\r\n```\r\n\r\n```\r\n[\'fc1/kernel:0\', \'fc1/bias:0\']\r\n[]\r\nTraceback (most recent call last):\r\n  File ""fff.py"", line 20, in <module>\r\n    grads = model.optimizer.get_gradients(model.losses, nodes)\r\n  File ""/private/tmp/ffffff/venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py"", line 766, in get_gradients\r\n    raise ValueError(""Variable {} has `None` for gradient. ""\r\nValueError: Variable <tf.Variable \'fc1/kernel:0\' shape=(10, 1) dtype=float32> has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\r\n```']",open,2020-05-26 11:47:38,
https://api.github.com/repos/keras-team/keras/issues/14062,b'Accessing the value of the loss from within the v2 optimizer',"[b""Our API guidelines on whether we add new features are described here:\r\nhttps://github.com/keras-team/governance/blob/master/keras_api_design_guidelines.md#carefully-weigh-whether-a-new-feature-should-be-included\r\n\r\nLooks like the the technique in the article didn't have sufficient adoption to justify adding them to the API at this time. Thanks!"", b'I am bit confused about the link you provided. The loss value did get passed to optimizers when it does the backprop, see https://github.com/keras-team/keras/blob/42bf9972492f47c3d3c249de9c20942ba217937d/keras/optimizer_v2/optimizer_v2.py#L499.\r\n\r\nAssigning to @chenmoneygithub who works on optimizers.', b""If you are using a custom training loop, then the loss value is not available inside optimizer.  Besides, even if you are using minimize(), which takes in the current loss, we will not keep the loss history. This behavior is expected, as tracking loss history is out of the scope of optimizer.\r\n\r\nThere are two ways to handle the learning rate adaption:\r\n1) in your custom training loop, before calling optimizer, you could manually compute the new learning rate, and set the optimizer's learning rate.\r\n2) via callback. You may override on_batch_end so that the learning rate is adapted, though it would lag behind by one step. That is, overriding on_batch_end would adapt the learning rate for next batch based on current batch's loss. This is an approximation method, but should achieve close performance.\r\n\r\nBesides, if you want to create a new optimizer, we encourage you to check out the reworked optimizer: [link](https://github.com/keras-team/keras/blob/master/keras/optimizer_experimental/optimizer.py). \r\n\r\nPlease let me know if you have any further questions, thanks!""]",open,2020-05-25 13:10:55,
https://api.github.com/repos/keras-team/keras/issues/14049,b'`CSVLogger` does not save validation loss and metrics when `validation_freq` is > 1 is passed to `model.fit`',"[b'try like this\r\n`csv_logger = keras.callbacks.CSVLogger(\'file.csv\',separator="","",append=True)`\r\n\r\n`model.fit(X, Y, batch_size=32, epochs=100, validation_split=0.1,callbacks [csv_logger],validation_freq=20)` ', b'Any update on this? I have the same problem. \r\n\r\n@Hemanthkumar2112  \r\nMy code is written as you suggested, but the problem persists. It seems that the metrics written to the csv file is decided on the first epoch, so when validation_freq > 1 only the train metrics are recorded and the validation metrics are overlooked. ', b""hi dude @karlau7  validation data is used to  validate training data with no of epochs. If you mentioned validation freq, for example \r\n validation_freq=10 in which it run or validate the validation data for every 10 epochs. my suggestion is don't mention any validation freq to a fit parameter. if you want to try with validation frequency use [2,10,20]. reference: [keras](https://keras.io/api/models/model/#fit) "", b""I second savaldor-dali! At a different epoch when a log with a new set of attributes is returned, it should find out that the header of csv is not comprehensive and therefore do a set difference on the old and new attributes. And simply append the new attributes (in this case 'val_loss') to the csv header. The previous entries can be left as it was. \r\n\r\nOr just add another argument for the list of metrics to log in CSVLogger. When some attributes are none, put none in the csv. \r\n\r\nAnother issue on the same topic [#40114](https://github.com/tensorflow/tensorflow/issues/40114).\r\nSomeone has submitted a commit [#40216](https://github.com/tensorflow/tensorflow/pull/40216).\r\n\r\n\r\n"", b'I experienced the same problem']",open,2020-05-15 08:17:02,
https://api.github.com/repos/keras-team/keras/issues/14034,b'Masking behavior of concatenation should be documented',"[b'@sushreebarsa: This issue notes a gap in the documentation. That gap still seems to be present.', b'Hey there @gabbard, would you be interested in contributing this?', b'@LukeWood I want to contribute to this.\r\ncan you tell me how do I contribute to documentation ', b""Hey @Krystal225!  The API doc page linked above is generated from the class docstrings.\r\n\r\nYou can see this specific example at:\r\nhttps://github.com/keras-team/keras/blob/master/keras/layers/merge.py#L429\r\n\r\nNote we use doctests:\r\n\r\nA brief overview... statements that output a value are tested against lines without prepended >>>:\r\n```\r\n>>> 'hello'\r\n'hello'\r\n```\r\npasses\r\n\r\n```\r\n>>> 'hello'\r\n'another value'\r\n```\r\nWould fail.  You can use this to demonstrate and validate behavior, as you'll see in https://github.com/keras-team/keras/blob/master/keras/layers/merge.py#L457\r\n\r\nPlease also be sure to review our Contributors guide!\r\nhttps://github.com/keras-team/keras/blob/master/CONTRIBUTING.md\r\n\r\nLet me know if you need any more information!"", b""@Krystal225 \r\n\r\nhere's an official documentation guide:\r\nhttps://www.tensorflow.org/community/contribute/docs_ref"", b'I have an issue \r\nI tried doctest on https://github.com/keras-team/keras/blob/master/keras/layers/merge.py#L429 \r\nfor demonstration. the test failed and said:\r\n```\r\nFailed example:\r\n    print(x)\r\nExpected:\r\n    [[[ 0  1  2  3  4]\r\n      [ 5  6  7  8  9]]\r\n     [[10 11 12 13 14]\r\n      [15 16 17 18 19]]]\r\nGot:\r\n    [[[ 0  1  2  3  4]\r\n      [ 5  6  7  8  9]]\r\n    <BLANKLINE>\r\n     [[10 11 12 13 14]\r\n      [15 16 17 18 19]]]\r\nTrying:\r\n    y = np.arange(20, 30).reshape(2, 1, 5)\r\nExpecting nothing\r\nok\r\nTrying:\r\n    print(y)\r\nExpecting:\r\n    [[[20 21 22 23 24]]\r\n     [[25 26 27 28 29]]]\r\n```\r\n\r\nneed help on how to fix BLANKLINE', b'For testing I needed to import numpy as np at the beginning of merge.py\r\n\r\nShould I directly add it to code and send a PR?', b'@LukeWood : I believe this should be assigned to @Krystal225 instead of me.', b'> @LukeWood : I believe this should be assigned to @Krystal225 instead of me.\r\n\r\nah thanks -  I updated it.', b'> For testing I needed to import numpy as np at the beginning of merge.py\r\n> \r\n> Should I directly add it to code and send a PR?\r\n\r\nCan you elaborate on what you mean here exactly?  If I remember correctly numpy is pre-imported in doctests (I may be wrong)', b'on my system I had to add \r\n```py\r\nimport numpy as np\r\n```\r\nin the beginning of merge.py for doctest to work.\r\nother wise all tests that had .np in their code failed.\r\n\r\n@LukeWood \r\n\r\n\r\nAlso is the BLANKLINE because of which the test at  https://github.com/keras-team/keras/blob/master/keras/layers/merge.py#L429 failed okay?\r\nor we need to correct it?']",open,2020-05-07 21:58:44,
https://api.github.com/repos/keras-team/keras/issues/12803,b'Initializing the val_loss with not Inf value',"[b'Please fill the issue [template](https://github.com/keras-team/keras/issues/new?template=a--tensorflow-backend-users.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. . Thanks!', b'Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will open a new issue. Thanks!', b'Would also need this, because I feed in subsequent training samples and read in old best weights (which have associated val_loss).', b""Hi,\r\nI had the same situation, after the first resumed epoch the model is saved simply because val_loss is improved from inf, without regard of the actual loss of the loaded model.\r\n\r\nMy network is slow and I have time limited access on the university HPC, so this was frustrating because every time I resumed my training, the first few epochs were lost to get back the network to original val_loss.\r\n\r\nBut I think I figured out where the problem is. The issue is [here](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L1196), in ModelCheckpoint the variable **self.best** is the monitor value of the previously saved best model. This quantity is compared to the current val_loss (varibale **current** in **on_epoch_end**) at the end of the specified epoch, and based on that a model with better loss is saved.\r\n\r\nWhen starting from begin **self.best** is initialized to Inf (positive or negative depending on the monitor), but in case of resuming this shouldn't be the case. This is because ModelCheckpoint has no information that it's a resuming training.\r\n\r\nAs a quick fix, I created a costumed callback, copying the source code of ModelCheckpoint and passing a new variable **prev_best** that correspond to the loss of the saved model (otherwise Inf if not specified)\r\n\r\nHere:\r\n\r\n```\r\nclass SaveModelCheckpoint(Callback):\r\n    def __init__(self, filepath, monitor='val_loss', verbose=0,\r\n                 save_best_only=False, save_weights_only=False,\r\n                 mode='auto', period=1, prev_best=None):\r\n        super(SaveModelCheckpoint, self).__init__()\r\n        self.monitor = monitor\r\n        self.verbose = verbose\r\n        self.filepath = filepath\r\n        self.save_best_only = save_best_only\r\n        self.save_weights_only = save_weights_only\r\n        self.period = period\r\n        self.epochs_since_last_save = 0\r\n\r\n        if mode not in ['auto', 'min', 'max']:\r\n            warnings.warn('SaveModelCheckpoint mode %s is unknown, '\r\n                          'fallback to auto mode.' % (mode),\r\n                          RuntimeWarning)\r\n            mode = 'auto'\r\n        \r\n        if(prev_best == None):\r\n            if mode == 'min':\r\n                self.monitor_op = np.less\r\n                self.best = np.Inf\r\n            elif mode == 'max':\r\n                self.monitor_op = np.greater\r\n                self.best = -np.Inf\r\n            else:\r\n                if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\r\n                    self.monitor_op = np.greater\r\n                    self.best = -np.Inf\r\n                else:\r\n                    self.monitor_op = np.less\r\n                    self.best = np.Inf\r\n        else:\r\n            if mode == 'min':\r\n                self.monitor_op = np.less\r\n                self.best = prev_best\r\n            elif mode == 'max':\r\n                self.monitor_op = np.greater\r\n                self.best = prev_best\r\n            else:\r\n                if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\r\n                    self.monitor_op = np.greater\r\n                    self.best = prev_best\r\n                else:\r\n                    self.monitor_op = np.less\r\n                    self.best = prev_best\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs = logs or {}\r\n        self.epochs_since_last_save += 1\r\n        if self.epochs_since_last_save >= self.period:\r\n            self.epochs_since_last_save = 0\r\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\r\n            if self.save_best_only:\r\n                current = logs.get(self.monitor)\r\n                if current is None:\r\n                    warnings.warn('Can save best model only with %s available, '\r\n                                  'skipping.' % (self.monitor), RuntimeWarning)\r\n                else:\r\n                    if self.monitor_op(current, self.best):\r\n                        if self.verbose > 0:\r\n                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\r\n                                  ' saving model to %s'\r\n                                  % (epoch + 1, self.monitor, self.best,\r\n                                     current, filepath))\r\n                        self.best = current\r\n                        if self.save_weights_only:\r\n                            self.model.save_weights(filepath, overwrite=True)\r\n                        else:\r\n                            self.model.save(filepath, overwrite=True)\r\n                    else:\r\n                        if self.verbose > 0:\r\n                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\r\n                                  (epoch + 1, self.monitor, self.best))\r\n            else:\r\n                if self.verbose > 0:\r\n                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\r\n                if self.save_weights_only:\r\n                    self.model.save_weights(filepath, overwrite=True)\r\n                else:\r\n                    self.model.save(filepath, overwrite=True)\r\n```"", b'@micbia - Thanks a zillion. Your code snippet came very handy. I wonder keras team is yet not worried about this issue. Working with Google Colab is a challenge as it timesout in few mins of inactivity and to restart the model from previous check point, loss is always calculated from inf. \r\n\r\nI wish Keras team tries to focus on this very basic lack of capability and fix it as soon as possible and provide this functionality out of the box. ', b'i too have this problem\r\n\r\nfor the keras team to consider that this is a real issue::\r\nin my model, i begin training a classifying layer with the rest of the model frozen\r\nnext, i unfreeze the last 25 layers (25 layers previous to the classifying layer)\r\nnext, the following 25, etc\r\n\r\nin this process i use the keras callback to save the best checkpoints and later load them back for the next stage\r\n\r\nafter every training stage and upon beginning a new one, the val_loss of the previous stage is lost and val_loss is initialized to inf\r\n\r\nthanks for the custom callback @micbia!', b'i need this too\r\n', b'Reopening it. Can one of you please provide a standalone code to reproduce the issue? Thanks', b'@jvishnuvardhan the issue can be replicated with any code. For instance, I can suggest you use the simple [CNN on MNIST](https://keras.io/examples/vision/mnist_convnet/) provided by Keras API.\r\n\r\n It is enough to run the network, let it save a model with the standard [ModelCheckpoint](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L1057) callback, stop the training before it is finished and then resume the network training from the stopped epoch while loading the previously best-saved model. \r\n\r\nYou will notice that after the first resumed epoch, no matter what loss you get a new model will be saved just because the monitored variable [best](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L1196) is reset to np.inf (or -np.inf).\r\n\r\nI proposed a quick fix by passing the variable **best=None** (the loss of the best-saved model before the stop), such that the user can define it in case of resumed training. Have a look at my callback [here](https://github.com/micbia/SegU-Net/blob/master/utils_network/callbacks.py#L141).\r\n\r\nTo notice that also the [ReduceLROnPlateau](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2270) callback has this problem. Once resumed the training the [wait](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/callbacks.py#L2332) variable is reset to 0. As a quick fix, I pass this variable so that the user can define it as the difference between the resume epoch and the epoch of the best-saved model, in case of resumed training. Check out my code where I use the callbacks [here](https://github.com/micbia/SegU-Net/blob/master/segUNet.py#L150). You can find the modified callback [here](https://github.com/micbia/SegU-Net/blob/master/utils_network/callbacks.py#L59).\r\n\r\nThank you for reopening the post.\r\nCheers', b'as @micbia says, you can use any code that uses the modelcheckpoint and earlystopping/reducelronplateau, etc\r\nanyways, here\'s quickly glued-together code:\r\n\r\n```\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ncheckpoint_dir_path = \'./keras_val_loss_test/\'\r\ncheckpoint_filepath1 = os.path.join(checkpoint_dir_path,\r\n                                \'step1/cp.ckpt\')\r\n\r\ncheckpoint_dir_path = \'./keras_val_loss_test/\'\r\ncheckpoint_filepath2 = os.path.join(checkpoint_dir_path,\r\n                                \'step2/cp.ckpt\')\r\n\r\n# Defining a dictionary with the hyperparameters\r\nhyperparameters = {\r\n    ""base_learning_rate"": 0.0001,\r\n    ""BATCH_SIZE"": 8,\r\n    ""beta1"": 0.9,\r\n    ""beta2"": 0.999,\r\n    ""epsilon"": 1e-07\r\n    }\r\n\r\nIMG_SIZE=(283, 577)\r\nIMG_SHAPE = IMG_SIZE + (3,)\r\n\r\n_URL = \'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\'\r\npath_to_zip = tf.keras.utils.get_file(\'cats_and_dogs.zip\', origin=_URL, extract=True)\r\nPATH = os.path.join(os.path.dirname(path_to_zip), \'cats_and_dogs_filtered\')\r\n\r\ntrain_dir = os.path.join(PATH, \'train\')\r\nvalidation_dir = os.path.join(PATH, \'validation\')\r\n\r\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\r\n                                             shuffle=False,\r\n                                             label_mode=\'categorical\',\r\n                                             batch_size=hyperparameters[""BATCH_SIZE""],\r\n                                             image_size=IMG_SIZE)\r\nvalidation_dataset = tf.keras.preprocessing.image_dataset_from_directory(validation_dir,\r\n                                             shuffle=False,\r\n                                             label_mode=\'categorical\',\r\n                                             batch_size=hyperparameters[""BATCH_SIZE""],\r\n                                             image_size=IMG_SIZE)\r\n\r\ndef recompile(model, layers, lr, b1, b2, e, checkpoint_filepath):\r\n    # This function takes a model ""model"" as input, unfreezes layers up to the \r\n    # specified ""layers"" param. Can also modify a model\'s hyperparameters \r\n    # through ""lr"", ""b1"", ""b2"" and ""e"" and load new saved weights from \r\n    # ""checkpoint_filepath"". Finally, it recompiles the model to effect all \r\n    # changes.\r\n\r\n    checkpoint_dir = os.path.dirname(checkpoint_filepath)\r\n    \r\n    l = 0\r\n    model.get_layer(""model"").get_layer(""MobilenetV3large"").trainable = True\r\n    while l < layers:\r\n        model.get_layer(""model"").get_layer(""MobilenetV3large"").get_layer(\r\n            index=l).trainable = False\r\n        l += 1\r\n        # print(model.get_layer(""MobilenetV3large"").get_layer(index=l))\r\n\r\n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr,\r\n                                                         beta_1=b1,\r\n                                                         beta_2=b2,\r\n                                                         epsilon=e),\r\n                      loss=tf.keras.losses.CategoricalCrossentropy (\r\n                          from_logits=True),\r\n                      metrics=[\'accuracy\'])\r\n\r\n        if os.path.exists(checkpoint_dir):\r\n            model.load_weights(checkpoint_filepath)\r\n            print(\'Checkpoints loaded\')\r\n        else:\r\n            # !mkdir - p training_checkpoints\r\n            print(\'No checkpoints found\')\r\n    return model\r\n\r\ndef training(model, title, epochs, patience, steps, checkpoint_filepath): \r\n    \r\n    # Declaring the callbacks, including the hyperdash experiments, then running\r\n    # the training\r\n    callback_cp = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath, \r\n                                                    save_weights_only = True, \r\n                                                    save_best_only = True,\r\n                                                    monitor = \'val_loss\',\r\n                                                    save_freq = \'epoch\',\r\n                                                    verbose = 1\r\n                                                    )\r\n\r\n    callback_es = tf.keras.callbacks.EarlyStopping(monitor = \'val_loss\',\r\n                                                   patience = patience, \r\n                                                   restore_best_weights = True\r\n                                                  )\r\n    callbacks = [callback_cp, callback_es]\r\n\r\n    # Training the network\r\n    history = model.fit(train_dataset, \r\n                        epochs=epochs, \r\n                        callbacks=callbacks,\r\n                        validation_data=validation_dataset,\r\n                        validation_steps=steps\r\n                        )\r\n    return history\r\n\r\nbase_model = tf.keras.applications.MobileNetV3Large(\r\n                                               input_shape=IMG_SHAPE,\r\n                                               include_top=False,\r\n                                               weights=\'imagenet\',\r\n                                               minimalistic=False)\r\n\r\ndata_augmentation = tf.keras.Sequential([\r\n  tf.keras.layers.RandomFlip(\'horizontal\'),\r\n  tf.keras.layers.RandomRotation(0.2),\r\n])\r\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\nprediction_layer = tf.keras.layers.Dense(1)\r\n\r\ninputs = tf.keras.Input(shape=(IMG_SHAPE)\r\nx = data_augmentation(inputs)\r\nx = base_model(x, training=False)\r\nx = global_average_layer(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\noutputs = prediction_layer(x)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\nhistory = training(model=model, \r\n                title=\'Keras val_loss issue step1\', \r\n                epochs=2, \r\n                patience=70, \r\n                steps=32, \r\n                checkpoint_filepath=checkpoint_filepath1\r\n                )\r\n\r\nmodel2= recompile(model=training_model,layers=251, \r\n                            lr=hyperparameters[""base_learning_rate""]/100,\r\n                            b1=hyperparameters[""beta1""], \r\n                            b2=hyperparameters[""beta2""],\r\n                            e=hyperparameters[""epsilon""], \r\n                            checkpoint_filepath=checkpoint_filepath1\r\n                            )\r\n\r\nhistory = training(model=model2, \r\n                title=\'Keras val_loss issue step2\', \r\n                epochs=2, \r\n                patience=70, \r\n                steps=32, \r\n                checkpoint_filepath=checkpoint_filepath2\r\n                )\r\n```\r\n\r\nThis should work, didn\'t test 100%\r\nthe result should be something like this (this is taken from a notebook of mine):\r\n![image](https://user-images.githubusercontent.com/74593034/149492939-979ed250-a705-4fee-95b2-1675265ed521.png)\r\n\r\nYou can already see the big impact this can have in long training. In the previous training stage, this was the best checkpoint:\r\n![image](https://user-images.githubusercontent.com/74593034/149493741-f9dbcb88-ce3b-4332-a226-8229f37c1284.png)\r\n\r\nIn the second stage, as the validation loss was initialised to inf, the first epoch checkpoint is saved, with a val_loss of 0.3736 (more than double) and val_acurracy of 0.9248. The next epoch sees a decline in both val_loss and val_accuracy, and it takes the training process approximately 200 epochs to return to a val_loss of 0.2 (still even higher than before)\r\n\r\nI run a machine on AWS and pay 1\xe2\x82\xac/h, and hours quickly pile up for me to waste time just to get to a previous stage state\r\n\r\n---\r\n\r\nI\'d like to add a possible solution. I think it\'s very polyvalent and can tackle many issues that could arise from the val_loss initialization to inf.\r\nThe callback could both return the val_loss and take it as an _optional_ input. That way you could pass on val_loss either from a previous training stage, or even by hand if the memory is flushed', b'This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n', b""any comments from the keras team?\r\n\r\nalso, kind reminder to change your notification settings on the right if you don't want to get notified of every comment in this thread"", b'@rchao Can you please take a loot at it. Thanks!']",open,2019-05-08 21:18:08,
